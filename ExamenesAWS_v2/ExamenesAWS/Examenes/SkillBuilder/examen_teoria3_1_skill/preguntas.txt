You are working on a machine learning (ML) solution for your company. The business lead of the project reaches out after the ML model is developed and asks about the next step of the ML process. Which answer BEST describes the model deployment phase of the ML lifecycle?

Model deployment is the integration of the model and its resources into a production environment so it can be used to create predictions. (Correct)

Model deployment is the process of selecting, training, and optimizing a machine learning model on the prepared data.  

Model deployment is the process of cleaning and preparing the data for model training.

Model deployment is another term for describing the entire ML lifecycle.

---

You are working on a machine learning (ML) project for an ecommerce company. The model has been developed and tested, and now it's time to deploy it to production. You want to build a repeatable framework to deploy and monitor the model effectively. Which step is not a necessary component of a repeatable machine learning operations (MLOps) framework?

Version control

Continuous integration and continuous delivery (CI/CD) for model deployment

Manual model performance updates (Correct)

Monitoring

---

You need to orchestrate an end-to-end machine learning (ML) workflow that includes data preprocessing, model training, evaluation, and deployment.  Which AWS service is purpose-built for automating, monitoring, and managing your ML workflow steps? 

Apache Airflow

Kubernetes

MLflow

SageMaker Pipelines (Correct)

---

Your company needs to deploy a machine learning (ML) model that requires high-performance computing power and low latency for real-time predictions. The model needs to be updated frequently, and you want a fully managed solution with minimal operational overhead. Your application does not require advanced customization. Which deployment target would be the best choice?

Amazon SageMaker endpoints (Correct)

Amazon Elastic Kubernetes service (Amazon EKS)

Amazon Elastic Container Service (Amazon ECS)

AWS Lambda


---

You are designing a machine learning (ML) model for a recommendation system that suggests products to customers based on their browsing and purchase history. The application needs to handle user requests with low latency and high throughput in real time.  Which SageMaker Inference strategy would be BEST suited for this use case? 

Real-time (Correct)

Asynchronous

Serverless

Batch transform

---

Your company is trying to decide between using on-demand or provisioned resources for their machine learning (ML) workloads. What is the main difference between the two, in terms of performance and scaling?

On-demand resources can scale up and down instantly, whereas provisioned resources have a fixed capacity and can take time to scale.

Provisioned resources have better performance than on-demand resources.

On-demand resources are more cost-effective for short-term, unpredictable workloads, whereas provisioned resources are better for long-term, predictable workloads. (Correct)

Provisioned resources can scale up and down instantly, whereas on-demand resources have a fixed capacity.

---

Your company operates a network of Internet of Things (IoT) devices that collect sensor data from remote locations. Which benefit of edge computing would be the MOST relevant in this scenario?

Reduced latency (Correct)

Improved data privacy and security

Increased energy efficiency

Enhanced reliability and fault tolerance

---

You built an image classification model that can identify different types of flowers from images. You want to deploy this model to classify a continuous stream of user-submitted flower images on your website in real time. Your focus is to create a seamless user experience, and you want to be able to specify the EC2 instance type you will run the workload on. Which deployment inference strategy would work BEST in this scenario?

Batch transform

Serverless inference

Asynchronous inference

Real-time inference (Correct)

---

Your organization is launching a machine learning-powered application that processes large volumes of data in batch mode. You are being asked to choose a managed container orchestration service that minimizes overhead. This service will also need to manage the other features of your application. Which option is the best choice, based on these requirements?

Amazon Elastic Kubernetes Service (Amazon EKS)

AWS Lambda

Amazon Elastic Container Service (Amazon ECS) (Correct)

Amazon SageMaker Endpoints


---

Your company operates a fleet of delivery vehicles equipped with various sensors. These vehicles frequently travel to areas where connectivity is not reliable. Which benefits of edge computing would be the most relevant in this scenario?

Improved data availability and reliability (Correct)

Reduce operational costs

Increased scalability and flexibility

Enhanced data analytics and insights

---

Your company is planning to deploy a mission-critical machine learning model in production. What is the main benefit of using provisioned resources for this deployment?

Provisioned resources offer more flexibility in terms of resource configuration and customization.

Provisioned resources can scale up and down instantly to meet changing demand.

Provisioned resources are more cost-effective for long-term, predictable workloads. (Correct)

Provisioned resources provide better performance than on-demand resources.

---

You are working on a complex machine learning (ML) project that involves multiple steps, including data preprocessing, model training, and model deployment. You want to use a service purpose-built for managing ML workflows. Which orchestration service is purpose-built for managing end-to-end workflows?

AWS Step Functions

Amazon Managed Workflows for Apache Airflow (Amazon MWAA)

Amazon SageMaker Pipeline (Correct)

Kubeflow

---

You have deployed a machine learning (ML) model in a production environment, and it is receiving a high volume of requests. As you have learned, a couple of considerations to design effective pipelines and infrastructure are availability and scalability. Which approach would you consider to ensure high availability and scalability?

Deploy the model on a single, high-performance server and route all requests to that server.

Implement a caching mechanism to store and serve frequently accessed model predictions, reducing the load on the model itself.

Use a load balancer to distribute incoming requests across multiple instances of the model running on different servers. (Correct)

Shut down the model instances for maintenance and updates periodically, handling requests in a queue during downtime.

---

You are developing a machine learning (ML) solution for real-time inference on video streams. Your organization is already using the AWS Neuron SDK for model training. Your organization would also like to use this SDK to deploy the model for inference. Which AWS compute instance type would be MOST appropriate for this task?

Amazon EC2 P5 instances

Amazon EC2 G5 instances

Amazon EC2 Inf2 instances (Correct)

Amazon SageMaker ml.inf1.xlarge instance

---

A repeatable framework includes the following components: version control, continuous integration and continuous delivery (CI/CD), model building, model deployment, monitoring, workflow security, model registries, and evaluation. Which answer best describes which factors of the machine learning (ML) lifecycle are included in the model deployment component of a repeatable framework?

Deployment focuses exclusively on storing and managing machine learning models, their metadata, and associated artifacts.

Deployment involves setting up the trained model in a production environment. It can include steps such as containerizing the model, integrating it with the target application or service, and ensuring seamless integration with the overall system architecture. (Correct)

Deployment involves the processes of data preparation, feature engineering, and model training. 

Deployment focuses exclusively on ongoing evaluation of model performance in the production environment. 

---

Your team is developing a machine learning (ML) model that requires multiple containers to handle different tasks, such as data preprocessing, model training, and model serving. 

What is the best approach to deploy this model using Amazon SageMaker?

Use a single-container endpoint and deploy multiple models in that container.

Use a multi-container endpoint and deploy a single model across multiple containers.

Use a single-container endpoint and handle all tasks within that container.

Use a multi-container endpoint and deploy each task in a separate container. (Correct)

---

You are working on a machine learning (ML) application that responds to requests for inference that often require processing large volumes of data. The processing time for each request can vary significantly, sometimes close to an hour.

Which inference strategy would you choose?

Serverless inference

Real-time inference

Batch transform job

Asynchronous inference (Correct)

---

An online publishing company wants to implement a machine learning (ML) model to predict how many customers will continue their subscription. The company has developed the model and is ready for the next step: model deployment. Which step is considered model deployment?

Integrating the trained model into the company's production systems. (Correct)

Training the model on the prepared dataset. 

Collecting and preprocessing customer data.

Evaluating the model's performance on a test set.

---

A company is deploying a machine learning (ML) model that is expected to have fluctuating traffic patterns, with sudden spikes in requests.  Which factors should be the primary consideration when provisioning the compute resources for this deployment?

CPU and memory requirements

Network bandwidth

Storage capacity

Scalability and elasticity (Correct)

---

You are responsible for selecting how to containerize your machine learning (ML) model. You are deciding whether to use Amazon SageMaker fully managed container options or use SageMaker options to bring your own inference code. What is the MOST important factor to consider when deciding between these two options?

Whether you can bring your own data

Whether a turnkey solution or flexibility is more important to your business (Correct)

Whether your business is designing an ML solution for the public sector

Whether your solution contains sensitive data like Protected Health Information (PHI)

---

Your team is developing a recommendation system that needs to analyze large-scale user interaction data and generate personalized recommendations in real time. When should you consider using GPUs for this task?

When the recommendation model is a simple linear model with small-scale data

When the recommendation model is based on deep learning techniques requiring fast training and inference (Correct)

When the model requires frequent updates and quick iterations

When the deployment environment strictly limits power consumption

