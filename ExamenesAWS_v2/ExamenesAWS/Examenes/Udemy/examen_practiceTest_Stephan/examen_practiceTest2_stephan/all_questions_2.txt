Pregunta 1

A financial services company deployed a machine learning model using Amazon SageMaker Asynchronous Inference in the past with successful performance. Now, the company needs to deploy a new ML model that detects fraudulent credit card transactions in real-time within their banking application. However, when using SageMaker Asynchronous Inference for this new model, the performance is poor and does not meet the real-time requirements. Additionally, the company wants to receive notifications whenever there is a deviation in the model's quality.

As an AWS Certified Machine Learning Engineer Associate, what do you recommend?

Respuesta correcta
Switch to SageMaker Real-Time Inference for the deployment and use SageMaker Model Monitor for model quality deviations

Retain SageMaker Asynchronous Inference and enable autoscaling for the endpoint to improve performance, while using Amazon SQS for notifications

Increase the instance size and response timeout settings for SageMaker Asynchronous Inference and use Amazon SNS for quality deviation notifications

Use SageMaker Batch Transform for inference and set up a monitoring system with SageMaker Model Monitor to send notifications for quality deviations

Explicación general
Correct option:

Switch to SageMaker Real-Time Inference for the deployment and use SageMaker Model Monitor for model quality deviations

SageMaker Real-Time Inference is designed for low-latency applications, making it ideal for real-time fraud detection. Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker AI hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.

Model quality monitoring jobs monitor the performance of a model by comparing the predictions that the model makes with the actual Ground Truth labels that the model attempts to predict. To do this, model quality monitoring merges data that is captured from real-time or batch inference with actual labels that you store in an Amazon S3 bucket, and then compares the predictions with the actual labels. If you set the value of the enable_cloudwatch_metrics to True when you create the monitoring schedule, model quality monitoring jobs can send all metrics to CloudWatch, which can further be used to send notifications.

Incorrect options:

Increase the instance size and response timeout settings for SageMaker Asynchronous Inference and use Amazon SNS for quality deviation notifications - While increasing the instance size and timeout may improve the performance, Asynchronous Inference is not suitable for real-time inference use cases.

Use SageMaker Batch Transform for inference and set up a monitoring system with SageMaker Model Monitor to send notifications for quality deviations - Batch Transform is intended for offline, large-scale batch processing and is not suitable for real-time fraud detection applications.

Retain SageMaker Asynchronous Inference and enable autoscaling for the endpoint to improve performance, while using Amazon SQS for notifications - While autoscaling can help handle higher loads, Asynchronous Inference is inherently not designed for real-time inference applications, making it a poor fit for this use case.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html

https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 2

You are a data scientist at an insurance company that uses a machine learning model to assess the risk of potential clients and set insurance premiums accordingly. The model was trained on data from the past few years, but recently, the company has expanded its services to new regions with different demographic characteristics. You are concerned that these changes in the data distribution might affect the model's performance and lead to biased or inaccurate predictions. To address this, you decide to use Amazon SageMaker Clarify to monitor and detect any significant shifts in data distribution that could impact the model.

Which of the following actions is the MOST EFFECTIVE for detecting changes in data distribution using SageMaker Clarify and mitigating their impact on model performance?

Use SageMaker Clarify’s bias detection capabilities to analyze the model’s output and identify any disparities between different demographic groups, retraining the model only if significant bias is detected

Use SageMaker Clarify to perform a one-time bias analysis during model training, ensuring that the model is initially fair and accurate, and manually monitor future data distribution changes

Respuesta correcta
Set up a continuous monitoring job with SageMaker Clarify to track changes in feature distribution over time and alert you when a significant feature attribution drift is detected, allowing you to investigate and potentially retrain the model

Implement a random sampling process to manually review a subset of incoming data each month, comparing it with the original training data to check for distribution changes

Explicación general
Correct option:

Set up a continuous monitoring job with SageMaker Clarify to track changes in feature distribution over time and alert you when a significant feature attribution drift is detected, allowing you to investigate and potentially retrain the model

A drift in the distribution of live data for models in production can result in a corresponding drift in the feature attribution values, just as it could cause a drift in bias when monitoring bias metrics. Amazon SageMaker Clarify feature attribution monitoring helps data scientists and ML engineers monitor predictions for feature attribution drift on a regular basis.

Continuous monitoring with SageMaker Clarify is the most effective approach for detecting changes in data distribution. By tracking feature distributions over time, you can identify when a significant shift occurs, investigate its impact on model performance, and decide if retraining is necessary. This proactive approach helps ensure that your model remains accurate and fair as the underlying data evolves.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html

Incorrect options:

Use SageMaker Clarify’s bias detection capabilities to analyze the model’s output and identify any disparities between different demographic groups, retraining the model only if significant bias is detected - While SageMaker Clarify’s bias detection is useful, focusing solely on bias in the model’s output doesn’t address the broader issue of shifts in feature distribution that can impact overall model performance. Continuous monitoring is needed to detect such changes proactively.

Implement a random sampling process to manually review a subset of incoming data each month, comparing it with the original training data to check for distribution changes - Manual reviews of data can be labor-intensive, error-prone, and may not catch distribution changes in a timely manner. Automated monitoring with SageMaker Clarify is more efficient and reliable.

Use SageMaker Clarify to perform a one-time bias analysis during model training, ensuring that the model is initially fair and accurate, and manually monitor future data distribution changes - A one-time bias analysis during training helps ensure initial fairness, but it doesn’t address ongoing changes in data distribution after the model is deployed. Continuous monitoring is necessary to maintain model performance over time.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-feature-attribution-drift.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 3

A global e-commerce company collects video/audio/text based customer feedback in various languages, including German, and stores the data for analysis. The company wants to use a large language model (LLM) to analyze and generate summaries of customer feedback in English. The solution must handle multilingual audio data efficiently and complete the task with the least operational effort.

Which solution will meet these requirements?

Respuesta correcta
Use Amazon Transcribe to convert video/audio feedback into German text, Amazon Translate to translate the German text into English, and Amazon Comprehend to analyze sentiment and summarize the feedback in English

Use Amazon Rekognition to extract metadata from video and audio feedback and combine it with audio data transcription to summarize using Amazon Comprehend

Manually preprocess all data formats into plain text and use a custom-trained SageMaker model for summarization in English

Use Amazon Transcribe to directly summarize the audio and video feedback from German to English text and use Amazon Comprehend to analyze customer sentiment

Explicación general
Correct option:

Use Amazon Transcribe to convert video/audio feedback into German text, Amazon Translate to translate the German text into English, and Amazon Comprehend to analyze sentiment and summarize the feedback in English

This solution efficiently leverages AWS services designed for specific tasks:

Amazon Transcribe: Converts video/audio feedback into German text, making the content accessible for further processing.

Amazon Translate: Translates the German text into English, ensuring compatibility with downstream analysis tools.

Amazon Comprehend: Analyzes the English text for customer sentiment and generates summaries.

This approach minimizes operational effort by using fully managed services that seamlessly integrate to handle the multilingual data pipeline.

Incorrect options:

Use Amazon Rekognition to extract metadata from video and audio feedback and combine it with audio data transcription to summarize using Amazon Comprehend - Rekognition is designed for image/video analysis and not audio analysis. So, this option is incorrect.

Manually preprocess all data formats into plain text and use a custom-trained SageMaker model for summarization in English - This approach introduces significant manual effort and operational complexity compared to the fully managed services provided by AWS.

Use Amazon Transcribe to directly summarize the audio and video feedback from German to English text and use Amazon Comprehend to analyze customer sentiment - Amazon Transcribe only converts video/audio to text and does not support direct summarization. Additional tools are required for the summarization task.

References:

https://aws.amazon.com/pm/transcribe/

https://aws.amazon.com/rekognition/

https://aws.amazon.com/translate/

https://aws.amazon.com/blogs/machine-learning/break-through-language-barriers-with-amazon-transcribe-amazon-translate-and-amazon-polly/

Temática
ML Model Development
Pregunta 4

You are a machine learning engineer responsible for optimizing the cost and performance of an ML model deployed on Amazon SageMaker. The model serves real-time predictions for an e-commerce platform, and the current instance type is providing reliable performance but at a higher cost than anticipated. Your goal is to determine the most cost-effective instance type that still meets the performance requirements for low-latency predictions.

Which approach is the MOST EFFECTIVE for rightsizing the instance family and size for your SageMaker endpoint?

Use SageMaker Inference Recommender to select the lowest-cost instance type, regardless of performance, and configure autoscaling to handle any additional load during peak times

Use AWS Compute Optimizer to analyze the current instance’s CPU and memory usage, and automatically switch to the smallest recommended instance type that matches the utilization metrics

Respuesta correcta
Use SageMaker Inference Recommender to run load tests across various instance types and configurations, compare the performance and cost of each, and select the instance type that offers the best balance between cost and performance

Manually review the performance metrics from Amazon CloudWatch for the current instance and experiment with different instance types by redeploying the model on each until you find the optimal one

Explicación general
Correct option:

Use SageMaker Inference Recommender to run load tests across various instance types and configurations, compare the performance and cost of each, and select the instance type that offers the best balance between cost and performance

SageMaker Inference Recommender is specifically designed to help you select the best instance type for your model by running load tests across various configurations. It provides detailed insights into how different instance types perform under real-world conditions, allowing you to make an informed decision that balances cost and performance. This approach ensures that you rightsize your instance while meeting both cost and performance requirements.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html

Incorrect options:

Use AWS Compute Optimizer to analyze the current instance’s CPU and memory usage, and automatically switch to the smallest recommended instance type that matches the utilization metrics - AWS Compute Optimizer is effective for analyzing general-purpose EC2 instances, but it doesn’t provide the ML-specific insights needed for optimizing SageMaker endpoints. Simply switching to the smallest recommended instance could lead to insufficient performance for real-time predictions.

Manually review the performance metrics from Amazon CloudWatch for the current instance and experiment with different instance types by redeploying the model on each until you find the optimal one - Manually experimenting with instance types is time-consuming and may lead to suboptimal choices. SageMaker Inference Recommender automates this process and provides more precise recommendations based on your model’s actual performance.

Use SageMaker Inference Recommender to select the lowest-cost instance type, regardless of performance, and configure autoscaling to handle any additional load during peak times - While cost is important, selecting the lowest-cost instance without considering performance issues could degrade the user experience. SageMaker Inference Recommender provides a more balanced approach by considering both cost and performance.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-prospective.html

https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender-load-test.html

https://aws.amazon.com/compute-optimizer/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 5

You are a machine learning engineer tasked with building a deep learning model to classify images for an autonomous vehicle project. The dataset is massive, consisting of millions of labeled images. Initial training runs on a single GPU instance in Amazon SageMaker are taking too long, and the training costs are rising. You need to reduce the model training time without compromising performance significantly.

Which of the following approaches is the MOST LIKELY to effectively reduce the training time while maintaining model performance?

Respuesta correcta
Implement distributed training using multiple GPU instances to parallelize the training process, reducing the overall time

Switch to a smaller instance type to reduce computational costs, accepting a longer training time as a trade-off

Enable early stopping to halt training when the model’s performance on the validation set stops improving, thereby avoiding overfitting

Reduce the size of the training dataset to speed up training, even if it means using fewer examples per class

Explicación general
Correct option:

Implement distributed training using multiple GPU instances to parallelize the training process, reducing the overall time

Distributed training allows you to split the workload across multiple GPU instances, significantly reducing training time by processing more data in parallel. Amazon SageMaker supports distributed training, making this an effective approach for large datasets and complex models.

SageMaker provides distributed training libraries and supports various distributed training options for deep learning tasks such as computer vision (CV) and natural language processing (NLP). With SageMaker’s distributed training libraries, you can run highly scalable and cost-effective custom data parallel and model parallel deep learning training jobs.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html

Incorrect options:

Enable early stopping to halt training when the model’s performance on the validation set stops improving, thereby avoiding overfitting - Early stopping is a useful technique to prevent overfitting by stopping training once the validation performance plateaus. While this can reduce training time, its effectiveness depends on the model's behavior and may not significantly shorten training time if the model converges slowly, as exemplified by the long training runs on a single GPU instance for the given use case.

Switch to a smaller instance type to reduce computational costs, accepting a longer training time as a trade-off - Switching to a smaller instance type might reduce costs, but it will likely increase training time, which is counterproductive to the goal of reducing overall training time.

Reduce the size of the training dataset to speed up training, even if it means using fewer examples per class - Reducing the dataset size could speed up training, but it would likely compromise model performance by reducing the amount of data the model can learn from, especially in a scenario where data diversity is critical, such as image classification for autonomous vehicles.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html

https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html

Temática
ML Model Development
Pregunta 6

A travel agency has collected a large volume of customer feedback recordings from calls made to its customer support team after launching a new vacation package. The agency wants to evaluate the success of the vacation package by analyzing the sentiment of customer feedback. The ML engineer needs to process and analyze the recordings to identify positive and negative sentiments in the least amount of time.

Which action should the ML engineer take?

Use AWS Glue to preprocess the feedback recordings and implement a sentiment analysis solution using open-source libraries

Respuesta correcta
Use Amazon Transcribe to convert the customer feedback recordings into text, and then use Amazon Comprehend to analyze the text for sentiment

Manually transcribe the recordings, convert them to English using Amazon Translate, and run a custom script to analyze sentiment

Build a custom sentiment analysis model using Amazon SageMaker and train it with labeled customer feedback data

Explicación general
Correct option:

Use Amazon Transcribe to convert the customer feedback recordings into text, and then use Amazon Comprehend to analyze the text for sentiment

This solution uses AWS services optimized for the workflow:

Amazon Transcribe - Converts audio feedback recordings into text, enabling text-based analysis.

Amazon Comprehend - Analyzes text for sentiment (positive, negative, neutral, mixed), providing actionable insights.

This solution leverages fully managed AWS services that minimize the need for custom coding and manual preprocessing. Also, the integration of Transcribe and Comprehend enables efficient and scalable sentiment analysis with minimal operational overhead.

Incorrect options:

Build a custom sentiment analysis model using Amazon SageMaker and train it with labeled customer feedback data - Training a custom model requires significant time and effort to prepare the data, train the model, and validate its accuracy. Managed services like Comprehend eliminate this overhead for sentiment analysis.

Manually transcribe the recordings, convert them to English using Amazon Translate, and run a custom script to analyze sentiment - Manually transcribing the recordings introduces unnecessary delays. Additionally, Amazon Comprehend supports multiple languages natively, making translation unnecessary.

Use AWS Glue to preprocess the feedback recordings and implement a sentiment analysis solution using open-source libraries - Amazon Polly converts text to speech, not speech to text. Writing a custom script for sentiment analysis adds complexity compared to using Amazon Comprehend.

References:

https://aws.amazon.com/transcribe/

https://aws.amazon.com/comprehend/

https://aws.amazon.com/blogs/machine-learning/generate-high-quality-meeting-notes-using-amazon-transcribe-and-amazon-comprehend/

Temática
ML Model Development
Pregunta 7

You are a data scientist working for a healthcare company that develops predictive models for diagnosing diseases based on patient data. Due to regulatory requirements and the critical nature of healthcare decisions, model interpretability is a top priority. The company needs to ensure that the predictions made by the model can be explained to both medical professionals and regulatory bodies.

You are evaluating different algorithms in Amazon SageMaker for your model, balancing the trade-off between accuracy and interpretability. The initial trials show that more complex models like deep neural networks (DNNs) yield higher accuracy but are less interpretable, whereas simpler models like logistic regression provide clearer insights but may not perform as well on the dataset.

Given these considerations, which of the following approaches is MOST APPROPRIATE for achieving both interpretability and acceptable performance?

Respuesta correcta
Use a tree-based algorithm like XGBoost, which offers a balance between accuracy and interpretability with feature importance

Deploy an ensemble of models including a complex model for accuracy and a simpler model for interpretability, using model stacking in SageMaker

Choose a logistic regression model due to its high interpretability and supplement it with additional data preprocessing to improve accuracy

Select a deep neural network (DNN) model and use SHAP (SHapley Additive exPlanations) to provide interpretability

Explicación general
Correct option:

Use a tree-based algorithm like XGBoost, which offers a balance between accuracy and interpretability with feature importance

The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:

Its robust handling of a variety of data types, relationships, distributions.

The variety of hyperparameters that you can fine-tune.

 via - https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html

XGBoost is a tree-based algorithm that naturally provides feature importance, making it easier to interpret which features are influencing the model's predictions. This strikes a balance between achieving high accuracy and maintaining interpretability, which is crucial in healthcare applications. Therefore, this represents the best option for the given use case.

Incorrect options:

Select a deep neural network (DNN) model and use SHAP (SHapley Additive exPlanations) to provide interpretability - You can use Shapley values to determine the contribution that each feature made to model predictions. These attributions can be provided for specific predictions and at a global level for the model as a whole. For example, if you used an ML model for college admissions, the explanations could help determine whether the GPA or the SAT score was the feature most responsible for the model’s predictions, and then you can determine how responsible each feature was for determining an admission decision about a particular student.

While SHAP (Shapley values) can provide interpretability for complex models like DNNs, the explanations can be more challenging to understand for non-technical stakeholders, especially in a high-stakes environment like healthcare. This approach may not fully address the interpretability requirement.

Choose a logistic regression model due to its high interpretability and supplement it with additional data preprocessing to improve accuracy - Logistic regression is highly interpretable, but it may not perform well on complex datasets compared to more sophisticated algorithms. While data preprocessing can improve its performance, there is often a significant trade-off in accuracy.

Deploy an ensemble of models including a complex model for accuracy and a simpler model for interpretability, using model stacking in SageMaker - Using an ensemble model may complicate interpretability, especially when combining different types of models. Although this approach might improve accuracy, the resulting complexity could hinder the ability to explain predictions clearly.

References:

https://docs.aws.amazon.com/prescriptive-guidance/latest/ml-model-interpretability/overview.html

https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html

https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-shapley-values.html

Temática
ML Model Development
Pregunta 8

You are a machine learning engineer at a fintech company that has developed several models for various use cases, including fraud detection, credit scoring, and personalized marketing. Each model has different performance and deployment requirements. The fraud detection model requires real-time predictions with low latency and needs to scale quickly based on incoming transaction volumes. The credit scoring model is computationally intensive but can tolerate batch processing with slightly higher latency. The personalized marketing model needs to be triggered by events and doesn’t require constant availability.

Given these varying requirements, which deployment target is the MOST SUITABLE for each model?

Deploy all three models on a single Amazon EKS cluster to take advantage of Kubernetes orchestration, ensuring consistent management and scaling across different use cases

Respuesta correcta
Deploy the fraud detection model using SageMaker endpoints for low-latency, real-time predictions, deploy the credit scoring model on Amazon ECS for batch processing, and deploy the personalized marketing model using AWS Lambda for event-driven execution

Deploy the fraud detection model using AWS Lambda for serverless, on-demand execution, deploy the credit scoring model on Amazon EKS for scalable batch processing, and deploy the personalized marketing model on SageMaker endpoints to handle event-driven inference

Deploy the fraud detection model on Amazon ECS for auto-scaling based on demand, deploy the credit scoring model using SageMaker endpoints for real-time scoring, and deploy the personalized marketing model on Amazon EKS for event-driven processing

Explicación general
Correct option:

Deploy the fraud detection model using SageMaker endpoints for low-latency, real-time predictions, deploy the credit scoring model on Amazon ECS for batch processing, and deploy the personalized marketing model using AWS Lambda for event-driven execution

Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling. SageMaker endpoints are optimized for low-latency, real-time predictions, making them ideal for the fraud detection model.

Amazon ECS provides a service scheduler for long-running tasks and applications. It also provides the ability to run standalone tasks or scheduled tasks for batch jobs or single run tasks. You can specify the task placement strategies and constraints for running tasks that best meet your needs. Amazon ECS is well-suited for batch processing tasks, making it a good choice for the credit scoring model.

AWS Lambda is ideal for the event-driven nature of the personalized marketing model, allowing it to scale on-demand with minimal cost.

 via - https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html

Incorrect options:

Deploy the fraud detection model using AWS Lambda for serverless, on-demand execution, deploy the credit scoring model on Amazon EKS for scalable batch processing, and deploy the personalized marketing model on SageMaker endpoints to handle event-driven inference - AWS Lambda is serverless and ideal for event-driven tasks, but it may not provide the low-latency, real-time performance required for fraud detection. SageMaker endpoints are better suited for this use case. The credit scoring model is better suited for ECS, where batch processing can be efficiently managed, while personalized marketing is a good fit for AWS Lambda.

Deploy all three models on a single Amazon EKS cluster to take advantage of Kubernetes orchestration, ensuring consistent management and scaling across different use cases - Deploying all models on a single Amazon EKS cluster could be overkill and lead to unnecessary complexity. While Kubernetes provides powerful orchestration, it might be excessive for simple, event-driven or batch workloads.

Deploy the fraud detection model on Amazon ECS for auto-scaling based on demand, deploy the credit scoring model using SageMaker endpoints for real-time scoring, and deploy the personalized marketing model on Amazon EKS for event-driven processing - While Amazon ECS can handle auto-scaling, it is not as optimized for real-time, low-latency predictions as SageMaker endpoints. Additionally, using SageMaker endpoints for the credit scoring model does not align well with batch processing needs. The personalized marketing model is better suited to AWS Lambda rather than Amazon EKS, which is more complex and designed for containerized applications with continuous workloads.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html

https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 9

Which AWS services are specifically designed to aid in monitoring machine learning models and incorporating human review processes? (Select two)

Selección correcta
Amazon SageMaker Model Monitor

Amazon SageMaker Data Wrangler

Amazon SageMaker Feature Store

Selección correcta
Amazon Augmented AI (Amazon A2I)

Amazon SageMaker Ground Truth

Explicación general
Correct options:

Amazon SageMaker Model Monitor

Amazon SageMaker Model Monitor is a service that continuously monitors the quality of machine learning models in production and helps detect data drift, model quality issues, and anomalies. It ensures that models perform as expected and alerts users to issues that might require human intervention.

Amazon Augmented AI (Amazon A2I)

Amazon Augmented AI (A2I) is a service that helps implement human review workflows for machine learning predictions. It integrates human judgment into ML workflows, allowing for reviews and corrections of model predictions, which is critical for applications requiring high accuracy and accountability.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html

Incorrect options:

Amazon SageMaker Data Wrangler - Amazon SageMaker Data Wrangler is designed to simplify and streamline the process of data preparation for machine learning, not specifically for monitoring or human review.

Amazon SageMaker Feature Store - Amazon SageMaker Feature Store is a fully managed repository for storing, updating, and retrieving machine learning features. While it aids in managing features used by models, it does not directly handle monitoring or human review processes.

Amazon SageMaker Ground Truth - Amazon SageMaker Ground Truth is used for building highly accurate training datasets for machine learning quickly. It does involve human annotators for labeling data, but it is not specifically designed for monitoring or human review of model predictions in production.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 10

You are a machine learning engineer at a healthcare startup that uses an Amazon SageMaker endpoint to deliver real-time diagnostics based on patient data. The model needs to handle a high volume of requests with low latency to ensure timely results. Recently, the startup has experienced rapid growth, leading to occasional periods of high traffic where users experience increased latency and, in some cases, request timeouts. You also need to be mindful of cost, as the startup operates on a tight budget.

Which approach is the MOST EFFECTIVE for troubleshooting and resolving the capacity concerns while balancing cost and performance?

Increase the instance size for the SageMaker endpoint to handle more requests per instance, and manually monitor performance and costs using Amazon CloudWatch metrics

Use AWS Lambda with provisioned concurrency to handle the requests, ensuring that the function is always ready to serve traffic. Configure Lambda to auto-scale based on traffic but limit the maximum concurrency to control costs

Respuesta correcta
nable auto-scaling for the SageMaker endpoint to automatically adjust the number of instances based on request load, and set a budget alert in AWS Budgets to monitor cost increases as traffic scales

Request a service quota increase for the SageMaker endpoint to allow for more instances during peak traffic, and set up a CloudWatch Alarm to notify you when utilization exceeds 80% of the current quota

Explicación general
Correct option:

nable auto-scaling for the SageMaker endpoint to automatically adjust the number of instances based on request load, and set a budget alert in AWS Budgets to monitor cost increases as traffic scales

Enabling auto-scaling on the SageMaker endpoint allows the system to automatically adjust the number of instances based on incoming traffic, ensuring that it can handle spikes without degrading performance. With a target tracking scaling policy, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.

By setting up a budget alert in AWS Budgets, you can monitor cost increases and ensure that scaling does not exceed your budget. This approach provides a balance between performance and cost, allowing the system to scale dynamically while keeping expenses in check.

 via - https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html

Incorrect options:

Increase the instance size for the SageMaker endpoint to handle more requests per instance, and manually monitor performance and costs using Amazon CloudWatch metrics - Increasing the instance size might temporarily address performance issues, but it is less flexible and could lead to over-provisioning, resulting in unnecessary costs during periods of low traffic. Manual monitoring is also less efficient and can lead to delays in addressing issues.

Use AWS Lambda with provisioned concurrency to handle the requests, ensuring that the function is always ready to serve traffic. Configure Lambda to auto-scale based on traffic but limit the maximum concurrency to control costs - Provisioned concurrency is a feature that keeps Lambda functions initialized and hyper-ready to respond in double-digit milliseconds. This is ideal for implementing interactive services, such as web and mobile backends, latency-sensitive microservices, or synchronous APIs. Using AWS Lambda with provisioned concurrency is suitable for serverless workloads, but it may not be the best fit for a SageMaker model that requires consistent performance at scale. Additionally, limiting concurrency to control costs could lead to performance bottlenecks during high-traffic periods.

Request a service quota increase for the SageMaker endpoint to allow for more instances during peak traffic, and set up a CloudWatch Alarm to notify you when utilization exceeds 80% of the current quota - Your AWS account has default quotas, formerly referred to as limits, for each AWS service. Requesting a service quota increase can help if the current quota is a limiting factor, but it doesn’t directly address cost control or the need for dynamic scaling. It’s a reactive approach that doesn’t provide the flexibility of auto-scaling combined with budget monitoring.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html

https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/

https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 11

You are a data scientist at a financial services company tasked with deploying a lightweight machine learning model that predicts creditworthiness based on a customer’s transaction history. The model needs to provide real-time predictions with minimal latency, and the traffic pattern is unpredictable, with occasional spikes during business hours. The company is cost-conscious and prefers a serverless architecture to minimize infrastructure management overhead.

Which approach is the MOST SUITABLE for deploying this solution, and why?

Deploy the model as a SageMaker endpoint for real-time inference, and configure AWS Lambda to preprocess incoming requests before sending them to the SageMaker endpoint for prediction

Use an Amazon EC2 instance to host the model, with AWS Lambda functions handling the communication between the API Gateway and the EC2 instance for prediction requests

Respuesta correcta
Deploy the model directly within AWS Lambda as a function, and expose it through an API Gateway endpoint, allowing the function to scale automatically with traffic and provide real-time predictions

Deploy the model using Amazon ECS (Elastic Container Service) and configure an AWS Lambda to trigger the ECS service on-demand, ensuring that the model is only running during peak traffic periods

Explicación general
Correct option:

Deploy the model directly within AWS Lambda as a function, and expose it through an API Gateway endpoint, allowing the function to scale automatically with traffic and provide real-time predictions

Deploying the model within AWS Lambda as a function and exposing it through an API Gateway endpoint is ideal for lightweight, serverless, real-time inference. Lambda’s automatic scaling and pay-per-use model align well with unpredictable traffic patterns and the need for cost efficiency.

 via - https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/

Incorrect options:

Deploy the model as a SageMaker endpoint for real-time inference, and configure AWS Lambda to preprocess incoming requests before sending them to the SageMaker endpoint for prediction - While deploying the model as a SageMaker endpoint is suitable for more complex models requiring managed infrastructure, this approach might be overkill for a lightweight model, especially if you want to minimize costs and management overhead. Lambda functions can serve the model directly in a more cost-effective manner.

Use an Amazon EC2 instance to host the model, with AWS Lambda functions handling the communication between the API Gateway and the EC2 instance for prediction requests - Hosting the model on an Amazon EC2 instance with Lambda managing communication adds unnecessary complexity and overhead. EC2-based deployments require more management and may not be as cost-effective for serverless and real-time use cases.

Deploy the model using Amazon ECS (Elastic Container Service) and configure an AWS Lambda to trigger the ECS service on-demand, ensuring that the model is only running during peak traffic periods - Using Amazon ECS triggered by AWS Lambda adds complexity and may not provide the same level of real-time responsiveness as directly deploying the model in Lambda.

Reference:

https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 12

A cloud architect is tasked with deploying a pre-trained AI model for a customer-facing application. The demand for the model's services fluctuates throughout the day, with occasional peaks in traffic and long periods of low activity. The architect needs to find a solution that scales automatically to handle high traffic while minimizing costs during idle periods.

Which solution will fulfill these requirements with minimal management overhead?

Deploy the model by using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon EventBridge to trigger auto scaling

Respuesta correcta
Deploy the model to an Amazon SageMaker endpoint. Create SageMaker endpoint auto scaling policies based on Amazon CloudWatch metrics to adjust the number of instances dynamically

Deploy the model to a group of Amazon Elastic Container Service (Amazon ECS) managed EC2 instances and set up an Application Load Balancer (ALB) in front of the ECS fleet to distribute incoming traffic

Deploy the model to an Amazon SageMaker endpoint using Provisioned Concurrency with Serverless Inference to cater to dynamically changing traffic patterns

Explicación general
Correct option:

Deploy the model to an Amazon SageMaker endpoint. Create SageMaker endpoint auto scaling policies based on Amazon CloudWatch metrics to adjust the number of instances dynamically

Amazon SageMaker AI supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.

To automatically scale as workload changes occur, you have two options: target tracking and step scaling policies.

In most cases, AWS recommends using target tracking scaling policies. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.

Incorrect options:

Deploy the model to a group of Amazon Elastic Container Service (Amazon ECS) managed EC2 instances and set up an Application Load Balancer (ALB) in front of the ECS fleet to distribute incoming traffic

Deploy the model by using Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon EventBridge to trigger auto scaling

Leveraging Amazon Elastic Container Service (Amazon ECS) to deploy the inference endpoint entails significant management overhead, so both these options are ruled out.

Deploy the model to an Amazon SageMaker endpoint using Provisioned Concurrency with Serverless Inference to cater to dynamically changing traffic patterns - Serverless Inference with provisioned concurrency is a cost-effective option when you have predictable bursts in your traffic. Provisioned concurrency allows you to deploy models on serverless endpoints with predictable performance, and high scalability by keeping your endpoints warm. So, this option is not cost-optimal for the given use case, as you have occasional peaks in traffic that are unpredictable.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-policy.html

https://docs.aws.amazon.com/autoscaling/application/userguide/target-tracking-scaling-policy-overview.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 13

A telecommunications company collects data about customer interactions and stores it in an Amazon S3 bucket. The company uses Amazon Athena to query the data and identify patterns. The dataset includes a churn indicator as the target variable, and the company wants to assess whether an ML model can accurately predict customer churn.

Which solution will provide this information with the LEAST development effort?

Use Amazon Bedrock with a pre-trained foundation model to directly analyze the dataset and predict churn without training a custom model

Leverage Amazon SageMaker Studio to build a custom workflow for preprocessing, feature engineering, training, and evaluation of predictive models

Respuesta correcta
Use Amazon SageMaker Autopilot to automatically analyze the dataset, generate candidate models, and evaluate their ability to predict customer churn

Download the dataset locally, preprocess the data manually, and use a third-party AutoML library to train and evaluate models

Explicación general
Correct option:

Use Amazon SageMaker Autopilot to automatically analyze the dataset, generate candidate models, and evaluate their ability to predict customer churn

Amazon SageMaker Autopilot is the most efficient solution because:

It provides end-to-end automation for machine learning workflows, including data preprocessing, feature engineering, model training, and evaluation.

It evaluates multiple candidate models to determine their predictive power with minimal manual effort.

It is fully managed, making it ideal for teams looking to quickly assess a dataset's suitability for predictive modeling.

Incorrect options:

Use Amazon Bedrock with a pre-trained foundation model to directly analyze the dataset and predict churn without training a custom model - Amazon Bedrock provides pre-trained foundation models for NLP and generative tasks. It is not designed for custom predictive modeling on tabular datasets like customer churn data.

Download the dataset locally, preprocess the data manually, and use a third-party AutoML library to train and evaluate models - This approach involves significant manual effort, including data preprocessing and infrastructure setup, making it less efficient than using SageMaker Autopilot.

Leverage Amazon SageMaker Studio to build a custom workflow for preprocessing, feature engineering, training, and evaluation of predictive models - While SageMaker Studio provides flexibility, it requires substantial development effort compared to SageMaker Autopilot, which automates the entire process.

References:

https://aws.amazon.com/sagemaker-ai/autopilot/

https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html

Temática
ML Model Development
Pregunta 14

A healthcare organization has deployed a predictive ML model in production using Amazon SageMaker. To ensure the model's performance, the organization has enabled SageMaker Model Monitor to track data quality. After recent update to the model, a data scientist observes data quality issues flagged by Model Monitor checks.

What steps should the data scientist take to address the data quality issues identified by Model Monitor?

Respuesta correcta
Generate a new baseline using the latest dataset and configure Model Monitor to use this updated baseline for future evaluations

Ingest Ground Truth labels and use these as an updated baseline for future evaluations

Modify the model's hyperparameters and redeploy the updated version to production

The constraint_violations.json file lists the violations detected in the current dataset. Manually correct the data issues and rerun the model

Explicación general
Correct option:

Generate a new baseline using the latest dataset and configure Model Monitor to use this updated baseline for future evaluations

A baseline is used as a reference to compare real-time or batch predictions from the model. It computes statistics and metrics along with constraints on them. During monitoring, all of these are used in conjunction to identify violations.

Data quality monitoring automatically monitors machine learning (ML) models in production and notifies you when data quality issues arise. ML models in production have to make predictions on real-life data that is not carefully curated like most training datasets. If the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, the model begins to lose accuracy in its predictions. Amazon SageMaker Model Monitor uses rules to detect data drift and alerts you when it happens. A new baseline should be created with the latest dataset and configure Model Monitor to use this updated baseline for future evaluations.

Incorrect options:

Modify the model's hyperparameters and redeploy the updated version to production - Hyperparameters are used during the training phase of model development. Adjusting hyperparameters does not address data quality issues for the given use case, hence this option is incorrect.

Ingest Ground Truth labels and use these as an updated baseline for future evaluations - Model quality monitoring jobs monitor the performance of a model by comparing the predictions that the model makes with the actual Ground Truth labels that the model attempts to predict. Ground Truth labels are useful for model quality monitoring and not for data quality monitoring.

The constraint_violations.json file lists the violations detected in the current dataset. Manually correct the data issues and rerun the model - Manually fixing all the data violations listed in the constraint_violations.json file is impractical, making this option unsuitable.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-faqs.html

Temática
ML Model Development
Pregunta 15

A financial services company is using Amazon SageMaker Canvas to build machine learning models for predicting stock price trends. The company stores its data in Amazon S3, and the dataset includes large volumes of transactional records with a complex structure, including nested fields and multiple data types. The data scientist needs to choose a file format that will optimize data processing performance and minimize loading time into SageMaker Canvas.

Which file format will meet these requirements?

XML

CSV

JSON

Respuesta correcta
Apache Parquet

Explicación general
Correct option:

Apache Parquet

Apache Parquet is specifically designed for large-scale data processing, making it ideal for machine learning workflows with complex datasets. Its columnar format reduces the amount of data that needs to be processed by enabling efficient selection of relevant columns, thereby minimizing data processing time. Parquet also supports compression and parallel processing, which further enhances its performance when handling large datasets like transactional records. This makes it the most suitable file format for importing data into SageMaker Canvas efficiently.

Incorrect options:

JSON - JSON can represent complex structures but is verbose and inefficient compared to Parquet. Processing JSON files requires more computational resources, increasing data import times.

CSV - CSV is ideal for flat, tabular data but lacks support for nested structures. It is not optimized for large-scale, complex datasets and often results in slower processing times.

XML - XML is verbose and inefficient compared to Parquet. It is not optimized for performance, and SageMaker Canvas does not provide enhanced support for XML processing.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/canvas.html

https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-parquet-home.html

https://aws.amazon.com/about-aws/whats-new/2023/06/amazon-sagemaker-canvas-apache-parquet-file-format/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 16

A healthcare analytics company has implemented a data ingestion pipeline to process patient monitoring data from wearable devices. The pipeline uses Amazon Kinesis Data Firehose to ingest data into Amazon OpenSearch Service. Currently, the Firehose stream has a buffer interval of 60 seconds, and an OpenSearch dashboard displays real-time alerts about patients' health metrics based on the ingested data. The company needs to optimize the pipeline to achieve sub-second latency for the alerts on the dashboard to respond quickly to critical patient health events.

What do you recommend as the most optimal solution?

Enable caching on the OpenSearch dashboard to reduce latency by serving previously ingested data for alert generation

Respuesta correcta
Reduce the Kinesis Data Firehose buffer interval to zero seconds by leveraging 'buffering hints', thereby enabling immediate data delivery to OpenSearch Service

Replace the Firehose stream with Amazon Kinesis Data Streams and use AWS Lambda to write the data to OpenSearch Service with lower latency

Switch from Kinesis Data Firehose to Apache Spark Streaming to preprocess patient monitoring data and load it into OpenSearch Service in real time

Explicación general
Correct option:

Reduce the Kinesis Data Firehose buffer interval to zero seconds by leveraging 'buffering hints', thereby enabling immediate data delivery to OpenSearch Service

Kinesis Data Firehose provides configurable buffer intervals for data delivery to destinations. Setting the buffer interval to zero seconds ensures data is delivered to OpenSearch Service as soon as it is ingested, minimizing latency. You can create a buffering hint by setting a value of zero for IntervalInSeconds parameter. Firehose will buffer incoming data for the specified period of time, in seconds, before delivering it to the destination. This adjustment eliminates batching delays and allows the OpenSearch dashboard to display near real-time health alerts, meeting the sub-second latency requirement.

Incorrect options:

Enable caching on the OpenSearch dashboard to reduce latency by serving previously ingested data for alert generation - Caching can reduce query latency but does not address the data ingestion delay caused by the 60-second Firehose buffer interval.

Replace the Firehose stream with Amazon Kinesis Data Streams and use AWS Lambda to write the data to OpenSearch Service with lower latency - While Kinesis Data Streams and Lambda could reduce latency, this solution increases operational complexity compared to simply adjusting the Firehose buffer interval.

Switch from Kinesis Data Firehose to Apache Spark Streaming to preprocess patient monitoring data and load it into OpenSearch Service in real time - While Apache Spark Streaming could reduce latency, this solution increases operational complexity as you need to write custom code to implement the solution. So, this option is not the best fit.

References:

https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-kinesis-data-firehose-zero-buffering/

https://docs.aws.amazon.com/firehose/latest/APIReference/API_BufferingHints.html

https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 17

A data analytics company is using an Amazon EMR cluster to process large volumes of log data collected from various sources. The logs must be processed in batches, and the results are stored in Amazon S3 for downstream analytics. The company requires a cluster configuration that ensures no data loss during processing while being as cost-effective as possible.

Which instance purchasing option will meet these requirements?

Use Spot instances for primary and core nodes, and On-Demand instances for task nodes to balance cost and availability

Use Spot instances for all primary, core, and task nodes to minimize costs and automatically handle failures by replacing interrupted nodes

Respuesta correcta
Use an On-Demand instance for the primary node along with the core nodes, and Spot instances for task nodes to reduce costs while ensuring fault tolerance

Use an On-Demand instance for the primary node and Spot instances for the core nodes as well as the task nodes to reduce costs while ensuring fault tolerance

Explicación general
Correct option:

Use an On-Demand instance for the primary node along with the core nodes, and Spot instances for task nodes to reduce costs while ensuring fault tolerance

Amazon EMR cluster architecture consists of the following nodes:

Primary Node - Manages the cluster and must always be reliable. An On-Demand instance ensures continuous operation.

Core Nodes - Perform data processing and store intermediate results in HDFS. These nodes must also be fault-tolerant, requiring On-Demand instances.

Task Nodes - Execute additional processing without storing data, making Spot instances a cost-effective choice since their loss does not affect data durability.

This configuration combines fault tolerance for critical components (primary and core nodes) with cost savings for processing (task nodes).

Incorrect options:

Use Spot instances for all primary, core, and task nodes to minimize costs and automatically handle failures by replacing interrupted nodes - Spot interruptions can cause instability for primary and core nodes, risking data loss and cluster failures.

Use Spot instances for primary and core nodes, and On-Demand instances for task nodes to balance cost and availability - Spot instances for primary and core nodes risk data loss due to interruptions, violating the fault-tolerance requirement.

Use an On-Demand instance for the primary node and Spot instances for the core nodes as well as the task nodes to reduce costs while ensuring fault tolerance - Spot instances for core nodes risk data loss due to interruptions, violating the fault-tolerance requirement.

References:

https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html

https://aws.amazon.com/ec2/spot/use-case/emr/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 18

You are a Cloud Financial Manager at a SaaS company that uses various AWS services to run its applications and machine learning workloads. Your management team has asked you to reduce overall AWS spending while ensuring that critical applications remain highly available and performant. To achieve this, you need to use AWS cost analysis tools to monitor spending, identify cost-saving opportunities, and optimize resource utilization across the organization.

Which of the following actions can you perform using the appropriate AWS cost analysis tools to achieve your goal of reducing costs and optimizing AWS resource utilization? (Select two)

Selección correcta
Use AWS Cost Explorer to analyze historical spending patterns, identify cost trends, and forecast future costs to help with budgeting and planning

Use AWS Cost Explorer to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds

Leverage AWS Trusted Advisor to directly modify and reconfigure resources based on cost optimization recommendations without manual intervention

Use AWS Cost Explorer to automatically delete unused resources across your AWS environment, ensuring that no unnecessary costs are incurred

Selección correcta
Leverage AWS Trusted Advisor to receive recommendations for cost optimization, such as identifying underutilized or idle resources, and reserved instance purchasing opportunities

Explicación general
Correct options:

Use AWS Cost Explorer to analyze historical spending patterns, identify cost trends, and forecast future costs to help with budgeting and planning

AWS Cost Explorer allows you to analyze your past AWS spending, identify cost trends, and forecast future costs based on historical data. This tool is valuable for budgeting and financial planning, helping you make informed decisions about resource allocation and cost management.

Leverage AWS Trusted Advisor to receive recommendations for cost optimization, such as identifying underutilized or idle resources, and reserved instance purchasing opportunities

AWS Trusted Advisor provides actionable recommendations to optimize your AWS environment, including cost-saving suggestions. It identifies opportunities such as underutilized resources, idle instances, and reserved instance purchasing options that can significantly reduce your AWS costs.

 via - https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html

Incorrect options:

Use AWS Cost Explorer to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds You can only use AWS Budgets to set custom budgets for cost and usage to govern costs across your organization and receive alerts when costs exceed your defined thresholds.

Use AWS Cost Explorer to automatically delete unused resources across your AWS environment, ensuring that no unnecessary costs are incurred AWS Cost Explorer does not have the capability to automatically delete unused resources. It is a cost analysis tool that helps you visualize and understand your costs but does not manage or modify your AWS resources directly.

Leverage AWS Trusted Advisor to directly modify and reconfigure resources based on cost optimization recommendations without manual intervention - AWS Trusted Advisor provides recommendations but does not automatically modify or reconfigure resources. Changes must be made manually based on the insights provided by the tool.

References:

https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html

https://aws.amazon.com/aws-cost-management/aws-cost-explorer/

https://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 19

You are a machine learning engineer at a company building a recommendation system using deep learning. The system is based on a neural network architecture implemented in TensorFlow. Your team wants to train the model on a large dataset stored in Amazon S3 and leverage the scalability of Amazon SageMaker. You plan to use SageMaker script mode to customize the training process while taking advantage of SageMaker’s managed services.

Which of the following actions is the MOST LIKELY to help you successfully train the model using SageMaker script mode with TensorFlow?

Use SageMaker script mode to automatically convert your Python script into a SageMaker Estimator object without specifying the TensorFlow framework version

Create a custom Docker container with TensorFlow installed and push it to Amazon ECR, then use SageMaker script mode to launch the training job

Package your TensorFlow model as a .tar.gz file, upload it to Amazon S3, and use SageMaker script mode to directly deploy the model for training

Respuesta correcta
Write a training script in Python that adheres to TensorFlow's Estimator API, upload it to an S3 bucket, and use SageMaker script mode with the built-in TensorFlow container to execute the script

Explicación general
Correct option:

Write a training script in Python that adheres to TensorFlow's Estimator API, upload it to an S3 bucket, and use SageMaker script mode with the built-in TensorFlow container to execute the script

Script mode enables you to write custom training and inference code while still utilizing common ML framework containers maintained by AWS.

SageMaker supports most of the popular ML frameworks through pre-built containers, and has taken the extra step to optimize them to work especially well on AWS compute and network infrastructure in order to achieve near-linear scaling efficiency. These pre-built containers also provide some additional Python packages, such as Pandas and NumPy, so you can write your own code for training an algorithm. These frameworks also allow you to install any Python package hosted on PyPi by including a requirements.txt file with your training code or to include your own code directories.

For the given use case, you need to utilize the SageMaker script mode and write a Python script that leverages the TensorFlow framework (following the Estimator API or a custom training loop). Upload this script to S3, and specify the TensorFlow framework version in SageMaker’s built-in container. SageMaker will then run your script in the managed environment, handling tasks like data loading and distributed training.

 via - https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/

Incorrect options:

Create a custom Docker container with TensorFlow installed and push it to Amazon ECR, then use SageMaker script mode to launch the training job - While using a custom Docker container is a valid approach, it’s not necessary when using SageMaker script mode with supported frameworks like TensorFlow or PyTorch. SageMaker already provides built-in containers for these frameworks, allowing you to focus on your training script without the overhead of managing custom containers.

Package your TensorFlow model as a .tar.gz file, upload it to Amazon S3, and use SageMaker script mode to directly deploy the model for training - Packaging the model as a .tar.gz file and uploading it to S3 is relevant for deploying a pre-trained model, not for training a model. SageMaker script mode is intended for running custom training scripts, not directly deploying pre-packaged models for training.

Use SageMaker script mode to automatically convert your Python script into a SageMaker Estimator object without specifying the TensorFlow framework version - SageMaker script mode does not automatically convert your script into an Estimator object. You must write the script according to the framework’s requirements and specify the desired framework version when launching the training job.

References:

https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/

https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html

https://sagemaker.readthedocs.io/en/v2.27.1/frameworks/tensorflow/deploying_tensorflow_serving.html

Temática
ML Model Development
Pregunta 20

A retail company uses Amazon SageMaker to train ML models for predicting product demand. The company stores training data for different product categories in Amazon S3 within a single AWS account. Each data scientist is assigned to a specific product category and must have access to only their respective category's training data. Data scientists must not be able to access training data from other product categories. The company needs to ensure secure, fine-grained access control for the training data while maintaining centralized management of resources.

Which solution will meet these requirements?

Create separate S3 buckets for each product category. Use bucket policies to grant access to individual buckets based on each data scientist's IAM role

Set up separate AWS accounts for each product category under an AWS Organizations structure. Restrict each data scientist's access to their assigned account

Create a preprocessing pipeline using AWS Glue to separate training data for each product category into individual S3 buckets. Use bucket policies to grant access to these buckets

Respuesta correcta
Use IAM policies with resource-based conditions to grant each data scientist access to specific S3 bucket prefixes that correspond to their assigned product category. Attach these policies to the IAM roles used by SageMaker notebook instances

Explicación general
Correct option:

Use IAM policies with resource-based conditions to grant each data scientist access to specific S3 bucket prefixes that correspond to their assigned product category. Attach these policies to the IAM roles used by SageMaker notebook instances

IAM policies with resource-based conditions, such as s3:prefix, allow fine-grained access control within a single S3 bucket. This approach avoids the need for multiple buckets and simplifies management. By attaching these policies to SageMaker notebook instance roles, data scientists can securely access only their assigned training data.

Incorrect options:

Create separate S3 buckets for each product category. Use bucket policies to grant access to individual buckets based on each data scientist's IAM role - While creating separate S3 buckets for each product category can achieve the goal, it adds operational overhead. Managing multiple buckets and bucket policies for each category increases complexity, especially as the number of product categories grows.

Create a preprocessing pipeline using AWS Glue to separate training data for each product category into individual S3 buckets. Use bucket policies to grant access to these buckets - This approach introduces unnecessary cost and complexity by duplicating the training data across multiple S3 buckets. AWS Glue is primarily intended for ETL tasks and is not needed for simple access control requirements.

Set up separate AWS accounts for each product category under an AWS Organizations structure. Restrict each data scientist's access to their assigned account - Creating separate AWS accounts for each product category is excessive and introduces significant administrative overhead. This approach is not cost-effective for managing access control when the data resides in a single AWS account.

Reference:

https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 21

You are a data scientist working on a predictive maintenance model for an industrial manufacturing company. The model is designed to predict equipment failures based on sensor data collected over time. During the development process, you notice that the model performs exceptionally well on the training data but struggles to generalize to new, unseen data. Additionally, there are some indications that the model might not be fully capturing the complexity of the problem. To ensure the model performs well in production, you need to identify whether it is overfitting, underfitting, or both.

Which of the following strategies is the MOST EFFECTIVE for identifying overfitting and underfitting in your model?

Perform cross-validation with different subsets of the data; if the model’s performance varies significantly across folds, the model is underfitting

Reduce the number of features in the model; if performance improves, the model was previously overfitting

Respuesta correcta
Compare the training and validation loss curves over time; if the validation loss is much higher than the training loss, the model is likely overfitting

Analyze the model’s performance on a separate test set; if the model performs well on both the training and test sets, it is neither overfitting nor underfitting

Explicación general
Correct option:

Compare the training and validation loss curves over time; if the validation loss is much higher than the training loss, the model is likely overfitting

Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.

 via - https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html

Comparing the training and validation loss curves is an effective way to identify overfitting. If the validation loss is significantly higher than the training loss, it indicates that the model is overfitting the training data and failing to generalize to unseen data. This is a clear sign that the model is too complex or trained for too many epochs.

Incorrect options:

Reduce the number of features in the model; if performance improves, the model was previously overfitting - Reducing the number of features might reduce overfitting, but it’s not a diagnostic method. It’s a corrective measure rather than a strategy to identify overfitting or underfitting. The model’s behavior on training versus validation data is a better indicator of overfitting or underfitting.

Perform cross-validation with different subsets of the data; if the model’s performance varies significantly across folds, the model is underfitting - Cross-validation is a useful technique for assessing model performance, but significant variation in performance across folds is more indicative of data variance rather than underfitting. Underfitting is generally identified when the model performs poorly on both the training and validation sets.

Analyze the model’s performance on a separate test set; if the model performs well on both the training and test sets, it is neither overfitting nor underfitting - Analyzing performance on a test set is important, but it only confirms the final model’s ability to generalize. If the model performs well on both training and test data, it likely indicates a good fit, but this does not help identify overfitting or underfitting during the training process itself.

Reference:

https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html

https://aws.amazon.com/what-is/overfitting/

Temática
ML Model Development
Pregunta 22

You are responsible for deploying a machine learning model on AWS SageMaker for a real-time prediction application. The application requires low latency and high throughput. During deployment, you notice that the model’s response time is slower than expected, and the throughput is not meeting the required levels. You have already optimized the model itself, so the next step is to optimize the deployment environment. You are currently using a single instance of the ml.m5.large instance type with the default endpoint configuration.

Which of the following changes is MOST LIKELY to improve the model’s response time and throughput?

Change the instance type to ml.p2.xlarge and add multi-model support

Respuesta correcta
Enable Auto Scaling with a target metric for the instance utilization

Switch to an ml.m5.2xlarge instance type and use multi-AZ deployment

Increase the instance count to two and enable asynchronous inference

Explicación general
Correct option:

Enable Auto Scaling with a target metric for the instance utilization

Amazon SageMaker supports automatic scaling (auto scaling) for your hosted models. Auto scaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.

Enabling Auto Scaling allows the endpoint to dynamically adjust the number of instances based on actual traffic. By targeting instance utilization, the deployment can automatically scale out during peak times and scale in during low demand, improving both response time and throughput without over-provisioning. With target tracking, you choose an Amazon CloudWatch metric and target value. Auto scaling creates and manages the CloudWatch alarms for the scaling policy and calculates the scaling adjustment based on the metric and the target value. The policy adds and removes the number of instances as required to keep the metric at, or close to, the specified target value. For example, a scaling policy that uses the predefined InvocationsPerInstance metric with a target value of 70 can keep InvocationsPerInstance at, or close to 70.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

Incorrect options:

Change the instance type to ml.p2.xlarge and add multi-model support - While changing to an ml.p2.xlarge instance type, which is optimized for GPU, could improve performance for compute-intensive models, it may not be necessary for all types of models, especially if the model is CPU-bound. Adding multi-model support may further complicate the deployment without addressing the core issue of latency and throughput. Multi-model endpoints provide a scalable and cost-effective solution to deploying large numbers of models. They use the same fleet of resources and a shared serving container to host all of your models. This reduces hosting costs by improving endpoint utilization compared with using single-model endpoints. It also reduces deployment overhead because Amazon SageMaker manages loading models in memory and scaling them based on the traffic patterns to your endpoint.

The following diagram shows how multi-model endpoints work compared to single-model endpoints.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

Increase the instance count to two and enable asynchronous inference - Asynchronous inference is typically used when latency is less of a concern, which contradicts the requirements of real-time prediction. Increasing the instance count without addressing scalability could help throughput but may not effectively reduce latency.

Switch to an ml.m5.2xlarge instance type and use multi-AZ deployment - Switching to a more powerful ml.m5.2xlarge instance type and using multi-AZ deployment could improve performance, but this option mainly adds redundancy and fault tolerance rather than optimizing response time and throughput directly.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 23

You are an ML Engineer developing a model to predict credit card fraud for a financial institution. The dataset you have is extensive, containing millions of transactions, but it is highly imbalanced, with only a small fraction representing fraudulent transactions. To ensure your model generalizes well to unseen data, you need to split your dataset effectively into training, validation, and test sets. You must also avoid overfitting and ensure that the model's performance is accurately measured during development.

Which of the following strategies should you employ to effectively utilize the training, validation, and test sets to develop a robust and reliable machine learning model, given the imbalanced nature of the dataset? (Select two)

Selección correcta
Split the dataset into three sets: a training set for model learning, a validation set for hyperparameter tuning, and a test set for final performance evaluation on unseen data

Selección correcta
Use a stratified split to ensure that the training, validation, and test sets each contain a representative distribution of fraudulent and non-fraudulent transactions

Randomly shuffle the data and split it into training, validation, and test sets without considering the class distribution to avoid any bias

Combine the validation and test sets into a single evaluation set to simplify the process and reduce the number of data splits

Use the entire dataset for training to maximize the amount of data the model learns from, and evaluate the model’s performance using cross-validation

Explicación general
Correct options:

Split the dataset into three sets: a training set for model learning, a validation set for hyperparameter tuning, and a test set for final performance evaluation on unseen data

Splitting the dataset into training, validation, and test sets is a standard practice in machine learning. The training set is used to train the model, the validation set is used for hyperparameter tuning and model selection, and the test set is used for the final evaluation of the model’s performance on unseen data. This approach helps to prevent overfitting and provides an unbiased evaluation of the model.

Use a stratified split to ensure that the training, validation, and test sets each contain a representative distribution of fraudulent and non-fraudulent transactions

Given the imbalanced nature of the dataset, a stratified split is essential to ensure that each set (training, validation, and test) contains a similar distribution of the classes (fraudulent and non-fraudulent transactions). This prevents the model from learning misleading patterns and ensures that performance metrics are reliable.

 via - https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/

Incorrect options:

Use the entire dataset for training to maximize the amount of data the model learns from, and evaluate the model’s performance using cross-validation - Using the entire dataset for training without a separate test set can lead to overfitting, as the model’s performance would be evaluated on data it has already seen. Cross-validation alone cannot replace the need for a final test set for unbiased evaluation.

Randomly shuffle the data and split it into training, validation, and test sets without considering the class distribution to avoid any bias - Randomly shuffling the data without considering the class distribution can lead to imbalanced splits, where one or more sets may not have enough examples of the minority class (fraudulent transactions). This would make it difficult to accurately assess the model's performance.

Combine the validation and test sets into a single evaluation set to simplify the process and reduce the number of data splits - Combining the validation and test sets is not recommended because it blurs the distinction between tuning and evaluation. The test set should remain unseen until the final evaluation to provide an unbiased assessment of the model’s performance.

Reference:

https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/

Temática
ML Model Development
Pregunta 24

Which of the following summarizes the differences between a token and an embedding in the context of generative AI?

An embedding is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, a token is a vector of numerical values that represents condensed information obtained by transforming input into that vector

Both token and embedding refer to a sequence of characters that a model can interpret or predict as a single unit of meaning

Respuesta correcta
A token is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, an embedding is a vector of numerical values that represents condensed information obtained by transforming input into that vector

Both token and embedding refer to a vector of numerical values that represents condensed information obtained by transforming input into that vector

Explicación general
Correct option:

A token is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, an embedding is a vector of numerical values that represents condensed information obtained by transforming input into that vector

Embedding – The process of condensing information by transforming input into a vector of numerical values, known as the embeddings, in order to compare the similarity between different objects by using a shared numerical representation. For example, sentences can be compared to determine the similarity in meaning, images can be compared to determine visual similarity, or text and image can be compared to see if they're relevant to each other.

Token – A sequence of characters that a model can interpret or predict as a single unit of meaning. For example, with text models, a token could correspond not just to a word, but also to a part of a word with grammatical meaning (such as "-ed"), a punctuation mark (such as "?"), or a common phrase (such as "a lot").

 via - https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html

Incorrect options:

Both token and embedding refer to a sequence of characters that a model can interpret or predict as a single unit of meaning

An embedding is a sequence of characters that a model can interpret or predict as a single unit of meaning, whereas, a token is a vector of numerical values that represents condensed information obtained by transforming input into that vector

Both token and embedding refer to a vector of numerical values that represents condensed information obtained by transforming input into that vector

These three options contradict the explanation provided above, so these options are incorrect.

References:

Temática
Data Preparation for Machine Learning (ML)
Pregunta 25

A company uses a generative model to analyze animal images in the training dataset to record variables like different ear shapes, eye shapes, tail features, and skin patterns.

Which of the following tasks can the generative model perform?

The model can classify multiple species of animals such as cats, dogs, etc

The model can identify any image from the training dataset

The model can classify a single species of animals such as cats

Respuesta correcta
The model can recreate new animal images that were not in the training dataset

Explicación general
Correct option:

The model can recreate new animal images that were not in the training dataset

Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. AI technologies attempt to mimic human intelligence in nontraditional computing tasks like image recognition, natural language processing (NLP), and translation.

Generative models can analyze animal images to record variables like different ear shapes, eye shapes, tail features, and skin patterns. They learn features and their relations to understand what different animals look like in general. They can then recreate new animal images that were not in the training set.

 via - https://aws.amazon.com/what-is/generative-ai/

Incorrect options:

The model can classify a single species of animals such as cats

The model can classify multiple species of animals such as cats, dogs, etc

Traditional machine learning models were discriminative or focused on classifying data points. They attempted to determine the relationship between known and unknown factors. For example, they look at images—known data like pixel arrangement, line, color, and shape—and map them to words—the unknown factor. Only discriminative models can act as single-class classifiers or multi-class classifiers. Therefore, both these options are incorrect.

The model can identify any image from the training dataset - This option has been added as a distractor. A generative model is not an image-matching algorithm. It cannot identify an image from the training dataset.

Reference:

https://aws.amazon.com/what-is/generative-ai/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 26

You are working as a data scientist at a company that specializes in predictive analytics. You are tasked with training a deep learning model using Amazon SageMaker to predict customer churn. The dataset you have is large and contains millions of records. The training process is taking longer than expected, and you suspect that the hyperparameters need fine-tuning. You want to balance the training time while ensuring the model converges effectively. You have set the batch size to 256, epochs to 50, and learning rate to 0.01. However, the training job is still not performing as expected.

Given this scenario, which of the following adjustments is MOST LIKELY to reduce the training time without compromising model performance?

Increase the number of epochs and maintain the batch size

Decrease the batch size and increase the number of epochs

Respuesta correcta
Increase the batch size and decrease the number of epochs

Decrease the learning rate and increase the batch size

Explicación general
Correct option:

Increase the batch size and decrease the number of epochs

Hyperparameters are external configuration variables that data scientists use to manage machine learning model training. Sometimes called model hyperparameters, the hyperparameters are manually set before training a model. They're different from parameters, which are internal parameters automatically derived during the learning process and not set by data scientists.

 via - https://aws.amazon.com/what-is/hyperparameter-tuning/

Increasing the batch size allows more data to be processed in parallel, which can lead to faster training times because the model updates its weights less frequently. Reducing the number of epochs compensates for the larger batch size by ensuring the model doesn't overfit. This combination can significantly reduce training time while still allowing the model to converge effectively, provided the learning rate remains optimal.

Optimize the learning process of your models with hyperparameters:

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html

Incorrect options:

Decrease the batch size and increase the number of epochs - Decreasing the batch size means the model will update weights more frequently, but this will increase the training time due to smaller batches. Increasing the number of epochs will further slow down the training, which is counterproductive given the goal of reducing training time.

Increase the number of epochs and maintain the batch size - Increasing the number of epochs without adjusting the batch size could lead to longer training times, potentially causing overfitting without necessarily improving performance.

Decrease the learning rate and increase the batch size - Decreasing the learning rate typically results in a more stable but slower convergence process. Coupling this with an increased batch size may unnecessarily prolong training time without a significant gain in model performance.

References:

https://aws.amazon.com/what-is/hyperparameter-tuning/

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-llms-finetuning-hyperparameters.html

Temática
ML Model Development
Pregunta 27

You are a data scientist working on a machine learning project to predict customer lifetime value (CLV) for an e-commerce company. Before deploying a complex model like a deep neural network, you need to establish a performance baseline to measure the effectiveness of your advanced models. You decide to use Amazon SageMaker to create this baseline efficiently. The goal is to build a simple model that can be easily implemented and provide a reference point for evaluating the performance of more sophisticated models later.

Which of the following approaches is the MOST EFFECTIVE for creating a performance baseline using Amazon SageMaker?

Deploy a SageMaker BlazingText model to create word embeddings from customer reviews, which can be used as a baseline for evaluating CLV predictions

Implement SageMaker Autopilot to automatically explore various models and select the best one as the baseline, allowing you to skip manual model selection

Respuesta correcta
Train a basic linear learner model using Amazon SageMaker, focusing on key features like customer age, purchase frequency, and average order value, to establish a baseline for CLV prediction

Use SageMaker JumpStart to deploy a pre-trained model for customer segmentation, which can serve as a baseline for your CLV prediction model

Explicación general
Correct option:

Train a basic linear learner model using Amazon SageMaker, focusing on key features like customer age, purchase frequency, and average order value, to establish a baseline for CLV prediction

Using SageMaker’s linear learner algorithm is an effective approach for creating a simple and interpretable baseline. This method allows you to establish a performance benchmark using key features that are directly related to predicting CLV. The linear learner is quick to train and provides a clear point of comparison for more complex models.

Incorrect options:

Use SageMaker JumpStart to deploy a pre-trained model for customer segmentation, which can serve as a baseline for your CLV prediction model - SageMaker JumpStart is great for quickly deploying pre-built models, but a pre-trained customer segmentation model is not directly aligned with predicting CLV. It would not serve as an appropriate baseline for this specific task.

Implement SageMaker Autopilot to automatically explore various models and select the best one as the baseline, allowing you to skip manual model selection - SageMaker Autopilot can automatically explore and select the best models, but it typically aims for optimal performance rather than establishing a simple baseline. While Autopilot is useful, starting with a basic model like linear learner helps you understand the incremental value of more complex models.

Deploy a SageMaker BlazingText model to create word embeddings from customer reviews, which can be used as a baseline for evaluating CLV predictions - BlazingText is designed for natural language processing tasks like text classification and word embedding, which are not directly applicable to creating a CLV prediction baseline. It’s not the right tool for this use case.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html

https://aws.amazon.com/sagemaker/jumpstart/

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html

Temática
ML Model Development
Pregunta 28

You are a Machine Learning Operations (MLOps) Engineer at a large technology company that runs multiple machine learning workloads across different environments. Your company has a variety of ML use cases, including continuous real-time predictions, scheduled batch processing for weekly model retraining, and small-scale experimentation with multiple hyperparameter tuning jobs that can tolerate failure.

Which of the following strategies represents the best use of spot instances, on-demand instances, and reserved instances for different machine learning workloads, considering the requirements for cost optimization, reliability, and performance? (Select two)

Selección correcta
Use spot instances for hyperparameter tuning jobs where interruptions can be tolerated

Selección correcta
Use reserved instances for real-time predictions that require high availability

Use on-demand instances for hyperparameter tuning jobs where interruptions can be tolerated

Use reserved instances for scheduled batch processing for model retraining

Use on-demand instances for real-time predictions that require high availability

Explicación general
Correct options:

Use spot instances for hyperparameter tuning jobs where interruptions can be tolerated

Spot instances are well-suited for hyperparameter tuning jobs where interruptions are acceptable, as they offer significant cost savings but may be terminated by AWS if the capacity is needed elsewhere.

Use reserved instances for real-time predictions that require high availability

Reserved instances are the best choice for real-time predictions that require high availability, as they ensure that the necessary resources are always available while optimizing costs over time.

 via - https://aws.amazon.com/ec2/pricing/

Incorrect options:

Use on-demand instances for real-time predictions that require high availability - Although on-demand instances can certainly be used for real-time predictions that require high availability, however, reserved instances are a better fit, as they are more cost-effective.

Use on-demand instances for hyperparameter tuning jobs where interruptions can be tolerated - Although on-demand instances can certainly be used for hyperparameter tuning jobs where interruptions can be tolerated, however, spot instances are a better fit, as they are more cost-effective.

Use reserved instances for scheduled batch processing for model retraining - Since the model re-training is done on a weekly schedule, so on-demand instances are a better fit compared to the reserved instances.

References:

https://aws.amazon.com/ec2/pricing/

https://aws.amazon.com/ec2/pricing/reserved-instances/

https://aws.amazon.com/ec2/spot/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 29

You are a Senior ML Engineer at a global logistics company that heavily relies on machine learning models for optimizing delivery routes, predicting demand, and detecting anomalies in real-time. The company is rapidly expanding, and you are tasked with building a maintainable, scalable, and cost-effective ML infrastructure that can handle increasing data volumes and evolving model requirements. You must implement best practices to ensure that the infrastructure can support ongoing development, deployment, monitoring, and scaling of multiple models across different regions.

Which of the following strategies should you implement to create a maintainable, scalable, and cost-effective ML infrastructure for your company using AWS services? (Select three)

Use a monolithic architecture to manage all machine learning models in a single environment, simplifying management and reducing overhead

Selección correcta
Utilize infrastructure as code (IaC) with AWS CloudFormation to automate the deployment and management of ML resources, making it easy to replicate and scale infrastructure across regions

Store all model artifacts and data in Amazon CodeCommit for version control and managing changes over time

Selección correcta
Implement a microservices-based architecture with Amazon SageMaker endpoints, where each model is deployed independently, allowing for isolated scaling and updates

Provision fixed resources for each model to avoid unexpected costs, ensuring that the infrastructure is always available for each model

Selección correcta
Store all model artifacts and data in Amazon S3, and use versioning to manage changes over time, ensuring that models can be easily rolled back if needed

Explicación general
Correct options:

Implement a microservices-based architecture with Amazon SageMaker endpoints, where each model is deployed independently, allowing for isolated scaling and updates

A microservices-based architecture with Amazon SageMaker endpoints allows each model to be deployed, managed, and scaled independently. This approach enhances maintainability by isolating different components, making it easier to update models or scale specific services without affecting others. It also supports a more scalable and flexible infrastructure.

Utilize infrastructure as code (IaC) with AWS CloudFormation to automate the deployment and management of ML resources, making it easy to replicate and scale infrastructure across regions

Utilizing infrastructure as code (IaC) with AWS CloudFormation enables you to automate the deployment and management of your ML infrastructure. This approach ensures consistency across environments, simplifies scaling, and allows for rapid deployment in multiple regions. IaC also enhances maintainability by providing a version-controlled, repeatable process for managing infrastructure changes.

Store all model artifacts and data in Amazon S3, and use versioning to manage changes over time, ensuring that models can be easily rolled back if needed

Storing model artifacts and data in Amazon S3 with versioning is a good practice for maintaining model history and enabling rollbacks.

Incorrect options:

Use a monolithic architecture to manage all machine learning models in a single environment, simplifying management and reducing overhead - A monolithic architecture can simplify management in the short term but becomes difficult to maintain and scale as the number of models and services grows. It also limits flexibility in updating or scaling individual models, leading to potential bottlenecks and higher costs.

Provision fixed resources for each model to avoid unexpected costs, ensuring that the infrastructure is always available for each model - Provisioning fixed resources for each model may lead to underutilization or overprovisioning, resulting in higher costs. Dynamic resource allocation, such as using auto-scaling or spot instances, is generally more cost-effective and scalable.

Store all model artifacts and data in Amazon CodeCommit for version control and managing changes over time - Amazon CodeCommit is the right fit for code-specific version control. You should not use CodeCommit to store model related data.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cloudformation-overview.html

https://aws.amazon.com/codecommit/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 30

A telecommunications company receives real-time network performance data streams from thousands of devices across its infrastructure. The data streams consist of thousands of JSON records per second, detailing metrics like latency, packet loss, and jitter. The company needs to implement a scalable solution on AWS to monitor these metrics and detect anomalies in real time with the least operational overhead.

Which solution will meet these requirements?

Use Amazon Kinesis Firehose for real-time streaming of the network metrics data into an Amazon S3 bucket, then periodically run an AWS Glue ETL job to detect anomalies using a custom Python script

Respuesta correcta
Use Amazon Kinesis Data Streams to ingest the data, process it with Apache Flink to analyze the streams in real time, and apply the RANDOM_CUT_FOREST algorithm to detect anomalies

Ingest the data using Amazon Kinesis Data Streams, analyze the network metrics using Amazon Comprehend, and identify anomalies with a SageMaker model

Use AWS Lambda to process the network metrics streams in real time and write a custom function to detect anomalies directly from the data

Explicación general
Correct option:

Use Amazon Kinesis Data Streams to ingest the data, process it with Apache Flink to analyze the streams in real time, and apply the RANDOM_CUT_FOREST algorithm to detect anomalies

This solution uses AWS services designed for real-time processing and anomaly detection:

Amazon Kinesis Data Streams - Provides a scalable, low-latency service for ingesting high-volume, real-time network metrics.

Apache Flink on Kinesis Data Analytics - Processes the data streams in real time and offers powerful frameworks for data analysis.

RANDOM_CUT_FOREST algorithm - A built-in anomaly detection algorithm within Apache Flink that minimizes the need for custom code or external models.

This combination handles high-throughput data streams and detects anomalies efficiently, with minimal operational overhead.

Incorrect options:

Use Amazon Kinesis Firehose for real-time streaming of the network metrics data into an Amazon S3 bucket, then periodically run an AWS Glue ETL job to detect anomalies using a custom Python script - Kinesis Firehose delivers data in real-time to destinations like Amazon S3 but using AWS Glue for periodic jobs introduces latency and is unsuitable for real-time anomaly detection.

Ingest the data using Amazon Kinesis Data Streams, analyze the network metrics using Amazon Comprehend, and identify anomalies with a SageMaker model - Amazon Comprehend is a natural language processing tool and cannot analyze numerical or structured data like network metrics. Additionally, SageMaker for real-time analysis adds unnecessary complexity compared to Apache Flink’s built-in capabilities.

Use AWS Lambda to process the network metrics streams in real time and write a custom function to detect anomalies directly from the data - While Lambda can handle real-time data, it is not optimized for high-throughput or resource-intensive computations like anomaly detection. Scaling Lambda for such tasks adds operational complexity.

References:

https://aws.amazon.com/blogs/big-data/real-time-anomaly-detection-via-random-cut-forest-in-amazon-managed-service-for-apache-flink/

https://aws.amazon.com/managed-service-apache-flink/

https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html

Temática
ML Model Development
Pregunta 31

You are a machine learning engineer working for an e-commerce company. You have developed a recommendation model that predicts products customers are likely to buy based on their browsing history and past purchases. The model initially performs well, but after deploying it in production, you notice two issues: the model's performance degrades over time as new data is added (catastrophic forgetting) and the model shows signs of overfitting during retraining on updated datasets.

Given these challenges, which of the following strategies is the MOST LIKELY to help prevent overfitting and catastrophic forgetting while maintaining model accuracy?

Regularly update the training dataset with new data, apply L2 regularization to manage overfitting, and use an ensemble of models to prevent catastrophic forgetting

Respuesta correcta
Use early stopping during training to prevent overfitting, incorporate new data incrementally through transfer learning to mitigate catastrophic forgetting, and apply L1 regularization to ensure feature selection

Reduce the model complexity by decreasing the number of features, apply data augmentation to handle underfitting, and leverage L1 regularization to address catastrophic forgetting

Apply L2 regularization to reduce overfitting, use dropout to prevent underfitting, and retrain the model on the entire dataset periodically to avoid catastrophic forgetting

Explicación general
Correct option:

Use early stopping during training to prevent overfitting, incorporate new data incrementally through transfer learning to mitigate catastrophic forgetting, and apply L1 regularization to ensure feature selection

Early stopping is a proven method to prevent overfitting by halting training when the model’s performance on the validation set stops improving. Incorporating new data incrementally through transfer learning helps to mitigate catastrophic forgetting by allowing the model to learn new information while retaining its prior knowledge.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html

Regularization helps prevent linear models from overfitting training data examples by penalizing extreme weight values. L1 regularization reduces the number of features used in the model by pushing the weight of features that would otherwise have very small weights to zero. L1 regularization produces sparse models and reduces the amount of noise in the model. L2 regularization results in smaller overall weight values, which stabilizes the weights when there is high correlation between the features.

L1 regularization is beneficial for feature selection, which can improve model generalization and prevent both overfitting and underfitting.

 via - https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/

Incorrect options:

Apply L2 regularization to reduce overfitting, use dropout to prevent underfitting, and retrain the model on the entire dataset periodically to avoid catastrophic forgetting - While L2 regularization is effective for reducing overfitting, using dropout typically addresses overfitting, not underfitting. Retraining the model on the entire dataset periodically may help, but it could still lead to catastrophic forgetting as the model might "forget" previous knowledge when learning from new data.

Reduce the model complexity by decreasing the number of features, apply data augmentation to handle underfitting, and leverage L1 regularization to address catastrophic forgetting - Reducing model complexity can help with overfitting, but it might lead to underfitting if too many features are removed. Data augmentation is more relevant for addressing overfitting, particularly in image or text data, rather than underfitting. L1 regularization is effective for reducing overfitting, and not for addressing catastrophic forgetting.

Regularly update the training dataset with new data, apply L2 regularization to manage overfitting, and use an ensemble of models to prevent catastrophic forgetting - Regularly updating the dataset can help with model performance over time, but it might not address catastrophic forgetting unless combined with techniques like transfer learning. L2 regularization and ensembling are useful strategies, but ensembling does not specifically prevent catastrophic forgetting.

References:

https://aws.amazon.com/blogs/machine-learning/automatically-retrain-neural-networks-with-renate/

https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters.html

https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html

Temática
ML Model Development
Pregunta 32

You are a data scientist at an e-commerce company working to develop a recommendation system for customers. After building several models, including collaborative filtering, content-based filtering, and a deep learning model, you find that each model excels in different scenarios. For example, the collaborative filtering model works well for returning customers with rich interaction data, while the content-based filtering model performs better for new customers with little interaction history. Your goal is to combine these models to create a recommendation system that provides more accurate and personalized recommendations across all customer segments.

Which of the following strategies is the MOST LIKELY to achieve this goal?

Use a bagging approach to train multiple instances of the deep learning model on different subsets of the data and average their predictions to improve overall performance

Use stacking, where the predictions from the collaborative filtering and content-based filtering models are fed into a deep learning model as inputs, allowing the deep learning model to make the final recommendation

Respuesta correcta
Implement a hybrid model that combines the predictions of collaborative filtering, content-based filtering, and deep learning using a weighted average, where weights are based on model performance for different customer segments

Apply boosting by sequentially training the collaborative filtering, content-based filtering, and deep learning models, where each model corrects the errors of the previous one

Explicación general
Correct option:

Implement a hybrid model that combines the predictions of collaborative filtering, content-based filtering, and deep learning using a weighted average, where weights are based on model performance for different customer segments

 via - https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/

In bagging, data scientists improve the accuracy of weak learners by training several of them at once on multiple datasets. In contrast, boosting trains weak learners one after another. Stacking involves training a meta-model on the predictions of several base models. This approach can significantly improve performance because the meta-model learns to leverage the strengths of each base model while compensating for their weaknesses.

For the given use case, a hybrid model that combines the predictions of different models using a weighted average is the most appropriate approach. By assigning weights based on each model’s performance for specific customer segments, you can ensure that the recommendation system leverages the strengths of each model, providing more accurate and personalized recommendations across all customer segments.

Incorrect options:

Use a bagging approach to train multiple instances of the deep learning model on different subsets of the data and average their predictions to improve overall performance - Bagging is typically used to reduce variance and improve stability for a single type of model, like decision trees. However, it does not directly address the need to combine different models that perform well in different scenarios, which is key for your recommendation system.

Apply boosting by sequentially training the collaborative filtering, content-based filtering, and deep learning models, where each model corrects the errors of the previous one - Boosting is useful for improving the performance of weak learners by training them sequentially, but it is not designed to combine different types of models like collaborative filtering, content-based filtering, and deep learning, each of which has strengths in different areas.

Use stacking, where the predictions from the collaborative filtering and content-based filtering models are fed into a deep learning model as inputs, allowing the deep learning model to make the final recommendation - Stacking is a powerful technique for combining models, but in this case, the deep learning model is not necessarily better suited as a meta-model for making the final recommendation. A weighted hybrid model is more effective when different models excel in different contexts, as it allows you to balance their contributions based on performance.

References:

https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/

https://aws.amazon.com/what-is/boosting/

Temática
ML Model Development
Pregunta 33

A financial services company is building a fraud detection model using Amazon SageMaker. The ML engineer receives a 40 MB Apache Parquet file as input data. The file contains several correlated columns that are not needed for the model. The engineer needs to drop these unnecessary columns with the least effort while ensuring the data remains compatible with SageMaker for further preprocessing and model training.

What should the ML engineer do?

Respuesta correcta
Use SageMaker Data Wrangler to import the Parquet file, drop the unnecessary columns, and save the transformed data

Convert the Parquet file to CSV, manually edit the file to remove the columns, and re-upload the file to SageMaker

Use AWS Glue to transform the Parquet file and drop the unnecessary columns before importing it into SageMaker

Write a custom Python script using pandas to load the Parquet file, drop the unnecessary columns, and save it back as a Parquet file

Explicación general
Correct option:

Use SageMaker Data Wrangler to import the Parquet file, drop the unnecessary columns, and save the transformed data

Amazon SageMaker Data Wrangler (Data Wrangler) is a feature of Amazon SageMaker Studio Classic that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering (including dropping unused columns) using little to no coding.

Incorrect options:

Write a custom Python script using pandas to load the Parquet file, drop the unnecessary columns, and save it back as a Parquet file - While pandas can handle Parquet files, this approach requires custom scripting, which is more effort than using a built-in tool like SageMaker Data Wrangler designed for such tasks.

Convert the Parquet file to CSV, manually edit the file to remove the columns, and re-upload the file to SageMaker - Manually editing a file is time-consuming and error-prone. It also adds unnecessary complexity by converting the file format twice (Parquet to CSV and back).

Use AWS Glue to transform the Parquet file and drop the unnecessary columns before importing it into SageMaker - AWS Glue can perform this transformation, but it is a more complex and time-consuming process compared to SageMaker Data Wrangler, especially for a single file of this size.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-getting-started.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 34

You are a machine learning engineer responsible for managing a fraud detection model deployed on Amazon SageMaker. The model needs to be retrained periodically as new transaction data becomes available and as data distribution shifts over time. To ensure compliance and traceability, the company requires that all re-training activities, including data access and model updates, be logged and monitored. Additionally, the security team wants to be notified of any unauthorized attempts to retrain the model.

How can you use AWS CloudTrail to meet these guidelines?

Respuesta correcta
Enable AWS CloudTrail to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected

Enable detailed monitoring in Amazon CloudWatch to track all SageMaker activities, and use CloudWatch Logs to store and analyze the logs for any suspicious activity related to re-training

Enable AWS CloudWatch to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected

Configure AWS CloudTrail to log only the data access events that trigger re-training, and manually trigger model re-training when changes in the data are detected through these logs

Explicación general
Correct option:

Enable AWS CloudTrail to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected

This option leverages AWS CloudTrail to log all relevant API activities related to SageMaker, providing full traceability of re-training events and any updates to the model. By configuring CloudWatch Alarms, you can automatically notify the security team if any suspicious or unauthorized actions are detected. This approach meets both compliance and operational requirements.

 via - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html

Incorrect options:

Configure AWS CloudTrail to log only the data access events that trigger re-training, and manually trigger model re-training when changes in the data are detected through these logs - Monitoring only data access events in S3 is insufficient because it misses other critical re-training activities, such as creating training jobs or updating endpoints. Manual log reviews are prone to errors and delay in detecting unauthorized activities.

Enable detailed monitoring in Amazon CloudWatch to track all SageMaker activities, and use CloudWatch Logs to store and analyze the logs for any suspicious activity related to re-training - While CloudWatch is useful for performance monitoring, CloudTrail is specifically designed to log and monitor API activity across AWS services, making it more appropriate for tracking re-training activities and ensuring compliance.

Enable AWS CloudWatch to log all API calls related to SageMaker, including CreateTrainingJob and UpdateEndpoint, and configure CloudWatch Alarms to notify the security team if unauthorized API calls are detected - You can only use CloudTrail to log and monitor API activity across AWS services. CloudWatch is useful for performance monitoring. So, this option acts as a distractor.

References:

https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html

https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 35

Which of the following highlights the differences between model parameters and hyperparameters in the context of generative AI?

Both Hyperparameters and model parameters are values that define a model and its behavior in interpreting input and generating responses

Hyperparameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are values that can be adjusted for model customization to control the training process

Both Hyperparameters and model parameters are values that can be adjusted for model customization to control the training process

Respuesta correcta
Model parameters are values that define a model and its behavior in interpreting input and generating responses. Hyperparameters are values that can be adjusted for model customization to control the training process

Explicación general
Correct option:

Model parameters are values that define a model and its behavior in interpreting input and generating responses. Hyperparameters are values that can be adjusted for model customization to control the training process

Hyperparameters are values that can be adjusted for model customization to control the training process and, consequently, the output custom model. In other words, hyperparameters are external configurations set before the training process begins. They control the training process and the structure of the model but are not adjusted by the training algorithm itself. Examples include the learning rate, the number of layers in a neural network, etc.

Model parameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are controlled and updated by providers. You can also update model parameters to create a new model through the process of model customization. In other words, Model parameters are the internal variables of the model that are learned and adjusted during the training process. These parameters directly influence the output of the model for a given input. Examples include the weights and biases in a neural network.

 via - https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html

Incorrect options:

Both Hyperparameters and model parameters are values that can be adjusted for model customization to control the training process

Both Hyperparameters and model parameters are values that define a model and its behavior in interpreting input and generating responses

Hyperparameters are values that define a model and its behavior in interpreting input and generating responses. Model parameters are values that can be adjusted for model customization to control the training process

These three options contradict the explanation provided above, so these options are incorrect.

Reference:

https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 36

A healthcare organization is developing a deep learning model on Amazon SageMaker to predict patient outcomes. The training dataset is substantial, and the organization aims to optimize the model's hyperparameters to minimize the validation dataset's loss function.

Which hyperparameter tuning strategy should the organization use to achieve this goal with the least computational effort?

Grid search

Gradient descent

Random Forest

Respuesta correcta
Hyperband

Explicación general
Correct option:

Hyperband

Hyperband is a multi-fidelity based tuning strategy that dynamically reallocates resources. Hyperband uses both intermediate and final results of training jobs to re-allocate epochs to well-utilized hyperparameter configurations and automatically stops those that underperform. It also seamlessly scales to using many parallel training jobs. These features can significantly speed up hyperparameter tuning over random search and Bayesian optimization strategies.

By allocating fewer resources to underperforming configurations, Hyperband reduces the total computation time compared to exhaustive methods like grid search.

Incorrect options:

Grid search - With grid search, you specify a list of hyperparameters and a performance metric, and the algorithm works through all possible combinations to determine the best fit. Grid search works well, but it’s relatively tedious and computationally intensive, especially with large numbers of hyperparameters.

Gradient descent - Gradient descent is not a hyperparameter tuning strategy for a deep learning model because it is an optimization algorithm used to minimize the loss function by adjusting model parameters (weights and biases) during training. Hyperparameter tuning, on the other hand, involves selecting optimal values for parameters that are not learned during training, such as learning rate, batch size, number of layers, or activation functions. While gradient descent facilitates the learning process by guiding how the model's weights are updated, hyperparameter tuning determines the settings that define the model's architecture and training process.

Random Forest - It is a machine learning algorithm used for classification and regression tasks. It is an ensemble learning method that builds multiple decision trees and merges their results (e.g., via averaging or voting) to improve predictive accuracy and control overfitting. Note that 'random search' is a hyperparameter optimization (HPO) technique in Amazon SageMaker. It selects random combinations of hyperparameters within predefined ranges and evaluates the model’s performance for each combination.

References:

https://aws.amazon.com/what-is/hyperparameter-tuning/

https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html

Temática
ML Model Development
Pregunta 37

You are tasked with preparing a dataset for a machine learning model using SageMaker Data Wrangler. You need to perform a transformation that removes outliers from your dataset.

Which of the following actions in SageMaker Data Wrangler would you use to accomplish this task?

One-hot Encode

Format string

Respuesta correcta
Filter

Impute

Explicación general
Correct option:

Filter

You can filter the data in your columns using Data Wrangler. For a column that has the categories, male and female, you can filter out all the male values or all the female values.

You can also add multiple filters. The filters can be applied across multiple columns or the same column. For example, if you're creating a column that only has values within a certain range, you add two different filters. One filter specifies that the column must have values greater than the value that you provide. The other filter specifies that the column must have values less than the value that you provide.

For the given use case, Filter transform will help remove outliers.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html#data-wrangler-transform-filter-data

Incorrect options:

Impute - The Impute missing transform is used to create a new column that contains imputed values when missing values are found in input categorical or numerical data. Impute cannot remove outliers.

One-hot Encode - Encoding categorical data is the process of creating a numerical representation for categories. One-hot encode transformation helps encode data but cannot remove outliers.

Format string - The Format string transforms contain standard string formatting operations. For example, you can use these operations to remove special characters, normalize string lengths, and update string casing. The format string cannot remove outliers.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 38

You are a DevOps engineer responsible for maintaining a serverless machine learning application that provides real-time predictions using AWS Lambda. Recently, users have reported increased latency when interacting with the application, especially during peak usage hours. You need to quickly identify the root cause of the latency and resolve the performance issues to ensure the application remains responsive.

Which combination of monitoring and observability tools is the MOST EFFECTIVE for troubleshooting the latency and performance issues in this serverless application?

Respuesta correcta
Use AWS X-Ray to trace requests across the entire application, identify bottlenecks, and visualize the end-to-end latency for each request. Combine this with Amazon CloudWatch Lambda Insights to monitor the Lambda function’s memory usage, CPU usage, and invocation times

Enable detailed monitoring in Amazon CloudWatch to track Lambda invocations, errors, and throttles, and manually inspect the Lambda code to identify performance bottlenecks

Use Amazon CloudWatch Alarms to set thresholds for Lambda duration and error rates, and configure AWS X-Ray to periodically sample traces from the application for analysis

Deploy Amazon CloudWatch Logs Insights to query and analyze the application logs for errors, and use AWS Config to review recent changes to the infrastructure that might have introduced latency

Explicación general
Correct option:

Use AWS X-Ray to trace requests across the entire application, identify bottlenecks, and visualize the end-to-end latency for each request. Combine this with Amazon CloudWatch Lambda Insights to monitor the Lambda function’s memory usage, CPU usage, and invocation times

This approach leverages AWS X-Ray to trace the entire request path, providing detailed insights into where latency occurs, whether in the Lambda function, external APIs, or other integrated services. AWS X-Ray’s visualization helps identify bottlenecks and latency sources.

 via - https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html

Amazon CloudWatch Lambda Insights provides detailed metrics on Lambda function performance, including memory usage, CPU usage, and invocation times, allowing you to pinpoint performance issues specific to the Lambda environment. This combination is powerful for diagnosing and resolving latency issues in a serverless architecture.

 via - https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html

Incorrect options:

Enable detailed monitoring in Amazon CloudWatch to track Lambda invocations, errors, and throttles, and manually inspect the Lambda code to identify performance bottlenecks - Detailed monitoring in Amazon CloudWatch provides useful metrics, but it lacks the deep trace analysis that AWS X-Ray offers. Manually inspecting code is time-consuming and may not accurately identify the performance bottlenecks.

Deploy Amazon CloudWatch Logs Insights to query and analyze the application logs for errors, and use AWS Config to review recent changes to the infrastructure that might have introduced latency - Amazon CloudWatch Logs Insights is effective for analyzing logs, but it doesn’t provide the same end-to-end tracing capabilities as AWS X-Ray. AWS Config is useful for tracking infrastructure changes, but it may not directly help identify latency issues within the application.

Use Amazon CloudWatch Alarms to set thresholds for Lambda duration and error rates, and configure AWS X-Ray to periodically sample traces from the application for analysis - While setting alarms for Lambda duration and error rates is important, relying on periodically sampled traces with AWS X-Ray may miss intermittent issues. Continuous tracing and monitoring with AWS X-Ray and Lambda Insights offer a more comprehensive solution.

References:

https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Lambda-Insights.html

https://docs.aws.amazon.com/xray/latest/devguide/xray-gettingstarted.html

https://aws.amazon.com/blogs/aws/aws-lambda-support-for-aws-x-ray/

https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch-metrics-basic-detailed.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 39

Your company is starting a new machine learning project, and the data preparation tasks are being handled by a team of business analysts. These analysts are more comfortable working with visual tools rather than writing code, and they need to combine, transform, and clean large datasets efficiently. The goal is to use a SageMaker tool that allows them to perform these tasks using a visual, point-and-click interface.

Which SageMaker tool(s) would best suit the team's requirements for preparing and analyzing data without writing code?

Respuesta correcta
Use Amazon SageMaker Data Wrangler within Amazon SageMaker Canvas

Prepare data with SQL in Amazon SageMaker Studio. The SQL extension connections to Amazon Athena for working with datasets

Use Amazon SageMaker Data Wrangler within Amazon SageMaker JumpStart

Prepare data using Amazon EMR in Studio

Explicación general
Correct option:

Use Amazon SageMaker Data Wrangler within Amazon SageMaker Canvas

Use Amazon SageMaker Data Wrangler in Amazon SageMaker Canvas to prepare, featurize and analyze your data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering using little to no coding. You can also add your own Python scripts and transformations to customize workflows.

Data Flow – Create a data flow to define a series of ML data prep steps. You can use a flow to combine datasets from different data sources, identify the number and types of transformations you want to apply to datasets, and define a data prep workflow that can be integrated into an ML pipeline.

Transform – Clean and transform your dataset using standard transforms like string, vector, and numeric data formatting tools. Featurize your data using transforms like text and date/time embedding and categorical encoding.

Generate Data Insights – Automatically verify data quality and detect abnormalities in your data with Data Wrangler Data Quality and Insights Report.

Analyze – Analyze features in your dataset at any point in your flow. Data Wrangler includes built-in data visualization tools like scatter plots and histograms, as well as data analysis tools like target leakage analysis and quick modeling to understand feature correlation.

Export – Export your data preparation workflow to a different location.

SageMaker Canvas features for ML lifecycle:  via - https://aws.amazon.com/sagemaker/canvas/

Incorrect options:

Prepare data using Amazon EMR in Studio - The integration between Amazon EMR and Amazon SageMaker Studio provides a scalable environment for large-scale data preparation for machine learning. This option is viable only if users are comfortable writing code.

Use Amazon SageMaker Data Wrangler within Amazon SageMaker JumpStart - Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select FMs quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation. SageMaker JumpStart is not the right tool for the given use case.

Prepare data with SQL in Amazon SageMaker Studio. The SQL extension connections to Amazon Athena for working with datasets - Amazon SageMaker Studio provides a built-in SQL extension. This extension allows data scientists to perform tasks such as sampling, exploratory analysis, and feature engineering directly within the JupyterLab notebooks. However, this option does not offer any no-code features.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-data-prep.html

https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 40

You are a data scientist at a healthcare company working on deploying a machine learning model that predicts patient outcomes based on real-time data from wearable devices. The model needs to be containerized for easy deployment and scaling across different environments, including development, testing, and production. The company wants to ensure that container images are managed efficiently, securely, and consistently across all environments.

Given these requirements, which combination of AWS services is the MOST SUITABLE for building, storing, deploying, and maintaining the containerized ML solution?

Use Amazon ECS to manage and deploy the containerized model, Amazon S3 to store container images, and manually push updates to the containers using the AWS CLI

Use Docker Hub to store the container images, Amazon EKS for orchestrating the containers, and AWS Lambda to trigger updates to the containers when new images are pushed

Respuesta correcta
Use Amazon ECR to store the container images, Amazon EKS for orchestrating the containers, and AWS CodePipeline for automating the CI/CD pipeline, ensuring that updates to the model are seamlessly deployed

Use Amazon ECR to store container images, manually deploy containers on Amazon EC2 instances, and use AWS CloudFormation to manage the infrastructure configuration

Explicación general
Correct option:

Use Amazon ECR to store the container images, Amazon EKS for orchestrating the containers, and AWS CodePipeline for automating the CI/CD pipeline, ensuring that updates to the model are seamlessly deployed

Amazon ECR is a fully managed container registry that integrates seamlessly with Amazon EKS, allowing you to securely store and manage container images. Amazon EKS provides a scalable and managed Kubernetes environment for orchestrating containers. AWS CodePipeline can be used to automate the continuous integration and delivery (CI/CD) process, ensuring that new versions of the model are automatically built, tested, and deployed without manual intervention. This combination ensures consistency, security, and scalability across the ML solution.

 via - https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html

Incorrect options:

Use Amazon ECS to manage and deploy the containerized model, Amazon S3 to store container images, and manually push updates to the containers using the AWS CLI - While Amazon ECS is a powerful container management service, storing container images in Amazon S3 is not recommended since S3 is not optimized for container image storage. Additionally, manually pushing updates via the AWS CLI lacks automation and can introduce errors, making it less suitable for a production environment.

Use Amazon ECR to store container images, manually deploy containers on Amazon EC2 instances, and use AWS CloudFormation to manage the infrastructure configuration - Manually deploying containers on Amazon EC2 instances and managing infrastructure with AWS CloudFormation adds unnecessary complexity and management overhead. Using Amazon EKS for orchestration is more efficient and scalable for containerized workloads.

Use Docker Hub to store the container images, Amazon EKS for orchestrating the containers, and AWS Lambda to trigger updates to the containers when new images are pushed - Docker Hub is a widely used container registry, but for enterprise solutions on AWS, Amazon ECR is more secure and integrates better with other AWS services. Using AWS Lambda to trigger updates is unconventional and less efficient compared to using a CI/CD pipeline with AWS CodePipeline.

References:

https://docs.aws.amazon.com/whitepapers/latest/build-secure-enterprise-ml-platform/automation-pipelines.html

https://aws.amazon.com/blogs/machine-learning/build-a-ci-cd-pipeline-for-deploying-custom-machine-learning-models-using-aws-services/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 41

You are an ML engineer working for a logistics company that uses machine learning models to optimize delivery routes, predict maintenance needs, and forecast demand. The company wants to deploy several models into production, each serving different business functions but running on the same infrastructure to minimize costs. These models differ in the frequency of updates. The company is considering whether to use a multi-model deployment approach or a multi-container deployment approach on Amazon SageMaker to manage these models efficiently.

Given these requirements, which deployment strategy is MOST SUITABLE for managing these diverse models?

Respuesta correcta
Use a multi-model deployment on a single SageMaker endpoint to host all models together, allowing you to dynamically load and serve models as needed without needing separate endpoints

Implement a multi-container deployment strategy on a single SageMaker endpoint, where each model runs in its own container, allowing you to manage resource allocation more precisely across models

Deploy each model individually using separate SageMaker endpoints, ensuring each model has dedicated resources and can be scaled independently

Use a hybrid approach where frequently updated models are deployed using multi-model endpoints and more complex models are deployed using multi-container endpoints, balancing flexibility and resource management

Explicación general
Correct option:

Use a multi-model deployment on a single SageMaker endpoint to host all models together, allowing you to dynamically load and serve models as needed without needing separate endpoints

Multi-model endpoints are ideal for hosting a large number of models that use the same ML framework on a shared serving container. If you have a mix of frequently and infrequently accessed models, a multi-model endpoint can efficiently serve this traffic with fewer resources and higher cost savings. Your application should be tolerant of occasional cold start-related latency penalties that occur when invoking infrequently used models.

Multi-model endpoints support hosting both CPU and GPU backed models. By using GPU backed models, you can lower your model deployment costs through increased usage of the endpoint and its underlying accelerated compute instances.

Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

Incorrect options:

Deploy each model individually using separate SageMaker endpoints, ensuring each model has dedicated resources and can be scaled independently - Deploying each model on a separate endpoint provides dedicated resources and independent scaling but can lead to higher costs and complexity, especially when managing many models.

Implement a multi-container deployment strategy on a single SageMaker endpoint, where each model runs in its own container, allowing you to manage resource allocation more precisely across models - A multi-container deployment is useful when models have different dependencies or require isolation for security reasons. However, it is more complex to manage than multi-model deployment and may be overkill if your models do not require strict separation or have different runtime environments.

Use a hybrid approach where frequently updated models are deployed using multi-model endpoints and more complex models are deployed using multi-container endpoints, balancing flexibility and resource management - While a hybrid approach might seem to provide flexibility, it adds complexity in terms of management and deployment. It’s better to choose a single approach that meets most of your needs unless there are specific reasons to segregate models by deployment type.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-direct.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 42

Which of the following is correct regarding the techniques used to improve the performance of a Foundation Model (FM)?

Fine-tuning does not change the weights of the FM whereas Retrieval-augmented generation (RAG) changes the weights of the FM

Neither Fine-tuning nor Retrieval-augmented generation (RAG) changes the weights of the FM

Both Fine-tuning and Retrieval-augmented generation (RAG) change the weights of the FM

Respuesta correcta
Fine-tuning changes the weights of the FM whereas Retrieval-augmented generation (RAG) does not change the weights of the FM

Explicación general
Correct option:

Fine-tuning changes the weights of the FM whereas Retrieval-augmented generation (RAG) does not change the weights of the FM

Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model.

Another recommended way to first customize a foundation model to a specific use case is through prompt engineering. Providing your foundation model with well-engineered, context-rich prompts can help achieve desired results without any fine-tuning or changing of model weights.

Lastly, fine-tuning is a customization method for FMs that involves further training and does change the weights of your model.

Retrieval Augmented Generation (RAG) allows you to customize a model’s responses when you want the model to consider new knowledge or up-to-date information. When your data changes frequently, like inventory or pricing, it’s not practical to fine-tune and update the model while it’s serving user queries.

 via - https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html

Exam Alert:

Note the following regarding the techniques to improve the performance of a Foundation Model (FM):

Prompt engineering does NOT change the weights of the FM. Retrieval-Augmented Generation (RAG) does NOT change the weights of the FM. Fine-tuning DOES change the weights of the FM.

Incorrect options:

Neither Fine-tuning nor Retrieval-augmented generation (RAG) changes the weights of the FM

Both Fine-tuning and Retrieval-augmented generation (RAG) change the weights of the FM

Fine-tuning does not change the weights of the FM whereas Retrieval-augmented generation (RAG) changes the weights of the FM

These three options contradict the explanation provided above, so these options are incorrect.

References:

https://aws.amazon.com/blogs/machine-learning/best-practices-to-build-generative-ai-applications-on-aws/

https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-fine-tuning.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 43

A media analytics company processes raw video metadata stored in Amazon S3 buckets to generate insights on audience engagement. The company needs to create data ingestion pipelines for processing this metadata and ML model deployment pipelines to analyze and predict audience behavior patterns. The solution must efficiently handle large-scale data processing and integrate seamlessly with ML workflows.

Which solution will meet these requirements?

Use Kinesis Data Firehose to stream video metadata from Amazon S3 into the ingestion pipeline and deploy models using AWS Lambda

Use AWS Glue to preprocess raw video metadata in Amazon S3 and create a custom script to deploy models manually using SageMaker endpoints

Use SageMaker Studio Classic for both data ingestion pipelines and ML model deployment pipelines to simplify the process

Respuesta correcta
Use AWS Glue to create data ingestion pipelines for scalable data processing and SageMaker Studio Classic to manage ML model deployment pipelines

Explicación general
Correct option:

Use AWS Glue to create data ingestion pipelines for scalable data processing and SageMaker Studio Classic to manage ML model deployment pipelines

AWS Glue provides serverless, scalable ETL capabilities to preprocess and transform large amounts of raw metadata from S3, enabling efficient ingestion pipelines. SageMaker Studio Classic offers an integrated environment for creating, training, deploying, and monitoring ML models. Studio Classic simplifies the process of building and managing model deployment pipelines. Therefore, the combination of AWS Glue for data ingestion and SageMaker Studio Classic for ML pipelines ensures a streamlined, scalable solution optimized for media analytics.

Incorrect options:

Use Kinesis Data Firehose to stream video metadata from Amazon S3 into the ingestion pipeline and deploy models using AWS Lambda - Kinesis Data Firehose is designed for streaming data and is not optimized for large-scale batch ingestion from Amazon S3. AWS Lambda has runtime and resource limitations, making it unsuitable for managing complex ML deployment pipelines.

Use AWS Glue to preprocess raw video metadata in Amazon S3 and create a custom script to deploy models manually using SageMaker endpoints - While AWS Glue is suitable for data preprocessing, custom scripts for model deployment require significant manual effort and are less efficient than using SageMaker Studio Classic for deployment pipelines.

Use SageMaker Studio Classic for both data ingestion pipelines and ML model deployment pipelines to simplify the process - While SageMaker Studio Classic is excellent for ML workflows, it is not designed for large-scale ETL tasks, where AWS Glue provides a better solution.

References:

https://aws.amazon.com/glue/

https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 44

A fashion retailer wants to develop a machine learning model to predict the popularity of clothing designs. The dataset includes categorical data about the primary fabric type of each design, such as "Cotton," "Polyester," and "Silk." An ML engineer is preparing the data for a neural network model and needs to preprocess the fabric type information appropriately.

Which technique should the ML engineer use for feature engineering?

Use PCA (Principal Component Analysis) to reduce the dimensionality of the categorical fabric type data before feeding it into the neural network

Perform min-max normalization on the categorical fabric type data to scale the values between 0 and 1 for compatibility with the model

Use label encoding to assign unique integer values to each fabric type and input these integers directly into the neural network

Respuesta correcta
Apply one-hot encoding to convert the categorical fabric type data into binary vectors that the neural network can process

Explicación general
Correct option:

Apply one-hot encoding to convert the categorical fabric type data into binary vectors that the neural network can process

One-hot encoding is the most appropriate method for handling categorical data in this scenario because:

It converts each category into a binary vector representation, ensuring compatibility with the neural network.

It treats each category as independent, avoiding the introduction of artificial ordinal relationships between the fabric types.

This approach ensures the neural network interprets the fabric types correctly without introducing unintended biases.

For example, if the dataset contains the categories "Cotton," "Polyester," and "Silk," one-hot encoding represents them as:

Cotton: [1, 0, 0]

Polyester: [0, 1, 0]

Silk: [0, 0, 1]

Incorrect options:

Use label encoding to assign unique integer values to each fabric type and input these integers directly into the neural network - Label encoding assigns integers to categories (e.g., Cotton = 1, Polyester = 2). This implies an ordinal relationship between fabric types, which does not exist and could mislead the neural network.

Perform min-max normalization on the categorical fabric type data to scale the values between 0 and 1 for compatibility with the model - Min-max normalization is used for continuous numerical data, not for categorical data like colors. This technique does not apply to this problem.

Use PCA (Principal Component Analysis) to reduce the dimensionality of the categorical fabric type data before feeding it into the neural network - PCA applies to numerical data and cannot be directly used on categorical data without prior encoding.

Reference:

https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.ONE_HOT_ENCODING.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 45

A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in flight is lost. The company’s data science team wants to query ingested data in near-real time.

Which solution provides near-real-time data querying that is scalable with minimal data loss?

Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data

Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data

Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Write to Amazon S3 bucket and use Amazon Redshift Spectrum to query the data

Respuesta correcta
Publish data to Amazon Kinesis Data Streams, use Kinesis Data Analytics to query the data

Explicación general
Correct option:

Publish data to Amazon Kinesis Data Streams, use Kinesis Data Analytics to query the data

With Amazon Kinesis Data Analytics for SQL Applications, you can process and analyze streaming data using standard SQL. The service enables you to quickly author and run powerful SQL code against streaming sources to perform time series analytics, feed real-time dashboards, and create real-time metrics.

To get started with Kinesis Data Analytics, you create a Kinesis Data Analytics application that continuously reads and processes streaming data. The service supports ingesting data from Amazon Kinesis Data Streams and Amazon Data Firehose streaming sources. Kinesis Data Analytics supports Amazon Data Firehose (Amazon S3, Amazon Redshift, Amazon OpenSearch Service, and Splunk), AWS Lambda, and Amazon Kinesis Data Streams as destinations.

Kinesis Data Stream and Kinesis Data Analytics:  via - https://aws.amazon.com/kinesis/

Incorrect options:

Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data

Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Write to Amazon S3 bucket and use Amazon Redshift Spectrum to query the data

Amazon Elastic Block Store (EBS) is not typically used for streaming data. EBS is designed as block-level storage for use with Amazon EC2 instances, providing persistent storage that remains available independent of the lifecycle of the EC2 instance. It is ideal for workloads requiring consistent and low-latency storage performance, such as databases, file systems, and applications that need block storage. Therefore, both these options are incorrect.

Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data - The instance store is temporary storage and the data will be lost when the EC2 instance reboots.

References:

https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html

https://repost.aws/questions/QUpj1ciWvKTfCN8UGJ-UdqnQ/kinesis-data-ingestion

Temática
Data Preparation for Machine Learning (ML)
Pregunta 46

A healthcare company is automating the deployment of a machine learning solution to predict patient health risks. The ML engineer needs to use AWS CloudFormation to define a model that will be hosted on an Amazon SageMaker endpoint and serve real-time inference requests.

Which resource should the ML engineer create in the CloudFormation template to address this requirement?

Use AWS::EC2::Instance to host the ML model manually and handle inference requests

Respuesta correcta
Use AWS::SageMaker::Model to specify the ML model, including the model artifacts and the inference container configuration for hosting

Use AWS::SageMaker::Endpoint to define the SageMaker endpoint that will host the model and serve inference requests

Use AWS::SageMaker::EndpointConfig to define the configuration for the SageMaker endpoint without specifying the model

Explicación general
Correct option:

Use AWS::SageMaker::Model to specify the ML model, including the model artifacts and the inference container configuration for hosting

The AWS::SageMaker::Model CloudFormation resource is specifically designed to define an ML model hosted on Amazon SageMaker. It includes configuration details such as:

Model artifacts - Stored in Amazon S3.

Inference container - The container image that contains the inference code.

IAM Role - Permissions to allow SageMaker to access the resources required for hosting.

This resource is the foundation for creating a SageMaker endpoint, as it defines the model that the endpoint will host.

Incorrect options:

Use AWS::SageMaker::EndpointConfig to define the configuration for the SageMaker endpoint without specifying the model - The AWS::SageMaker::EndpointConfig resource specifies how an endpoint is configured (e.g., instance type and count) but requires a model defined by AWS::SageMaker::Model to function. It cannot directly define the model.

Use AWS::SageMaker::Endpoint to define the SageMaker endpoint that will host the model and serve inference requests - While AWS::SageMaker::Endpoint is required to host the model, it depends on a model resource (AWS::SageMaker::Model). It cannot directly define the model or its configuration.

Use AWS::EC2::Instance to host the ML model manually and handle inference requests - Hosting a model manually on an EC2 instance adds operational overhead and does not leverage SageMaker’s managed hosting capabilities, such as auto-scaling and endpoint management.

References:

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-model.html

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpoint.html

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-sagemaker-endpointconfig.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 47

You are a machine learning engineer responsible for deploying a customer churn prediction model using Amazon SageMaker into production at a telecommunications company. The model is critical for proactive customer retention efforts, so maintaining high availability and reliability is essential. Given the model’s importance, you must implement best practices for deployment, including versioning and rollback strategies, to ensure that any issues with the new model version can be quickly addressed without impacting the business.

Which of the following approaches BEST exemplifies deployment best practices in this scenario?

Deploy the new model version to a staging environment, test it thoroughly, and then manually replace the existing production model. If any issues occur, update the model in staging and redeploy

Deploy the new model version directly to production, replacing the existing model, and monitor its performance closely. If issues arise, retrain the model immediately and redeploy

Version the model by tagging the new version, deploy it to production, and use weighted traffic splitting to send a small percentage of traffic to the new model. If no issues are detected, gradually increase traffic to the new version

Respuesta correcta
Use a blue/green deployment strategy to deploy the new model version in parallel with the existing version, allowing you to switch traffic to the new model gradually and roll back if necessary

Explicación general
Correct option:

Use a blue/green deployment strategy to deploy the new model version in parallel with the existing version, allowing you to switch traffic to the new model gradually and roll back if necessary

A blue/green deployment strategy is a best practice in model deployment. It allows you to deploy the new model version in parallel with the existing one, gradually shifting traffic to the new version while monitoring its performance. If issues are detected, you can quickly roll back to the previous version without disrupting service.

In a blue/green deployment, SageMaker provisions a new fleet with the updates (the green fleet). Then, SageMaker shifts traffic from the old fleet (the blue fleet) to the green fleet. Once the green fleet operates smoothly for a set evaluation period (called the baking period), SageMaker terminates the blue fleet. You can specify Amazon CloudWatch alarms that SageMaker uses to monitor the green fleet. If an issue with the updated code trips any of the alarms, SageMaker initiates an auto-rollback to the blue fleet in order to maintain availability thereby minimizing risk.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html

Incorrect options:

Deploy the new model version directly to production, replacing the existing model, and monitor its performance closely. If issues arise, retrain the model immediately and redeploy - Deploying the new model directly to production without a rollback strategy is risky. If issues arise, the model would need to be retrained and redeployed, which could lead to significant downtime and business impact.

Deploy the new model version to a staging environment, test it thoroughly, and then manually replace the existing production model. If any issues occur, update the model in staging and redeploy - Deploying to a staging environment is a good practice for testing, but manually replacing the production model increases the risk of downtime and errors. A more automated and controlled strategy like blue/green deployment is preferable.

Version the model by tagging the new version, deploy it to production, and use weighted traffic splitting to send a small percentage of traffic to the new model. If no issues are detected, gradually increase traffic to the new version - Versioning and using weighted traffic splitting is also a good practice, but it’s more complex and might not provide the same immediate rollback capability as blue/green deployment. However, this approach is viable in certain scenarios where gradual rollout is necessary.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html

https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-rolling.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 48

You are a data scientist at a healthcare company that has deployed a machine learning model to predict patient readmission rates. The model plays a crucial role in optimizing patient care and managing hospital resources. After several months in production, the medical team has noticed that the model’s predictions seem less accurate than before, leading to concerns about data quality and model performance. To ensure that the model continues to deliver reliable predictions, you need to implement techniques to monitor both data quality and model performance continuously.

Which of the following approaches is the MOST EFFECTIVE for monitoring data quality and model performance in this scenario?

Respuesta correcta
Implement data validation rules to check for missing values, outliers, and distribution changes in the input data before feeding it to the model, and use model performance metrics such as accuracy and F1 score to monitor the model's output

Schedule periodic retraining of the model on the latest data to ensure it remains accurate, without additional monitoring of data quality or performance metrics

Perform manual checks on a sample of the input data each week to ensure data quality and manually track model performance metrics in a spreadsheet for analysis

Set up a dashboard that tracks model performance metrics, such as precision, recall, and AUC, and use version control to monitor changes in the model's code and training data over time

Explicación general
Correct option:

Implement data validation rules to check for missing values, outliers, and distribution changes in the input data before feeding it to the model, and use model performance metrics such as accuracy and F1 score to monitor the model's output

You may encounter data quality issues such as missing values or having the wrong data type. These issues may not be discovered until quite late in the process after training a ML model. So, you need to perform data validation to proactively check for issues in your data and provides guidance on resolutions. Implementing data validation rules helps ensure that the input data remains clean and consistent, which is crucial for maintaining model performance. By using better quality data, you will end up with a better performing ML model.

 via - https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/

Monitoring model performance metrics like accuracy and F1 score provides real-time insights into how well the model is performing, allowing you to detect any degradation early and take corrective actions.

Key metrics to measure machine learning model performance:





via - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Incorrect options:

Perform manual checks on a sample of the input data each week to ensure data quality and manually track model performance metrics in a spreadsheet for analysis - While manual checks can catch some issues, they are not scalable or reliable for continuous monitoring. Manually tracking performance metrics is also prone to human error and can delay the identification of problems.

Set up a dashboard that tracks model performance metrics, such as precision, recall, and AUC, and use version control to monitor changes in the model's code and training data over time - A dashboard is useful for visualizing performance metrics, but it does not directly address data quality. Additionally, while version control is important for tracking changes, it does not actively monitor data quality or model performance.

Schedule periodic retraining of the model on the latest data to ensure it remains accurate, without additional monitoring of data quality or performance metrics - Periodic retraining is important but does not replace the need for continuous monitoring of data quality and performance. Without monitoring, you may not know when retraining is needed, potentially leading to periods of poor model performance.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

https://aws.amazon.com/blogs/machine-learning/identifying-and-avoiding-common-data-issues-while-building-no-code-ml-models-with-amazon-sagemaker-canvas/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 49

A healthcare company uses a binary classification model to predict whether patients are at risk of developing a particular condition. The model is currently in production, but the company plans to develop a new version of the model to improve its accuracy. The ML engineer is tasked with recalibrating the model to maximize correct predictions for both patients at risk (positive labels) and patients not at risk (negative labels). The engineer must choose an appropriate metric to evaluate and adjust the model for these requirements.

Which metric should the ML engineer use for recalibrating the model?

Respuesta correcta
Accuracy

Precision

Root mean squared error (RMSE)

Recall

Explicación general
Correct option:

Accuracy

Amazon SageMaker Autopilot produces metrics that measure the predictive quality of machine learning model candidates. The metrics calculated for candidates are specified using an array of MetricDatum types.

The ratio of the number of correctly classified items to the total number of (correctly and incorrectly) classified items. It is used for both binary and multiclass classification. Accuracy measures how close the predicted class values are to the actual values. Values for accuracy metrics vary between zero (0) and one (1). A value of 1 indicates perfect accuracy, and 0 indicates perfect inaccuracy.

Incorrect options:

Precision - Precision measures how well an algorithm predicts the true positives (TP) out of all of the positives that it identifies. It is defined as follows: Precision = TP/(TP+FP), with values ranging from zero (0) to one (1), and is used in binary classification. Precision is an important metric when the cost of a false positive is high.

Root mean squared error (RMSE) - Root mean squared error (RMSE) measures the square root of the squared difference between predicted and actual values, and is averaged over all values. It is used in regression analysis to understand model prediction error. It's an important metric to indicate the presence of large model errors and outliers.

Recall - Recall measures how well an algorithm correctly predicts all of the true positives (TP) in a dataset. A true positive is a positive prediction that is also an actual positive value in the data. Recall is defined as follows: Recall = TP/(TP+FN), with values ranging from 0 to 1. Higher scores reflect a better ability of the model to predict true positives (TP) in the data.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Temática
ML Model Development
Pregunta 50

Which of the following is correct regarding the training set, validation set, and test set used in the context of machine learning? (Select two)

Selección correcta
Validation sets are optional

Test set is used for hyperparameter tuning

Selección correcta
Test set is used to determine how well the model generalizes

Validation set is used to determine how well the model generalizes

Test sets are optional

Explicación general
Correct options:

Data used for ML is typically split into the following datasets:

The training set is used to train the model, the validation set is used for tuning hyperparameters and selecting the best model during the training process, and the test set is used for evaluating the final performance of the model on unseen data.

Validation sets are optional

The validation set introduces new data to the trained model. You can use a validation set to periodically measure model performance as training is happening and also tune any hyperparameters of the model. However, validation datasets are optional.

Test set is used to determine how well the model generalizes

The test set is used on the final trained model to assess its performance on unseen data. This helps determine how well the model generalizes.

Incorrect options:

Test set is used for hyperparameter tuning - The test set is used for evaluating the final performance of the model on unseen data.

Test sets are optional - Only validation sets are optional.

Validation set is used to determine how well the model generalizes - Only the test set is used to determine how well the model generalizes.

Reference:

https://aws.amazon.com/blogs/machine-learning/create-train-test-and-validation-splits-on-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 51

You are a Data Scientist working for a retail company and you have been tasked with developing a demand forecasting model using Amazon SageMaker. The company requires the model to be highly accurate to optimize inventory levels, but you also need to consider constraints on training time and cost due to budget limitations. You have access to multiple SageMaker instance types and options like spot instances to reduce costs, but you must balance these factors against the need for a performant model. Your goal is to choose a configuration that provides an acceptable tradeoff between model performance, training time, and cost.

Which of the following strategies should you consider when balancing model performance, training time, and cost in this scenario using Amazon SageMaker? (Select two)

Deploy multiple models with different instance types simultaneously, choosing the one that completes training first, regardless of performance or cost

Use the largest available instance type to minimize training time, regardless of cost, ensuring that the model is trained as quickly as possible

Selección correcta
Optimize hyperparameters using Amazon SageMaker’s automatic model tuning (hyperparameter optimization) to improve performance, while using spot instances to reduce cost

Use a smaller instance type to save on costs, and accept longer training times, as model accuracy is not the highest priority

Selección correcta
Implement distributed training across multiple smaller instances to balance training time and cost while maintaining model performance

Explicación general
Correct options:

Optimize hyperparameters using Amazon SageMaker’s automatic model tuning (hyperparameter optimization) to improve performance, while using spot instances to reduce cost

Amazon SageMaker’s automatic model tuning allows you to optimize hyperparameters, which can significantly improve model performance without manually tuning each parameter. By using spot instances, you can reduce the cost of training while still focusing on achieving high performance. This approach helps balance the tradeoff between cost and performance effectively.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html

Implement distributed training across multiple smaller instances to balance training time and cost while maintaining model performance

Distributed training across multiple smaller instances can help reduce overall training time while managing costs. This strategy allows you to leverage parallel processing without incurring the high costs associated with using a single, large instance. It also ensures that the model training process is efficient without compromising on performance.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html

Incorrect options:

Use the largest available instance type to minimize training time, regardless of cost, ensuring that the model is trained as quickly as possible - Using the largest available instance type might minimize training time but can result in prohibitively high costs, which is not suitable given the budget constraints. This strategy does not balance cost-effectiveness with the need for performance.

Use a smaller instance type to save on costs, and accept longer training times, as model accuracy is not the highest priority - While using a smaller instance type may save on costs, it can significantly increase training time. Given that model accuracy is important for optimizing inventory levels, this tradeoff might not be acceptable.

Deploy multiple models with different instance types simultaneously, choosing the one that completes training first, regardless of performance or cost - Deploying multiple models simultaneously on different instance types and selecting the one that finishes first is inefficient and can lead to unnecessary costs. Additionally, this approach does not consider the need for balanced performance, cost, and training time.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html

https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html

Temática
ML Model Development
Pregunta 52

A publishing company wants to automatically identify and extract meaningful, unique keywords from a large collection of documents using AWS services.

What solution should the ML engineer implement to achieve this goal with minimal operational overhead?

Leverage Amazon Rekognition to search and extract insights about the entities and Key phrases present in the documents

Leverage Amazon Textract to gather insights about the entities and Key phrases present in the documents

Respuesta correcta
Leverage Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords

Leverage Amazon Kendra to search and extract meaningful, unique keywords present in the documents

Explicación general
Correct option:

Leverage Amazon Comprehend custom entity recognition and key phrase extraction to identify and extract relevant keywords

Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text. Amazon Comprehend can analyze a document or set of documents to gather insights about it. Some of the insights that Amazon Comprehend develops about a document include:

Entities – Amazon Comprehend returns a list of entities, such as people, places, and locations, identified in a document.

Events – Amazon Comprehend detects speciﬁc types of events and related details.

Key phrases – Amazon Comprehend extracts key phrases that appear in a document. For example, a document about a basketball game might return the names of the teams, the name of the venue, and the final score.

Personally identifiable information (PII) – Amazon Comprehend analyzes documents to detect personal data that identify an individual, such as an address, bank account number, or phone number.

Dominant language – Amazon Comprehend identifies the dominant language in a document. Amazon Comprehend can identify 100 languages.

Sentiment – Amazon Comprehend determines the dominant sentiment of a document. Sentiment can be positive, neutral, negative, or mixed.

Targeted Sentiment – Amazon Comprehend determines the sentiment of specific entities mentioned in a document. The sentiment of each mention can be positive, neutral, negative, or mixed.

Syntax analysis – Amazon Comprehend parses each word in your document and determines the part of speech for the word. For example, in the sentence "It is raining today in Seattle," "it" is identified as a pronoun, "raining" is identified as a verb, and "Seattle" is identified as a proper noun.

Incorrect options:

Leverage Amazon Textract to gather insights about the entities and Key phrases present in the documents - Amazon Textract extracts text, handwriting, and data from scanned documents, invoices, letters, and other documents. It supports tabular data and many languages and dialects. Amazon Textract and Amazon Comprehend are often used together for various applications. However, for the given use case, the input is in the form of documents, so Amazon Comprehend alone is sufficient to identify and extract key phrases. Additionally, Amazon Comprehend provides a custom entity recognition feature for more tailored extractions.

Leverage Amazon Rekognition to search and extract insights about the entities and Key phrases present in the documents - Amazon Rekognition can detect text in images and videos. It can then convert the detected text into machine-readable text. Uses computer vision to analyze images and videos at scale. It can perform object and scene detection, facial analysis, and text detection. Rekognition can also be used for content moderation.

We use Amazon Comprehend for processing and understanding text data, such as analyzing customer reviews, extracting keywords, or identifying entities from text. On the other hand, Amazon Rekognition is used when your focus is on analyzing visual content like detecting objects in images, identifying faces, or moderating videos for inappropriate content.

Leverage Amazon Kendra to search and extract meaningful, unique keywords present in the documents - Amazon Kendra provides search and Retrieval Augmented Generation (RAG) functionality for your application. It indexes your documents directly—or from your third-party document repository—and intelligently serves relevant information to your users. You can use Amazon Kendra to create an updatable index of documents of a variety of types.

Reference:

https://docs.aws.amazon.com/comprehend/latest/dg/concepts-insights.html

Temática
ML Model Development
Pregunta 53

You are a machine learning engineer at a tech company responsible for maintaining a recommendation engine that drives personalized content for users. The current model has been performing well, but you’ve recently developed a new model that incorporates additional features and advanced techniques. Before fully replacing the existing model, you want to evaluate the new model's performance in a real-world environment. To do this, you decide to use A/B testing to compare the performance of the new model against the current model in terms of business outcomes.

Which of the following approaches is the MOST EFFECTIVE for conducting A/B testing for this use case?

Respuesta correcta
Randomly assign a percentage of users to each model and measure engagement metrics, such as click-through rates and session duration, using Amazon CloudWatch to aggregate the results and determine the winning model

Fully deploy the new model and observe its performance over time. If it performs worse than the old model, revert to the previous model and analyze the differences

Deploy the new model as the primary endpoint and keep the current model as a fallback. Use AWS Lambda to route a percentage of traffic to the new model and compare the predictions manually to decide which model to keep

Deploy both models behind separate endpoints and use Amazon SageMaker Model Monitor to track key performance metrics like accuracy and latency, choosing the better model based on these metrics

Explicación general
Correct option:

Randomly assign a percentage of users to each model and measure engagement metrics, such as click-through rates and session duration, using Amazon CloudWatch to aggregate the results and determine the winning model

This option correctly implements A/B testing by randomly assigning users to each model and measuring real-world engagement metrics like click-through rates, conversion rates and session duration. Using Amazon CloudWatch to aggregate and analyze these results allows for a data-driven decision on which model performs better in practice, based on actual user behavior.

 via - https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/

Incorrect options:

Deploy both models behind separate endpoints and use Amazon SageMaker Model Monitor to track key performance metrics like accuracy and latency, choosing the better model based on these metrics - Deploying both models behind separate endpoints and tracking metrics with SageMaker Model Monitor can provide valuable insights into technical performance, but it does not measure real-world user engagement. A/B testing is most effective when comparing user-driven metrics, which directly reflect business outcomes.

Deploy the new model as the primary endpoint and keep the current model as a fallback. Use AWS Lambda to route a percentage of traffic to the new model and compare the predictions manually to decide which model to keep - Routing a percentage of traffic to the new model using AWS Lambda introduces some comparison, but manual evaluation is prone to bias and error. A/B testing should be automated and driven by metrics to ensure an objective comparison.

Fully deploy the new model and observe its performance over time. If it performs worse than the old model, revert to the previous model and analyze the differences - Fully deploying the new model without a direct comparison to the old model risks disrupting the user experience. A/B testing provides a safer and more controlled environment to assess the new model's performance before making a full switch.

Reference:

https://aws.amazon.com/blogs/machine-learning/a-b-testing-ml-models-in-production-using-amazon-sagemaker/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 54

You are a machine learning engineer tasked with deploying a machine learning model to a mobile device for real-time object detection. The model currently performs well but is too large to fit within the device's memory and computational constraints. To ensure the model can run efficiently on the device without sacrificing too much accuracy, you need to reduce its size.

Given these constraints, which combination of techniques is the MOST LIKELY to effectively reduce the model’s size while maintaining acceptable performance?

Use feature selection to reduce the number of input features, compress the model by converting the model weights from floating-point to integer format

Reduce the size of the training dataset, use dropout to decrease model complexity, thereby shrinking the deployed model size

Respuesta correcta
Prune the model by removing less important neurons and layers, and apply quantization to reduce the precision of the model’s weights from 32-bit to 8-bit

Apply dimensionality reduction using PCA on the input data, and use model ensembling to combine smaller models, which collectively match the performance of the original model

Explicación general
Correct option:

Prune the model by removing less important neurons and layers, and apply quantization to reduce the precision of the model’s weights from 32-bit to 8-bit

Model pruning can significantly reduce model size without sacrificing accuracy. The idea is simple: identify the redundant parameters in the model that contribute little to the training process.

 via - https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/

Quantization is a general term that refers to using technologies that store numbers and perform calculations on them in more compact and lower precision form than their original format (e.g., 32-bit floating point)





via - https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/

Pruning and quantization are both effective methods for reducing model size. Pruning removes parts of the model that contribute little to overall performance, such as unnecessary neurons or layers, which reduces the model’s complexity. Quantization further reduces the model size by decreasing the precision of weights (e.g., from 32-bit floating-point to 8-bit integers), significantly lowering memory requirements without drastically impacting accuracy.

Incorrect options:

Apply dimensionality reduction using PCA on the input data, and use model ensembling to combine smaller models, which collectively match the performance of the original model - Dimensionality reduction using PCA can reduce the input data’s complexity but does not directly shrink the model itself. Model ensembling generally increases model size because it involves combining multiple models, making this approach unsuitable for reducing size.

Use feature selection to reduce the number of input features, compress the model by converting the model weights from floating-point to integer format - Feature selection can reduce input data size. However, converting weights from floating-point to integer format might be less precise than quantization methods like 8-bit quantization, which are specifically optimized for model compression.

Reduce the size of the training dataset, use dropout to decrease model complexity, thereby shrinking the deployed model size - Reducing the training dataset size and using dropout are strategies to prevent overfitting, not to reduce the deployed model size.

References:

https://aws.amazon.com/blogs/machine-learning/leveraging-low-precision-and-quantization-for-deep-learning-using-the-amazon-ec2-c5-instance-and-bigdl/

https://aws.amazon.com/blogs/machine-learning/pruning-machine-learning-models-with-amazon-sagemaker-debugger-and-amazon-sagemaker-experiments/

Temática
ML Model Development
Pregunta 55

You are a Data Scientist working for an e-commerce platform that uses a machine learning model to recommend products to customers. The model has been in production for over a year and was initially performing well. However, you have recently noticed a decrease in the model's accuracy, particularly when recommending products to new customers. This decline suggests that the model may be experiencing drift due to changing customer preferences and market trends. To address this issue, you need to implement a strategy for detecting and managing model drift using Amazon SageMaker.

Which of the following strategies should you implement to effectively detect and manage model drift in your product recommendation model using Amazon SageMaker? (Select two)

Selección correcta
Use Amazon SageMaker Model Monitor to set up monitoring for data quality and data drift, enabling you to receive alerts and initiate model retraining when the distribution of input data changes significantly from the training data

Use Amazon SageMaker Clarify to continuously monitor and mitigate bias in the model, and initiate model retraining due to changes in data distribution

Selección correcta
Retrain the model with the new training data, ensuring that the model remains up to date with new customer preferences

Manually review model performance every quarter and initiate retraining only if a significant drop in accuracy is observed, minimizing unnecessary retraining costs

Deploy multiple versions of the model simultaneously using Amazon SageMaker multi-model endpoints, and switch between them based on performance metrics

Explicación general
Correct options:

Use Amazon SageMaker Model Monitor to set up monitoring for data quality and data drift, enabling you to receive alerts and initiate model retraining when the distribution of input data changes significantly from the training data

Retrain the model with the new training data, ensuring that the model remains up to date with new customer preferences

Amazon SageMaker Model Monitor allows you to continuously monitor the input data for data quality and data drift. By setting up alerts, you can detect when the input data distribution has changed significantly from the training data, which is a key indicator of model drift. This proactive approach helps in taking timely action, such as retraining the model or adjusting it to account for new patterns in the data.

With SageMaker Model Monitor, you can set alerts that notify you when there are deviations in the model quality. Early and proactive detection of these deviations lets you to take corrective actions. You can take actions like retraining models, auditing upstream systems, or fixing quality issues without having to monitor models manually or build additional tooling.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

Incorrect options:

Deploy multiple versions of the model simultaneously using Amazon SageMaker multi-model endpoints, and switch between them based on performance metrics - Deploying multiple versions of the same model using Amazon SageMaker multi-model endpoints is resource-intensive and cost-inefficient. So, this option is ruled out.

Manually review model performance every quarter and initiate retraining only if a significant drop in accuracy is observed, minimizing unnecessary retraining costs - Manually reviewing performance every quarter is reactive and might not catch drift early enough, leading to prolonged periods of suboptimal model performance. Automated monitoring with Model Monitor provides a more timely and systematic approach to detecting drift.

Use Amazon SageMaker Clarify to continuously monitor and mitigate bias in the model, and initiate model retraining due to changes in data distribution - While Amazon SageMaker Clarify is useful for monitoring and mitigating bias, it is not specifically designed for detecting model drift caused by changes in data distribution. Model Monitor is the more appropriate tool for addressing drift in this context.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

https://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html

https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/

https://aws.amazon.com/blogs/architecture/detecting-data-drift-using-amazon-sagemaker/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 56

A robotics company is training a deep learning model in Amazon SageMaker Studio to optimize the movement of autonomous robots in a warehouse. The training process involves fine-tuning a pre-trained model on a large dataset of movement trajectories. In previous training runs, the company encountered issues such as vanishing gradients, underutilized GPUs, and overfitting, leading to suboptimal model performance and wasted resources.

The company’s ML engineer needs a solution that meets the following requirements with the LEAST operational overhead:

Detect these training issues in real-time.

Automatically react to these issues by stopping training or triggering notifications.

Provide comprehensive metrics for monitoring without increasing operational complexity.

What do you suggest?

Leverage Amazon CloudWatch Logs to monitor GPU usage and manually analyze logs to identify vanishing gradients and overfitting during training

Respuesta correcta
Use Amazon SageMaker Debugger with built-in rules to detect vanishing gradients, GPU underutilization, and overfitting. Configure Debugger to automatically react based on predefined thresholds

Develop custom Python scripts to track training metrics and set up logic to monitor for issues like vanishing gradients and overfitting in real time

Use Amazon SageMaker Experiments to log training metrics and manually evaluate them for signs of vanishing gradients, GPU inefficiency, and overfitting

Explicación general
Correct option:

Use Amazon SageMaker Debugger with built-in rules to detect vanishing gradients, GPU underutilization, and overfitting. Configure Debugger to automatically react based on predefined thresholds

Amazon SageMaker Debugger provides a low-effort, integrated solution to address common training issues like vanishing gradients, GPU underutilization, and overfitting. By enabling built-in rules, SageMaker Debugger can monitor training jobs in real time, detect issues, and automatically trigger actions such as stopping training or generating alerts. These rules offer deep insights and actionable metrics while integrating seamlessly with SageMaker Studio for visualization. The comprehensive monitoring and automated reactions reduce manual effort and allow the ML engineer to focus on improving the model rather than troubleshooting issues.

Here are the rules relevant to the given use case:

VanishingGradient - This rule detects if the gradients in a trial become extremely small or drop to a zero magnitude. If the mean of the absolute values of the gradients drops below a specified threshold, the rule returns True.

Overfit - This rule detects if your model is being overfit to the training data by comparing the validation and training losses. This rule can be applied either to one of the supported deep learning frameworks (TensorFlow, MXNet, and PyTorch) or to the XGBoost algorithm.

LowGPUUtilization - The LowGPUUtilization rule helps detect if GPU utilization is low or suffers from fluctuations. This is checked for each GPU on each worker. Rule returns True if 95th quantile is below threshold_p95 which indicates underutilization. Rule returns true if 95th quantile is above threshold_p95 and 5th quantile is below threshold_p5 which indicates fluctuations.

Incorrect options:

Leverage Amazon CloudWatch Logs to monitor GPU usage and manually analyze logs to identify vanishing gradients and overfitting during training - CloudWatch can track GPU usage but does not have built-in capabilities to detect deep learning-specific issues like vanishing gradients and overfitting. This approach also requires manual log analysis, increasing operational overhead.

Use Amazon SageMaker Experiments to log training metrics and manually evaluate them for signs of vanishing gradients, GPU inefficiency, and overfitting - SageMaker Experiments is useful for tracking and organizing training runs but lacks automated rules to detect and react to specific training issues in real time.

Develop custom Python scripts to track training metrics and set up logic to monitor for issues like vanishing gradients and overfitting in real time - Writing custom scripts introduces significant development and maintenance overhead. SageMaker Debugger’s built-in rules provide the same functionality with minimal effort.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-profiler-rules.html#low-gpu-utilization

https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#overfit

https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html#vanishing-gradient

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 57

What is Feature Engineering in the context of machine learning?

Feature Engineering refers to the visualization of data to understand patterns, and it is important because it helps in identifying trends in the dataset

Feature Engineering is the process of collecting raw data, and it is important because it ensures the availability of data for model training

Respuesta correcta
Feature Engineering involves selecting, modifying, or creating features from raw data to improve the performance of machine learning models, and it is important because it can significantly enhance model accuracy and efficiency

Feature Engineering is the process of tuning hyperparameters in a machine learning model, and it is important because it optimizes the model’s performance

Explicación general
Correct option:

Feature Engineering involves selecting, modifying, or creating features from raw data to improve the performance of machine learning models, and it is important because it can significantly enhance model accuracy and efficiency

Feature Engineering is the process of selecting, modifying, or creating new features from raw data to enhance the performance of machine learning models. It is crucial because it can lead to significant improvements in model accuracy and efficiency by providing the model with better representations of the data.

 via - https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html

Incorrect options:

Feature Engineering is the process of collecting raw data, and it is important because it ensures the availability of data for model training - Feature Engineering is not about collecting raw data; it focuses on transforming raw data into meaningful features for model training.

Feature Engineering is the process of tuning hyperparameters in a machine learning model, and it is important because it optimizes the model’s performance - Hyperparameter tuning is the process of selecting the best set of hyperparameters for a machine learning model. Hyperparameters are the external configurations of the model that are set before training and cannot be learned from the data. Feature Engineering is not related to Hyperparameter tuning.

Feature Engineering refers to the visualization of data to understand patterns, and it is important because it helps in identifying trends in the dataset - While data visualization is important, Feature Engineering specifically refers to transforming raw data into useful features, not just visualizing data.

References:

https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html

https://aws.amazon.com/what-is/feature-engineering/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 58

You are an ML Engineer at a financial services company tasked with deploying a machine learning model for real-time fraud detection in production. The model requires low-latency inference to ensure that fraudulent transactions are flagged immediately. However, you also need to conduct extensive testing and experimentation in a separate environment to fine-tune the model and validate its performance before deploying it. You must provision compute resources that are appropriate for both environments, balancing performance, cost, and the specific needs of testing and production.

Which of the following strategies should you implement to effectively provision compute resources for both the production environment and the test environment using Amazon SageMaker, considering the different requirements for each environment? (Select two)

Provision CPU-based instances in both production and test environments to reduce costs, as CPU instances are generally cheaper than GPU instances

Selección correcta
Use CPU-based instances in the test environment to save on costs during experimentation

Provision identical instances in both production and test environments to ensure consistent performance between the two, eliminating the risk of discrepancies during deployment

Use GPU-based instances in both production and test environments to ensure that the model inference and testing are both performed at maximum speed, regardless of cost

Selección correcta
Leverage AWS Inferentia accelerators in the production environment to meet high throughput and low latency requirements

Explicación general
Correct options:

Use CPU-based instances in the test environment to save on costs during experimentation

For the test environment, CPU-based instances can be used to run experiments and validate the model, which helps reduce costs without compromising the ability to test different configurations and models.

Leverage AWS Inferentia accelerators in the production environment to meet high throughput and low latency requirements

AWS Inferentia accelerators are designed by AWS to deliver high performance at the lowest cost in Amazon EC2 for your deep learning (DL) and generative AI inference applications. The first-generation AWS Inferentia accelerator powers Amazon Elastic Compute Cloud (Amazon EC2) Inf1 instances that deliver higher throughput and lower latency than comparable Amazon EC2 instances. They also offer up to 70% lower cost per inference than comparable Amazon EC2 instances. Therefore, you can meet performance requirements in the production environment while optimizing costs.

Incorrect options:

Use GPU-based instances in both production and test environments to ensure that the model inference and testing are both performed at maximum speed, regardless of cost - While using GPU-based instances in both environments ensures high performance, it is not cost-effective. The test environment does not typically require the same level of performance as production, making GPU instances unnecessary and expensive.

Provision CPU-based instances in both production and test environments to reduce costs, as CPU instances are generally cheaper than GPU instances - Provisioning only CPU-based instances in both environments might save costs but would likely fail to meet the low-latency requirements in production. Inference times could be unacceptably slow, which is critical for real-time fraud detection.

Provision identical instances in both production and test environments to ensure consistent performance between the two, eliminating the risk of discrepancies during deployment - Although using identical instances in both environments ensures consistency, it is not cost-efficient. The test environment does not need to replicate the full performance of the production environment, so using less powerful and less expensive instances is more appropriate for testing purposes.

Reference:

https://aws.amazon.com/machine-learning/inferentia/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 59

You are a data scientist working for an e-commerce company that wants to implement personalized product recommendations for its users. The company has a large dataset of user interactions, including clicks, purchases, and reviews. The goal is to create a recommendation system that can scale to millions of users while providing real-time recommendations based on user behavior. You need to choose the most appropriate built-in algorithm in Amazon SageMaker to achieve this goal.

Given the requirements, which of the following Amazon SageMaker built-in algorithms is the MOST SUITABLE for this use case?

K-Means Algorithm to cluster users into segments and recommend products based on these segments

BlazingText Algorithm to analyze the text in user reviews and identify product similarities

XGBoost Algorithm to rank the products based on user behavior and demographic features

Respuesta correcta
Factorization Machines Algorithm to model user-item interactions for collaborative filtering

Explicación general
Correct option:

Factorization Machines Algorithm to model user-item interactions for collaborative filtering

The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically. For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.

Factorization Machines is well-suited for collaborative filtering. It excels at modeling sparse user-item interactions, making it ideal for large-scale recommendation systems where there are many users and items but relatively few interactions for each user-item pair. This algorithm can effectively capture latent factors to provide personalized recommendations.

Mapping use cases to built-in algorithms:

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html

Incorrect options:

XGBoost Algorithm to rank the products based on user behavior and demographic features - XGBoost is a powerful algorithm for ranking and classification tasks, but it’s not optimized for collaborative filtering, which is crucial for personalized recommendations in this context.

BlazingText Algorithm to analyze the text in user reviews and identify product similarities - BlazingText is effective for text classification and word embedding but is not specifically designed for recommendation systems. While it can be used to analyze user reviews, it does not address the core requirement of user-item interaction modeling.

K-Means Algorithm to cluster users into segments and recommend products based on these segments - K-Means is useful for clustering users into segments, but this approach is more generalized and does not provide the level of personalization required for individual recommendations based on specific user-item interactions.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html

https://aws.amazon.com/blogs/machine-learning/accelerate-and-improve-recommender-system-training-and-predictions-using-amazon-sagemaker-feature-store/

https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html

Temática
ML Model Development
Pregunta 60

You are a data scientist working for a media company that processes large volumes of video and image data to generate personalized content recommendations. The dataset, which is stored in Amazon S3, contains tens of millions of small image files and several terabytes of high-resolution large video files. The training jobs you run on Amazon SageMaker require low-latency access to this data and need to be completed quickly to keep up with the dynamic content pipeline.

Given the characteristics of your data and the requirements for low-latency, high-throughput access, which approach is the MOST APPROPRIATE for this scenario?

Use Fast File mode with Amazon S3 to stream the small image files directly to the training instances on-demand, minimizing the time required to start training

Use Amazon FSx for Lustre to mount the entire dataset as a high-performance file system, providing consistently low-latency access to both the small image files and the large video files

Use Fast File mode with Amazon S3 for the large video files, enabling on-demand streaming of data, and store the small image files locally on the training instances to reduce I/O latency

Respuesta correcta
Create an FSx for Lustre file system linked with the relevant Amazon S3 bucket folder having the training data for the small image files and apply Fast File mode to the relevant Amazon S3 bucket folder to access the video files, thereby combining the strengths of both approaches

Explicación general
Correct option:

Create an FSx for Lustre file system linked with the relevant Amazon S3 bucket folder having the training data for the small image files and apply Fast File mode to the relevant Amazon S3 bucket folder to access the video files, thereby combining the strengths of both approaches





via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html

FSx for Lustre can scale to hundreds of gigabytes of throughput and millions of IOPS with low-latency file retrieval. When starting a training job, SageMaker mounts the FSx for Lustre file system to the training instance file system, then starts your training script. Mounting itself is a relatively fast operation that doesn't depend on the size of the dataset stored in FSx for Lustre.

If your dataset is too large for file mode, has many small files that you can't serialize easily, or uses a random read access pattern, FSx for Lustre is a good option to consider. Its file system scales to hundreds of gigabytes per second (GB/s) of throughput and millions of IOPS, which is ideal when you have many small files.



via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data-best-practices.html

For the given use case, you can create an FSx for Lustre file system linked with the Amazon S3 bucket folder having the training data for the small image files, like so:



via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html

You can then apply Fast File mode for the video files in the relevant Amazon S3 bucket folder, like so:



via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html

Incorrect options:

Use Amazon FSx for Lustre to mount the entire dataset as a high-performance file system, providing consistently low-latency access to both the small image files and the large video files - Amazon FSx for Lustre is designed for high-performance workloads with large datasets, especially when you need low-latency access to many small files that you can't serialize easily, or uses a random read access pattern. FSx is not the optimal solution to provide low-latency access to many large video files, you should rather use the Fast File mode for the video files in the relevant Amazon S3 bucket folder. So, this option is incorrect.

Use Fast File mode with Amazon S3 to stream the small image files directly to the training instances on-demand, minimizing the time required to start training - While Fast File mode is effective for large files, it does not provide the low-latency, high-throughput access needed for a large number of small files. So, this option is incorrect.

Use Fast File mode with Amazon S3 for the large video files, enabling on-demand streaming of data, and store the small image files locally on the training instances to reduce I/O latency - Splitting data management between Fast File mode for large files and local storage for small files adds unnecessary complexity and additional costs without providing a proportional performance improvement, so this option is incorrect.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html

https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/

https://aws.amazon.com/fsx/lustre/faqs/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 61

You are a Machine Learning Engineer at a healthcare company working on a binary classification model to predict whether a patient has a particular disease based on several medical features. The consequences of misclassifications are severe: false positives lead to unnecessary and expensive follow-up tests, while false negatives could result in a failure to provide critical treatment. You need to evaluate the model using appropriate metrics to balance the risks associated with these types of errors.

Given the critical nature of the application, which combination of evaluation metrics should you prioritize to minimize both false positives and false negatives while ensuring that the model is reliable for deployment? (Select two)

Selección correcta
Prioritize recall to reduce the number of false negatives, ensuring that as many patients with the disease as possible are correctly identified

Focus on precision to reduce the number of false positives, thus avoiding unnecessary follow-up tests for patients who do not have the disease

Evaluate the model using the Area Under the ROC Curve (AUC) to understand its performance across different classification thresholds

Prioritize accuracy, as it provides a general sense of how well the model is performing across all predictions

Selección correcta
Use the F1 score to balance the trade-off between precision and recall, ensuring that both false positives and false negatives are considered

Explicación general
Correct options:

Use the F1 score to balance the trade-off between precision and recall, ensuring that both false positives and false negatives are considered

The F1 score is the harmonic mean of precision and recall, and it provides a single metric that balances both false positives and false negatives. It is particularly useful in scenarios where both types of errors have significant consequences, as in this healthcare scenario.

Prioritize recall to reduce the number of false negatives, ensuring that as many patients with the disease as possible are correctly identified

Given the severe risk associated with false negatives (failing to identify a patient with the disease), recall is a crucial metric. By prioritizing recall, you ensure that the model captures as many actual positive cases as possible, reducing the chance of missing a patient who needs treatment.

Key metrics to measure machine learning model performance:





via - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Incorrect options:

Prioritize accuracy, as it provides a general sense of how well the model is performing across all predictions - Accuracy alone is not a sufficient metric in this scenario because it does not differentiate between the types of errors (false positives and false negatives). High accuracy might still result in unacceptable levels of false negatives or false positives.

Focus on precision to reduce the number of false positives, thus avoiding unnecessary follow-up tests for patients who do not have the disease - Focusing solely on precision would reduce false positives but might increase false negatives, which could be catastrophic in this healthcare scenario where missing a disease diagnosis could lead to severe consequences.

Evaluate the model using the Area Under the ROC Curve (AUC) to understand its performance across different classification thresholds - AUC-ROC is valuable for understanding the model's performance across thresholds, but it doesn't provide the same direct focus on balancing precision and recall as the F1 score does. In a healthcare scenario with high stakes, the direct balance offered by F1 score and a focus on recall are more critical.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html

Temática
ML Model Development
Pregunta 62

You are the senior project manager at a global e-commerce company that runs multiple machine learning projects, including recommendation systems, fraud detection, and demand forecasting. The company has a cloud budget that is tightly monitored, and you are required to provide detailed reports on the costs associated with each ML project. To do this effectively, you need to track and allocate costs across different teams and projects, ensuring that each project stays within its allocated budget.

Which of the following approaches is the MOST EFFECTIVE for tracking and allocating costs across your ML projects using AWS services?

Use Amazon CloudWatch to monitor usage metrics for each resource and manually calculate the associated costs based on the metrics. Allocate costs by assigning each resource to a specific project or team

Create separate AWS accounts for each ML project, allowing costs to be isolated and tracked at the account level. Manually aggregate the costs for reporting purposes using monthly billing statements

Respuesta correcta
Implement resource tagging across all AWS resources used by your ML projects, including SageMaker instances, S3 buckets, and Lambda functions. Use AWS Cost Explorer to filter costs by tags such as project, team, and environment to generate detailed cost reports

Set up AWS Budgets for each project and rely on the alerts when the budget threshold is exceeded. Use these alerts to monitor costs and manually adjust resource usage as needed

Explicación general
Correct option:

Implement resource tagging across all AWS resources used by your ML projects, including SageMaker instances, S3 buckets, and Lambda functions. Use AWS Cost Explorer to filter costs by tags such as project, team, and environment to generate detailed cost reports

Resource tagging is a best practice for cost tracking and allocation in AWS.

AWS Cost Explorer allows you to analyze your past AWS spending, identify cost trends, and forecast future costs based on historical data. This tool is valuable for budgeting and financial planning, helping you make informed decisions about resource allocation and cost management. By tagging resources with metadata such as project name, team, and environment, you can use AWS Cost Explorer to break down costs by these tags, providing detailed insights into where your budget is being spent.

This approach allows for automated, granular tracking and reporting of costs across multiple ML projects, making it easier to stay within budget.

 via - https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/billing-what-is.html

Incorrect options:

Create separate AWS accounts for each ML project, allowing costs to be isolated and tracked at the account level. Manually aggregate the costs for reporting purposes using monthly billing statements - Creating separate AWS accounts for each project provides isolation but can lead to higher management overhead and complexity. Aggregating costs manually across accounts is time-consuming and less efficient than using resource tags within a single account or consolidated billing setup.

Use Amazon CloudWatch to monitor usage metrics for each resource and manually calculate the associated costs based on the metrics. Allocate costs by assigning each resource to a specific project or team - While CloudWatch provides valuable usage metrics, manually calculating costs from these metrics is cumbersome and error-prone. This approach does not offer the same level of automation or granularity as tagging and using Cost Explorer.

Set up AWS Budgets for each project and rely on the alerts when the budget threshold is exceeded. Use these alerts to monitor costs and manually adjust resource usage as needed - AWS Budgets is useful for setting and monitoring budget thresholds, but it is reactive rather than proactive. It provides alerts but lacks the detailed cost breakdowns needed for effective tracking and allocation across multiple projects.

References:

https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html

https://aws.amazon.com/aws-cost-management/aws-cost-explorer/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 63

You are a machine learning engineer responsible for maintaining an ML pipeline that processes customer transaction data to detect fraudulent activity. The pipeline includes several stages: data ingestion, data preprocessing, model inference, and result storage. The pipeline runs continuously, processing large volumes of data in near real-time. Recently, you’ve noticed that some transactions are being incorrectly classified as non-fraudulent, raising concerns about potential anomalies or errors in the data processing or model inference stages.

Given the critical nature of the pipeline, which approach is the MOST EFFECTIVE for addressing these issues?

Use AWS Lambda functions to log and audit every transaction processed by the pipeline, storing detailed logs in Amazon S3 for manual analysis when anomalies are suspected

Set up Amazon CloudWatch alarms to monitor CPU and memory usage across the pipeline’s compute resources, and trigger notifications if these metrics exceed predefined thresholds

Schedule periodic manual reviews of a random sample of processed transactions to check for anomalies or errors, documenting any issues for further investigation

Respuesta correcta
Implement Amazon SageMaker Model Monitor to track the distribution of input data features and model predictions over time, alerting you when deviations from expected patterns occur

Explicación general
Correct option:

Implement Amazon SageMaker Model Monitor to track the distribution of input data features and model predictions over time, alerting you when deviations from expected patterns occur

Amazon SageMaker Model Monitor is designed to track the distribution of input data features and monitor model predictions for anomalies. It provides alerts when deviations from expected patterns are detected, making it an effective tool for detecting data drift, concept drift, and other issues that could lead to incorrect classifications.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

Incorrect options:

Set up Amazon CloudWatch alarms to monitor CPU and memory usage across the pipeline’s compute resources, and trigger notifications if these metrics exceed predefined thresholds - While monitoring CPU and memory usage can help identify performance issues, it does not directly address anomalies or errors in data processing or model inference. These metrics are not sufficient to detect issues like data drift or changes in model behavior.

Schedule periodic manual reviews of a random sample of processed transactions to check for anomalies or errors, documenting any issues for further investigation - Manual reviews can be helpful for spot-checking, but they are not scalable or reliable for continuous monitoring, especially in a high-volume, real-time pipeline. Automated monitoring is more effective for catching issues as they arise.

Use AWS Lambda functions to log and audit every transaction processed by the pipeline, storing detailed logs in Amazon S3 for manual analysis when anomalies are suspected - Logging and auditing every transaction with AWS Lambda functions can provide detailed insights, but it is resource-intensive and does not offer real-time anomaly detection. This approach is better suited for post-mortem analysis rather than proactive monitoring.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 64

Which of the following are examples of supervised learning? (Select two)

Selección correcta
Neural network

Document classification

Association rule learning

Selección correcta
Linear regression

Clustering

Explicación general
Correct options:

Supervised learning algorithms train on sample data that specifies both the algorithm's input and output. For example, the data could be images of handwritten numbers that are annotated to indicate which numbers they represent. Given sufficient labeled data, the supervised learning system would eventually recognize the clusters of pixels and shapes associated with each handwritten number.

 via - https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/

Linear regression

Linear regression refers to supervised learning models that, based on one or more inputs, predict a value from a continuous scale. An example of linear regression is predicting a house price. You could predict a house’s price based on its location, age, and number of rooms after you train a model on a set of historical sales training data with those variables.

Neural network

A neural network solution is a more complex supervised learning technique. To produce a given outcome, it takes some given inputs and performs one or more layers of mathematical transformation based on adjusting data weightings. An example of a neural network technique is predicting a digit from a handwritten image.

Incorrect options:

Document classification - This is an example of semi-supervised learning. Semi-supervised learning is when you apply both supervised and unsupervised learning techniques to a common problem. This technique relies on using a small amount of labeled data and a large amount of unlabeled data to train systems. When applying categories to a large document base, there may be too many documents to physically label. For example, these could be countless reports, transcripts, or specifications. Training on the unlabeled data helps identify similar documents for labeling.

Association rule learning - This is an example of unsupervised learning. Association rule learning techniques uncover rule-based relationships between inputs in a dataset. For example, the Apriori algorithm conducts market basket analysis to identify rules like coffee and milk often being purchased together.

Clustering - Clustering is an unsupervised learning technique that groups certain data inputs, so they may be categorized as a whole. There are various types of clustering algorithms depending on the input data. An example of clustering is identifying different types of network traffic to predict potential security incidents.

References:

https://aws.amazon.com/what-is/machine-learning/

https://aws.amazon.com/compare/the-difference-between-machine-learning-supervised-and-unsupervised/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 65

A retail company uses a recommendation model deployed on an Amazon SageMaker endpoint to provide product suggestions to customers in real time. The company has developed a new version of the recommendation model and wants to evaluate its performance using live customer data without affecting the current production model. The company needs to determine whether the new model provides better recommendations before replacing the production model.

What do you recommend?

Respuesta correcta
Use Amazon SageMaker shadow testing to route a copy of live customer data to the new model for evaluation while maintaining the production model’s operation. Compare predictions from both models to assess performance

Replace the current production model with the new model on the existing SageMaker endpoint, monitor its performance, and roll back if issues are detected

Use Amazon SageMaker multi-model endpoints to deploy both the current and new models simultaneously and manually analyze the logs for prediction comparisons

Deploy the new model to a separate Amazon SageMaker endpoint and use a custom application to split live traffic between the endpoints for evaluation purposes

Explicación general
Correct option:

Use Amazon SageMaker shadow testing to route a copy of live customer data to the new model for evaluation while maintaining the production model’s operation. Compare predictions from both models to assess performance

Amazon SageMaker shadow testing is the most efficient and reliable solution for evaluating a new model's performance using live data without impacting production systems. In shadow testing, a copy of the live traffic is routed to the new model for predictions while the production model continues to serve end users. This setup allows the company to compare predictions from both models in real time and assess the new model’s performance under actual usage conditions.

Shadow testing ensures that the end-user experience remains unaffected, as the new model operates in parallel without influencing the production results. Additionally, this approach minimizes risks by enabling side-by-side evaluation before deciding whether to replace the production model. The automated nature of shadow testing also reduces operational overhead compared to custom or manual solutions, making it the ideal choice for this scenario.

Incorrect options:

Deploy the new model to a separate Amazon SageMaker endpoint and use a custom application to split live traffic between the endpoints for evaluation purposes - This approach requires significant custom development effort to split and manage live traffic. It introduces additional complexity compared to SageMaker’s built-in shadow testing feature.

Replace the current production model with the new model on the existing SageMaker endpoint, monitor its performance, and roll back if issues are detected - Replacing the production model directly risks exposing users to potential performance issues with the new model. It does not allow for a side-by-side comparison without impacting end users.

Use Amazon SageMaker multi-model endpoints to deploy both the current and new models simultaneously and manually analyze the logs for prediction comparisons - Multi-model endpoints are designed for hosting multiple models cost-effectively, but they do not provide built-in mechanisms for routing live traffic or performing comparisons automatically.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/shadow-tests.html