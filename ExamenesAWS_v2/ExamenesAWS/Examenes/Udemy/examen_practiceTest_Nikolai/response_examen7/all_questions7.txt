Pregunta 1

You are a data scientist at a healthcare company tasked with building a predictive model using SageMaker. Your team wants to avoid the complexity of managing infrastructure and prefers a solution that minimizes setup time.

Which of the following SageMaker features would be MOST appropriate for your use case?

Respuesta correcta
Use SageMaker’s built-in algorithms, which come pre-packaged in containers, requiring only data input and hyperparameter setup.

Explicación
SageMaker’s built-in algorithms are pre-packaged, with AWS managing the infrastructure, making it easy to get started with only data input and hyperparameter setup.

Build your own deep learning model using TensorFlow or PyTorch and manually deploy it on an Amazon EC2 instance.

Explicación
Building a deep learning model from scratch using custom frameworks like TensorFlow or PyTorch involves manual infrastructure setup, which the team wants to avoid.

Build a custom container with proprietary algorithms and manage the infrastructure manually for optimized performance.

Explicación
Custom containers require more manual setup and infrastructure management, which is not aligned with the team’s goal of avoiding complexity.

Use SageMaker Ground Truth to automatically handle infrastructure setup and dataset labeling.

Explicación
SageMaker Ground Truth is a data labeling service, not directly related to training and deploying models with built-in algorithms.

Temática
ML Model Development
Pregunta 2

A retail company is using Amazon Kinesis Data Firehose to stream clickstream data to Amazon S3. Occasionally, the data fails transformation due to malformed records. The company wants to ensure that these failed records are captured for further analysis. What is the best approach to handle this scenario?

Enable error logging in Amazon CloudWatch to identify and fix the records manually.

Explicación
Although logging errors in CloudWatch is useful for monitoring, it does not capture the actual malformed records for later analysis.

Respuesta correcta
Configure Amazon Kinesis Data Firehose to send failed records to a separate Amazon S3 bucket for analysis using the backup configuration.

Explicación
Kinesis Data Firehose supports a backup configuration where records that fail to be transformed are sent to a backup S3 bucket. This allows the company to analyze the malformed records later.

Configure Amazon Kinesis Data Firehose to use a retry policy to reprocess failed records indefinitely until successful.

Explicación
A retry policy for Firehose will not resolve the issue of malformed records, as the transformation would keep failing indefinitely.

Enable data transformation with AWS Lambda and configure a Dead Letter Queue (DLQ) to capture malformed records that cannot be processed.

Explicación
Kinesis Data Firehose does not support direct integration with Dead Letter Queues (DLQ); DLQs are more commonly used in services like Amazon SQS or Lambda directly.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 3

A company is training a large image classification model on SageMaker using PyTorch, but the training process is taking much longer than expected. The machine learning engineer suspects there is a resource bottleneck. They decide to use SageMaker Profiler to diagnose and fix the issue.

Which of the following actions should the engineer take to identify and resolve the bottleneck? (Choose two.)

Set up custom metrics to monitor memory allocation, and adjust the number of epochs if memory usage is too high.

Explicación
Memory usage can be monitored, but adjusting the number of epochs is not typically related to resolving high memory usage.

Selección correcta
Use SageMaker Profiler to monitor disk I/O and reduce the dataset size if slow disk reads are detected.

Explicación
Monitoring disk I/O with SageMaker Profiler can reveal slow data access, which could be a bottleneck in training. If disk reads are slow, reducing dataset size or optimizing data access can improve training time.

Adjust the learning rate dynamically during training based on real-time profiler metrics.

Explicación
Adjusting the learning rate based on profiler metrics is unrelated to resource bottlenecks like GPU or disk I/O issues.

Selección correcta
Use SageMaker Profiler to monitor GPU utilization in real-time and check for underutilization.

Explicación
Monitoring GPU utilization in real-time helps identify whether the GPUs are underutilized, which could slow down training. This allows the engineer to optimize GPU usage.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 4

A retail company wants to process large volumes of real-time customer interaction data to personalize user experience on their website. They plan to use Amazon Kinesis to ingest, analyze, and store the streaming data. They need to perform complex analytics on the data in real-time and store it for future analysis. Which combination of services should they use to achieve these objectives?

Use Kinesis Firehose for data ingestion, Kinesis Data Analytics for real-time analysis, and AWS Glue for data storage in Amazon S3.

Explicación
Kinesis Firehose is for delivering data, not ingestion, and AWS Glue is used for ETL jobs, not for real-time data streaming or storage.

Use Kinesis Data Streams for data ingestion, Kinesis Firehose for real-time analysis, and Kinesis Data Analytics to store the processed data in Amazon RDS.

Explicación
Kinesis Firehose is not used for real-time analytics, and Kinesis Data Analytics cannot directly store data in Amazon RDS.

Respuesta correcta
Use Kinesis Data Streams for data ingestion, Kinesis Data Analytics for real-time analysis, and Kinesis Firehose to deliver the processed data to Amazon S3.

Explicación
Kinesis Data Streams is used for ingesting real-time data, Kinesis Data Analytics (now Managed Apache Flink) performs real-time analytics with SQL-like queries, and Kinesis Firehose delivers the processed data to Amazon S3, a common long-term storage solution.

Use Kinesis Data Streams for data ingestion, AWS Lambda for real-time analysis, and Kinesis Firehose to deliver the processed data to Amazon Redshift.

Explicación
AWS Lambda is not suited for complex real-time analytics and is more appropriate for event-driven serverless functions.

Temática
Data Preparation for Machine Learning
Pregunta 5

An e-commerce platform ingests high volumes of real-time customer behavior data and has multiple applications consuming this data. Due to high throughput needs, the company wants to avoid bottlenecks when scaling the number of consumers. Which Kinesis Data Streams feature should they configure?

Provisioned Throughput

Explicación
Provisioned Throughput involves manual management of shard throughput, which does not directly solve the bottleneck problem for multiple consumers.

Respuesta correcta
Enhanced Fan-Out

Explicación
Enhanced Fan-Out enables each consumer to have its own dedicated throughput of 2 MB/s per shard, preventing bottlenecks when there are multiple consumers accessing the data stream.

On-Demand Sharding

Explicación
On-Demand Sharding scales the number of shards automatically but doesn't address the consumer bottleneck issue.

Partition Key

Explicación
A Partition Key helps distribute data across shards for parallel processing but doesn’t resolve bottlenecks from multiple consumers.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 6

What is the main difference between identity-based policies and resource-based policies in AWS IAM?

Resource-based policies must always be AWS managed policies, while identity-based policies must be inline policies.

Explicación
Both identity-based and resource-based policies can be managed or inline; there is no such restriction.

Identity-based policies can only be applied to groups, while resource-based policies can only be applied to individual users.

Explicación
Identity-based policies can be applied to both users, groups, and roles, while resource-based policies are specific to resources.

Identity-based policies are applied directly to AWS resources, while resource-based policies are attached to users, groups, or roles.

Explicación
Identity-based policies are applied to identities (users, groups, roles) while resource-based policies are attached to AWS resources like S3 buckets.

Respuesta correcta
Identity-based policies are applied to IAM users, groups, or roles, whereas resource-based policies are directly attached to AWS resources like S3 buckets.

Explicación
Identity-based policies apply to IAM identities (users, roles, groups), and resource-based policies are attached to resources such as S3 buckets or EC2 instances.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 7

A machine learning team wants to track the history of model versions, including training data, hyperparameters, and performance metrics over time. Which feature of SageMaker should they use?

SageMaker Notebook Instances

Explicación
SageMaker Notebook Instances are used for development and experimentation, not for version tracking of models.

Respuesta correcta
SageMaker Model Registry

Explicación
The SageMaker Model Registry is designed to track different versions of a model, including training data, hyperparameters, performance metrics, and other metadata. It helps manage model versions and ensures that important information is centralized and easy to retrieve.

SageMaker Pipelines

Explicación
SageMaker Pipelines automate ML workflows but don’t focus on tracking model version history.

SageMaker Training Jobs

Explicación
SageMaker Training Jobs are used to run model training, not to manage or track model versions over time.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 8

An organization uses AWS Secrets Manager to store sensitive data such as API keys and database credentials. They want to share a secret with another AWS account without replicating it. How can they achieve this securely while ensuring the secret remains encrypted?

Share the secret through an encrypted S3 bucket that the other account has access to, and send the KMS decryption key separately.

Explicación
Sharing through S3 introduces unnecessary complexity and does not leverage AWS Secrets Manager's built-in features for cross-account access.

Use the Secrets Manager API to export the secret and share it with the other account via email.

Explicación
Sharing secrets through email is insecure and not an appropriate method for sensitive data management.

Use AWS Systems Manager to create a shared parameter that references the secret across accounts.

Explicación
AWS Systems Manager does not directly reference secrets from AWS Secrets Manager across accounts; Secrets Manager is the appropriate service for this use case.

Respuesta correcta
Attach a resource policy to the secret that grants read access to an IAM role in the other account, and ensure that the KMS key policy allows decryption by the same role.

Explicación
By attaching a resource policy to the secret, you can securely grant access to the secret to an IAM role in another AWS account. Additionally, modifying the KMS key policy ensures that the other account can decrypt the secret, maintaining encryption and security.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 9

A financial institution is using Amazon Athena to run SQL queries on its transaction logs stored in Amazon S3. They need a scalable way to keep track of schema changes in these logs, as the structure of the logs may change over time. Additionally, they want to ensure the queries are fast and cost-effective.

Which of the following is the most effective approach to ensure efficient schema management and optimized query performance?

Create an Amazon Athena table with a hardcoded schema and manually update it whenever the log structure changes.

Explicación
Hardcoding a schema in Athena and manually updating it is time-consuming and error-prone. AWS Glue provides an automated solution for this.

Store the transaction logs in Amazon RDS and create a pipeline to replicate them to Amazon Athena for better querying capabilities.

Explicación
Storing logs in Amazon RDS is not necessary for Athena queries. Amazon Athena is built to query data directly from S3 without needing a separate database.

Respuesta correcta
Use AWS Glue Crawlers to automatically detect schema changes and store the metadata in the Glue Data Catalog for Amazon Athena to query.

Explicación
AWS Glue Crawlers are designed to scan data sources, detect schema changes, and store the updated schema in the Glue Data Catalog. This ensures that Amazon Athena can always access up-to-date metadata and query the data efficiently.

Enable S3 Select to query only the necessary parts of the logs, ensuring that schema changes are handled automatically.

Explicación
S3 Select is useful for retrieving specific parts of data from S3 objects but does not handle schema changes or optimize query performance in Athena.

Temática
Data Preparation for Machine Learning
Pregunta 10

A machine learning team is using Amazon SageMaker Ground Truth to label a dataset for an object detection task. They want to reduce labeling costs without sacrificing quality. What strategy provided by Ground Truth would BEST help the team achieve this?

Fully manual labeling by a private workforce within the organization.

Explicación
Fully manual labeling increases costs since every data point must be handled by humans, which is expensive and time-consuming.

Use a completely automated machine learning model to handle all labeling tasks.

Explicación
Fully automated labeling may reduce costs but risks losing accuracy, especially for more complex cases.

Utilize a third-party vendor for all labeling tasks to ensure the highest accuracy.

Explicación
Using a third-party vendor could provide accuracy but may not reduce costs as effectively as the hybrid active learning approach.

Respuesta correcta
Implement active learning, where a model labels simple cases, and human labelers handle complex cases.

Explicación
Active learning combines machine learning and human intervention. The model handles simpler labeling tasks while human labelers focus on more complex cases, optimizing both cost and accuracy.

Temática
Data Preparation for Machine Learning
Pregunta 11

A data scientist is working on a binary classification problem to detect fraudulent transactions. The model has a high precision but a relatively low recall. What does this indicate about the model’s performance?

Respuesta correcta
The model has a high rate of correctly predicted frauds, but misses many actual frauds.

Explicación
High precision means the model is correctly identifying a large proportion of the positive predictions as actual frauds, but low recall indicates it is missing many true frauds (actual positives) in the dataset.

The model is accurately identifying most of the fraudulent transactions.

Explicación
A high recall would indicate the model is identifying most fraudulent transactions, which is not the case here.

The model's predictions are consistent, but it makes many false positive predictions.

Explicación
High precision means fewer false positives, not more.

The model is poorly identifying true negatives.

Explicación
Precision and recall focus on the positive class (frauds), not on true negatives.

Temática
ML Model Development
Pregunta 12

Team of data scientists is working on a computer vision project and wants to leverage AWS SageMaker’s built-in algorithms. They plan to use SageMaker JumpStart to streamline the process. What are the key advantages of using SageMaker JumpStart for this project?

JumpStart forces data scientists to only use SageMaker Studio for deployment, limiting flexibility.

Explicación
JumpStart offers multiple options for deploying models, including SageMaker Studio and the SageMaker Python SDK, providing flexibility in how models are managed and deployed.

Using JumpStart will eliminate the need for any hyperparameter tuning and data preparation.

Explicación
While JumpStart reduces effort by providing pre-trained models and pipelines, data scientists may still need to perform hyperparameter tuning and customize the data processing for better results.

SageMaker JumpStart only supports text-based models, so it will not be useful for image classification tasks.

Explicación
JumpStart supports various types of models, including those for image classification and text generation, making it highly versatile.

Respuesta correcta
SageMaker JumpStart allows the use of pre-built solutions that come with end-to-end pipelines, reducing time to deployment.

Explicación
SageMaker JumpStart provides access to pre-built solutions that include data processing pipelines, pre-trained models, and deployment configurations. This makes it easier for the data science team to quickly set up and deploy their computer vision models without building everything from scratch.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 13

An organization needs to train a machine learning model with a custom algorithm that is not natively supported by SageMaker's built-in algorithms. The organization prefers to use SageMaker’s pre-built containers for framework support, but with custom training and inference code.
Which approach should the organization take?

Use SageMaker's built-in algorithms and modify them to include custom training code.

Explicación
SageMaker's built-in algorithms cannot be easily modified to include custom training code, making this approach infeasible.

Write custom training code directly in the SageMaker notebook instance and run it using the notebook’s execution kernel.

Explicación
Running custom training code directly on the notebook instance does not leverage SageMaker’s managed training and scaling features, which are essential for efficient model training and deployment.

Build a custom Docker container from scratch, package all dependencies, and upload it to Amazon ECR for use in SageMaker.

Explicación
While building a custom Docker container is an option for more control, SageMaker Script Mode simplifies the process by allowing users to focus on training and inference code without dealing with container management.

Respuesta correcta
Utilize SageMaker’s Script Mode, write the custom training logic in a Python script, and specify the entry point in a SageMaker estimator.

Explicación
SageMaker’s Script Mode allows users to bring their own training code while leveraging pre-built containers for frameworks like TensorFlow, PyTorch, and scikit-learn. Users need to write the custom training logic in a Python script and specify it as the entry point in the estimator.

Temática
ML Model Development
Pregunta 14

You are tasked with training a deep neural network model with billions of parameters using Amazon SageMaker. The model is too large to fit into a single GPU, and the dataset is also substantial. Which SageMaker technique should you use to ensure efficient training?

Respuesta correcta
Model parallelism, where the model itself is split across multiple GPUs, and each GPU handles a portion of the model.

Explicación
Model parallelism is the most appropriate technique when the model is too large to fit into a single GPU. It divides the model across multiple GPUs, allowing each GPU to process a part of the model.

Use pre-built SageMaker containers without any adjustments for distributed training.

Explicación
Using pre-built containers without enabling model parallelism would not solve the challenge of training a large model across multiple GPUs.



Data parallelism, where the dataset is split across multiple machines but the model remains on one GPU.

Explicación
Data parallelism only divides the dataset, not the model. This would not solve the issue of a large model that cannot fit on a single GPU.

Use only a single machine with larger memory to handle both the data and the model.

Explicación
Using a larger memory machine might not be efficient or possible for extremely large models. Distributed model parallelism is the proper approach for very large models.

Temática
ML Model Development
Pregunta 15

A global retail company collects customer feedback in various languages through email and web forms. The company wants to automatically detect customer sentiment, categorize the feedback into relevant product categories, and translate the feedback into English for further analysis by the product team. The solution should support scalability and automate the entire process.

Which combination of services would BEST meet the company’s needs?

Amazon Comprehend for both sentiment analysis and categorization, Amazon Translate for translation, and Amazon Kendra for providing search capabilities across the feedback.

Explicación
While Amazon Kendra is great for intelligent search, it is not needed for this use case, which focuses on sentiment analysis, translation, and categorization, not search.

Amazon Comprehend for sentiment analysis, Amazon Translate for translating feedback to English, and Amazon SageMaker for classifying feedback into product categories.

Explicación
Amazon SageMaker could be used to build a custom classification model, but Amazon Comprehend already provides built-in capabilities for text classification, making it more efficient in this case. Amazon Translate can handle the language translation, but this combination adds unnecessary complexity.



Amazon Rekognition for analyzing the text within images, Amazon Comprehend for categorizing feedback, and Amazon Translate for language translation.

Explicación
Amazon Rekognition is used for image and video analysis, not text analysis. Amazon Comprehend is more appropriate for text categorization and sentiment analysis.

Respuesta correcta
Amazon Translate for translating feedback, Amazon Comprehend for sentiment analysis and entity recognition, and AWS Lambda for automating the process.

Explicación
Amazon Translate can automatically translate the feedback into English, and Amazon Comprehend can handle both sentiment analysis and entity recognition (e.g., identifying product categories). AWS Lambda is a good choice for automating the orchestration of these services without the need for complex infrastructure.

Temática
Data Preparation for Machine Learning
Pregunta 16

A data science team is using SageMaker Ground Truth to perform text classification labeling. They are interested in reducing their labeling costs by automating some tasks while ensuring sensitive data is handled by trusted individuals.

Which combination of strategies should the team use to achieve this?

Set up Mechanical Turk workers for sensitive data and automate labeling for complex tasks.

Explicación
Mechanical Turk workers are not ideal for handling sensitive data as they are external contractors.

Rely entirely on private workforce labelers to handle both simple and complex tasks.

Explicación
Relying entirely on a private workforce can significantly increase costs and reduce efficiency without leveraging automation.

Respuesta correcta
Use a private workforce for sensitive data, and leverage active learning to automate simple tasks.

Explicación
A private workforce ensures sensitive data is handled securely, while active learning automates simple tasks, reducing costs and maintaining quality.

Use a third-party vendor for sensitive data while relying on fully automated models for all labeling tasks.

Explicación
Using third-party vendors for sensitive data may raise security concerns, and fully automated labeling for all tasks could sacrifice accuracy.

Temática
Data Preparation for Machine Learning
Pregunta 17

Which of the following is an example of unsupervised learning in machine learning?

Respuesta correcta
Grouping customers based on purchasing behavior without labeled categories

Explicación
Grouping customers based on their purchasing behavior is a clustering task, which is an example of unsupervised learning. In unsupervised learning, the model identifies patterns in data without labeled categories.

Forecasting sales for the next quarter

Explicación
Sales forecasting is a regression problem, a supervised learning task.

Predicting whether an email is spam or not

Explicación
Predicting whether an email is spam is a classification task, a type of supervised learning.

Predicting the price of a stock based on historical data

Explicación
Predicting stock prices is a regression problem, which is also a supervised learning task.

Temática
Data Preparation for Machine Learning
Pregunta 18

A company is building a machine learning pipeline using SageMaker. They want to automatically set up an environment where multiple data scientists can collaborate, manage various machine learning tasks, and store their work in a persistent and scalable environment. Which feature of SageMaker would best support these requirements?

AWS Glue

Explicación
AWS Glue is for ETL processes and not designed for managing machine learning development environments.

SageMaker Notebook Instances

Explicación
Although suitable for individual tasks, SageMaker Notebook Instances are not as collaborative or integrated as Studio.

Respuesta correcta
SageMaker Studio

Explicación
SageMaker Studio provides a fully integrated development environment (IDE) for machine learning with collaboration capabilities, persistent storage, and support for multiple projects. It’s ideal for teams of data scientists working on shared tasks.

Amazon EMR

Explicación
Amazon EMR is used for big data processing and does not provide the comprehensive environment needed for machine learning workflows.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 19

A machine learning engineer is tasked with building a predictive model for an e-commerce platform using Amazon SageMaker. The team is considering whether to use a SageMaker Notebook Instance or SageMaker Studio. The engineer needs the following features:
- The ability to run multiple notebooks in parallel within the same environment.
- A centralized space for persistent storage of datasets, notebooks, and logs.
- Seamless integration with existing AWS services like S3 and IAM.

Which SageMaker environment should the engineer choose, and why?

SageMaker Notebook Instance because it allows running multiple notebooks in isolated environments, ensuring that tasks remain independent.

Explicación
While Notebook Instances provide isolated environments, they are standalone and don't offer the ability to manage multiple notebooks in one centralized interface.

SageMaker Studio because it is specifically designed for large-scale deep learning workloads, offering GPU-based instances for parallel notebook execution.

Explicación
SageMaker Studio is not exclusively designed for deep learning workloads, though it supports GPU-based instances. The key advantage here is its centralized management of multiple notebooks.

SageMaker Notebook Instance because it provides access to the full AWS CLI, enabling tighter control over resources and configurations.

Explicación
While SageMaker Notebook Instances allow full CLI access, this is less relevant for a project that requires parallel notebook execution and integrated storage, which are better handled by SageMaker Studio.

Respuesta correcta
SageMaker Studio because it offers a persistent Jupyter Lab space where multiple notebooks can be managed, allowing centralized data access and integration with AWS services.

Explicación
SageMaker Studio provides an integrated Jupyter Lab environment where multiple notebooks can run in parallel, and it offers persistent storage for managing datasets and code. It also integrates seamlessly with AWS services like S3 for data storage and IAM for managing access control, making it the most suitable environment for collaborative, large-scale projects.

Temática
ML Model Development
Pregunta 20

Which of the following are valid sources for importing data into Amazon SageMaker Data Wrangler? (Choose TWO)

AWS Glue DataBrew

Explicación
AWS Glue DataBrew is a separate service for data preparation, not a source for importing data into Data Wrangler.

Selección correcta
Amazon S3

Explicación
Data Wrangler can import data from Amazon S3, making it a common source for datasets used in machine learning workflows.

Amazon Kinesis

Explicación
Amazon Kinesis is a streaming data service, but it is not directly integrated as a data source for SageMaker Data Wrangler.

Selección correcta
SageMaker Feature Store

Explicación
SageMaker Feature Store is another valid source for importing features into Data Wrangler for preprocessing and transformations.

Amazon RDS

Explicación
Amazon RDS is not directly listed as a supported data source for Data Wrangler imports.

Temática
Data Preparation for Machine Learning
Pregunta 21

An organization is working on a text classification problem and decides to use Amazon SageMaker's built-in algorithms. The team is concerned about optimizing model performance without manually tuning hyperparameters. What features of SageMaker can help address this requirement?

Utilize custom algorithms to bypass hyperparameter tuning altogether.

Explicación
Custom algorithms still need hyperparameter tuning in most cases, and bypassing it could lead to suboptimal performance.

Manually adjust hyperparameters for the built-in algorithm by trial and error.

Explicación
Manually adjusting hyperparameters can be time-consuming and inefficient, especially when SageMaker provides built-in features for automatic tuning.

Use SageMaker JumpStart to automatically deploy pre-trained text generation models without any customization.

Explicación
While SageMaker JumpStart offers pre-trained models, the question is focused on optimizing performance for a text classification task, which requires hyperparameter tuning for the custom dataset.

Respuesta correcta
Use SageMaker's automatic hyperparameter tuning with the built-in linear learner algorithm.

Explicación
SageMaker's automatic hyperparameter tuning feature can be used with built-in algorithms, such as the linear learner. This feature allows users to automatically search for the best hyperparameters that optimize model performance.

Temática
ML Model Development
Pregunta 22

A machine learning engineer is training a deep learning model on SageMaker using TensorFlow. During training, the model's performance stops improving, and the engineer suspects that the model may be experiencing vanishing gradients. They want to use SageMaker Debugger to monitor the training and resolve this issue. Which steps should the engineer take to identify and address the vanishing gradients using SageMaker Debugger?

Enable SageMaker Debugger to monitor resource utilization and set up alarms for underutilized GPUs.

Explicación
While SageMaker Debugger can monitor resources, vanishing gradients are a model-specific issue related to learning dynamics, not GPU utilization.

Run a custom SageMaker Debugger rule to visualize CPU and memory usage, then adjust the batch size to optimize resource utilization.

Explicación
Visualizing CPU and memory usage does not directly address vanishing gradients, which are a model training issue.

Respuesta correcta
Use SageMaker Debugger’s built-in rules to monitor the gradient values during training and adjust the learning rate if vanishing gradients are detected.

Explicación
SageMaker Debugger provides real-time monitoring for training jobs and includes built-in rules to detect vanishing gradients, which can hinder model improvement. The engineer can adjust learning rate or other hyperparameters when such an issue is detected, improving training results.



Utilize SageMaker Profiler to collect GPU utilization data and implement early stopping when vanishing gradients are detected.

Explicación
The SageMaker Profiler focuses on resource utilization (e.g., CPU/GPU), not model training metrics like gradients, so it wouldn’t help detect vanishing gradients.

Temática
ML Model Development
Pregunta 23

A company wants to ensure efficient ETL operations on a large dataset stored in Amazon S3 using AWS Glue. They decide to partition the dataset by the "year" and "month" fields to optimize query performance and reduce costs. Which of the following is the MOST significant advantage of partitioning data in this manner?

It ensures that the entire dataset is always scanned during each query.

Explicación
Partitioning reduces the amount of data scanned, not increases it. Only relevant partitions are scanned, which optimizes the process.

It eliminates the need for Glue crawlers to detect new data.

Explicación
Glue crawlers are still useful for detecting new partitions and updating the Glue Data Catalog, making partitioning more efficient.

Respuesta correcta
It allows Glue ETL jobs to process partitions independently, improving performance and reducing query costs.

Explicación
Partitioning allows AWS Glue to process each partition (e.g., by year and month) independently, leading to faster processing and reduced query costs, as only relevant partitions are scanned. This improves both performance and cost-efficiency in services like Athena, where cost is based on the amount of data scanned.

Partitioning ensures that data integrity is maintained across different S3 locations.

Explicación
Partitioning does not directly affect data integrity, but rather improves query performance and cost-efficiency.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 24

A healthcare company needs to ensure compliance with data privacy regulations like HIPAA and GDPR for their sensitive information stored in S3 buckets. They plan to use AWS Macie to monitor and secure this data. Which of the following are the key benefits of using AWS Macie in this scenario? (Choose TWO)

Selección correcta
Detects anomalous access patterns to identify potential unauthorized access to sensitive data.

Explicación
AWS Macie can detect anomalous access patterns that may indicate unauthorized access to sensitive data, helping to prevent data breaches.

Selección correcta
Automatically classifies sensitive data such as PII, financial data, and health records stored in S3.

Explicación
AWS Macie uses machine learning models to automatically classify sensitive data like PII, financial data, and health records, which is essential for regulatory compliance.

Monitors the performance of S3 buckets to ensure optimal read/write operations.

Explicación
Macie focuses on security and compliance, not on monitoring the performance of S3 buckets.

Automatically rotates sensitive data keys in S3 buckets for enhanced security.

Explicación
Macie does not rotate keys; AWS KMS or Secrets Manager would handle key management and rotation.

Provides encryption for sensitive data using AWS KMS without additional configuration.

Explicación
While AWS KMS provides encryption, Macie is not responsible for encryption configuration; its role is data classification and monitoring.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 25

Which of the following correctly describes the relationship between trials and trial components within SageMaker Experiments?

A trial component tracks the hardware resources used during training, while a trial compares different algorithms.

Explicación
Trial components do not track hardware resources; they capture stages in the ML workflow.

A trial captures different models trained in parallel, and trial components represent different datasets used.

Explicación
A trial represents a specific configuration, not parallel models, and trial components are not used to represent different datasets.

A trial consists of multiple experiments, and each experiment captures metadata about data processing.

Explicación
Trials do not consist of multiple experiments; an experiment holds multiple trials.

Respuesta correcta
A trial is a single training run with specific configurations, and trial components capture different stages of the workflow such as data processing or model training.

Explicación
A trial in SageMaker Experiments represents a single run of a training job with a specific configuration, and trial components capture the stages within that trial, such as data preprocessing, model training, and evaluation.

Temática
ML Model Development
Pregunta 26

Your team has deployed a machine learning model for batch inference on Amazon SageMaker. You want to monitor the model's performance and be notified if the model's accuracy declines over time, indicating a potential model quality drift.

Which steps should you take to set up model quality monitoring for this batch inference model using SageMaker Model Monitor? (Choose Two)

Selección correcta
Schedule monitoring jobs to capture the incoming data and baseline comparison results.

Explicación
SageMaker Model Monitor allows the setup of scheduled jobs to check for deviations in model performance, which is crucial for detecting drift.

Selección correcta
Merge the ground truth labels with the captured predictions to assess the model’s accuracy.

Explicación
This is necessary to evaluate how well the model's predictions match the actual outcomes, which is critical for assessing model quality.



Deploy a new version of the model every time drift is detected.

Explicación
Model Monitor alerts for drift, but this does not mean an immediate redeployment is necessary.



Manually track the model’s prediction results on a daily basis.

Explicación
SageMaker Model Monitor automates this process.

Enable CloudTrail logs to track the data drift in batch jobs.

Explicación
CloudTrail tracks API activity, not data drift or model quality.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 27

A data scientist is tasked with ensuring that their machine learning model is trustworthy, transparent, and avoids potential risks, including bias and poor generalization. They want to use a technique that assesses the model's fairness and explainability, both before and after training, while also meeting regulatory requirements.
Which AWS service can help them achieve this?



Amazon Personalize

Explicación
Amazon Personalize is a recommendation engine and is not designed to detect bias or ensure model fairness.

Amazon SageMaker Ground Truth

Explicación
Amazon SageMaker Ground Truth is used for creating and managing high-quality training datasets, not specifically for detecting or mitigating bias or ensuring explainability.

AWS Glue

Explicación
AWS Glue is primarily an ETL service, used for data transformation and does not address model fairness, transparency, or bias detection.

Respuesta correcta
Amazon SageMaker Clarify

Explicación
Amazon SageMaker Clarify is specifically designed to detect and mitigate biases in machine learning models, ensuring transparency, fairness, and explainability both during and after training. It provides pre-training and post-training bias detection and generates reports to guide actions to reduce bias.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 28

A data scientist is working with Amazon SageMaker Studio and needs to set up a JupyterLab environment for collaboration with their team. Which of the following best describes how SageMaker Studio notebooks are different from standalone SageMaker notebook instances?

Standalone SageMaker notebook instances have built-in collaboration tools, whereas SageMaker Studio notebooks do not support collaboration.

Explicación
SageMaker Studio provides built-in collaboration tools, allowing multiple users to work within the same environment, whereas standalone notebook instances are typically used individually.

SageMaker Studio notebooks are pre-configured only for data preparation tasks and cannot be used for model training or deployment.

Explicación
SageMaker Studio notebooks can be used for the entire ML lifecycle, from data preparation to model training and deployment.

Respuesta correcta
SageMaker Studio notebooks are integrated within SageMaker Studio, allowing persistent storage and multiple notebooks per project, while standalone notebook instances operate independently.

Explicación
SageMaker Studio notebooks are integrated within SageMaker Studio, offering persistent storage, collaboration features, and the ability to manage multiple notebooks in a single JupyterLab environment. Standalone instances operate independently and are not as tightly integrated into a collaborative interface.

SageMaker Studio notebooks are for lightweight tasks, whereas standalone SageMaker notebook instances are used for heavy machine learning workloads.

Explicación
Both types of notebooks can handle heavy workloads and full ML pipelines, from data preparation to model training and deployment.



Temática
ML Model Development
Pregunta 29

Which of the following is the most appropriate problem type for regression models?

Grouping users into segments based on browsing patterns without knowing specific user behaviors in advance.

Explicación
This is an unsupervised learning problem, specifically clustering, where users are grouped without labeled data or predefined segments.

Predicting whether a customer will churn or not based on past behavior.

Explicación
This is a binary classification problem, a type of supervised learning, where the outcome is categorical (churn or not).

Classifying images into different categories such as cats, dogs, and birds.

Explicación
This is a multi-class classification problem, which involves predicting categories like cats, dogs, or birds.

Respuesta correcta
Estimating the future sales revenue based on advertising spend and historical sales data.

Explicación
Regression is used for predicting continuous numerical values, such as predicting sales revenue based on input features like advertising spend.

Temática
ML Model Development
Pregunta 30

A machine learning team is using Amazon SageMaker Experiments to evaluate different model configurations. They want to track and compare the performance of several training runs with varying hyperparameters, data features, and algorithms to identify the best performing model. What components in SageMaker Experiments should they use to organize and track these training runs?

Hyperparameters, Experiments, Trackers, Notebooks

Explicación
While experiments and trackers are correct, hyperparameters and notebooks are not SageMaker Experiment components.

Trackers, Algorithms, Metrics, Pipelines

Explicación
Algorithms and metrics are tracked as part of experiments, but they are not core components of SageMaker Experiments.

Pipelines, Trials, Stages, Logs

Explicación
Pipelines and stages are not part of SageMaker Experiments, though pipelines may be used for automation outside of the experiment tracking.

Respuesta correcta
Experiments, Trials, Trial Components, Trackers

Explicación
The team should organize their work into Experiments, which group multiple Trials (individual training runs). Each trial is composed of Trial Components that capture different stages of the machine learning process (e.g., data preprocessing, training). Trackers are used to log the inputs (such as hyperparameters) and outputs (such as metrics) for reproducibility and traceability.

Temática
Data Preparation for Machine Learning
Pregunta 31

A machine learning engineer is tasked with training a custom PyTorch model using Amazon SageMaker. They want to take advantage of SageMaker's pre-built PyTorch container while maintaining control over the training logic. The engineer also needs to include several custom Python libraries to handle specific pre-processing tasks.

What steps should the engineer follow to set up and train the model using SageMaker's Script Mode?

Use AWS Lambda to trigger a training job, passing the model and training script from the notebook instance.

Explicación
AWS Lambda is not typically used to run SageMaker training jobs; the SageMaker estimator handles the entire training job, including setting up the environment and managing the instance types.

Create a custom Docker container for the model, push it to Amazon ECR, and pull it into SageMaker for training.

Explicación
Creating a custom Docker container is unnecessary when using Script Mode since SageMaker already provides pre-built containers for popular frameworks like PyTorch. Custom Docker containers are used when more control is needed over the environment.

Respuesta correcta
Write a Python script with the training logic, specify it as an entry point in the SageMaker estimator, and include a requirements.txt file to install custom libraries during training.

Explicación
In Script Mode, the engineer can use SageMaker's pre-built containers for PyTorch and write a custom Python script for training. By specifying the script as an entry point in the SageMaker estimator and using a requirements.txt file, the engineer can easily include any additional libraries required during the training process.

Install the required Python packages directly on the SageMaker notebook instance and run the training using the SageMaker built-in algorithms.

Explicación
While custom Python packages can be installed on the notebook instance, this approach would not automate the inclusion of dependencies in the training job itself, which is done via SageMaker's estimator in Script Mode.

Temática
ML Model Development
Pregunta 32

A retail company is developing a machine learning model to forecast product demand using Amazon SageMaker. The dataset includes sales history, customer demographics, and seasonal trends. The data is stored in S3 and updated frequently. The company wants to automate the workflow for data preprocessing, feature engineering, and model training. They also need to ensure that the model is continuously retrained as new data arrives, and performance degradation is monitored. Additionally, the company wants to track different versions of the model and hyperparameter configurations during experimentation.
Which solution should the company implement?

Use SageMaker Feature Store for real-time feature updates, SageMaker Clarify to monitor data bias, and SageMaker Pipelines to retrain the model based on new data.

Explicación
Clarify is useful for monitoring bias but not necessary for this demand forecasting scenario. This option also lacks Model Monitor to track model performance degradation and Experiments to track model versions and hyperparameter settings.

Respuesta correcta
Use SageMaker Pipelines to automate data preprocessing, model training, and retraining, SageMaker Model Monitor to detect performance degradation, and SageMaker Experiments to track different model versions and hyperparameter settings.

Explicación
This combination fully automates the machine learning workflow: SageMaker Pipelines orchestrates the entire process, including data preprocessing, training, and retraining when new data is added. SageMaker Model Monitor detects performance degradation or data drift, triggering retraining when necessary. SageMaker Experiments allows the company to track and compare different model versions and hyperparameter settings, making it easier to choose the best model.

Use SageMaker Data Wrangler for data preprocessing, SageMaker Feature Store for feature engineering, and SageMaker Autopilot to automate the model training and hyperparameter tuning process.

Explicación
While this option covers data preprocessing and training, it lacks automation for retraining based on performance degradation and doesn't provide a method to track different model versions, which is a critical requirement.

Use SageMaker Ground Truth to label data, SageMaker Model Monitor to automate model retraining, and SageMaker Autopilot for hyperparameter optimization.

Explicación
Ground Truth is used for labeling data, which is irrelevant in this case as the dataset (sales history, customer demographics) likely doesn't require labeling. Also, Model Monitor tracks data drift but doesn’t automate retraining. This solution also lacks a feature for managing different model versions.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 33

A machine learning engineer is working on a binary classification problem for detecting fraudulent transactions. The model's performance is being evaluated using multiple metrics. The model has a precision of 90% and a recall of 60%. The engineer is concerned about missing too many fraudulent transactions.

Which evaluation metric should the engineer focus on to improve the model’s ability to identify more fraud cases?

Respuesta correcta
Recall

Explicación
The recall measures the proportion of actual positives (fraud cases) that are correctly identified by the model. A recall of 60% means the model is missing 40% of the actual fraud cases. To reduce the number of missed fraud cases, the engineer should focus on improving recall, even if it slightly reduces precision.

Accuracy

Explicación
Accuracy measures the proportion of correct predictions out of all predictions. In imbalanced datasets (like fraud detection), accuracy may not provide meaningful insights, as it doesn’t specifically address false negatives (missed fraud cases).

Precision

Explicación
Precision focuses on the proportion of predicted positives that are correct. While the model has high precision, the engineer is concerned about missing fraud cases, so recall is more important in this context.

F1 Score

Explicación
The F1 score is the harmonic mean of precision and recall. While useful for balancing both metrics, in this case, the engineer is more concerned with improving recall to catch more fraud cases.

Temática
ML Model Development
Pregunta 34

A media company wants to automate the content moderation process for user-submitted images and accompanying text comments. They need to identify inappropriate content such as violent imagery in photos and offensive language in the text. The solution should automatically flag inappropriate submissions and store the flagged data for further review.

Which combination of services should the company use to implement this automated system?

Amazon Rekognition to detect inappropriate content in images, Amazon Comprehend for sentiment analysis of the text, and Amazon DynamoDB for storing flagged data.

Explicación
Amazon Comprehend is a good choice for text analysis, but sentiment analysis is not specifically designed to detect offensive language. Amazon Comprehend should be used with custom classifiers or built-in toxicity detection for this purpose.

Amazon Comprehend to detect sentiment and entity extraction from text, Amazon Lex for conversational moderation, and Amazon Elasticsearch for storing flagged data.

Explicación
Amazon Lex is not required here, as there is no need for conversational AI. Also, Amazon Elasticsearch is more appropriate for search-based applications rather than storing and reviewing flagged content.

Respuesta correcta
Amazon Rekognition for content moderation of images, Amazon Comprehend for identifying offensive language in the text, and Amazon S3 for storing flagged data.

Explicación
Amazon Rekognition can detect inappropriate or violent content in images, and Amazon Comprehend can analyze text for offensive language using its natural language processing capabilities. Amazon S3 is a good choice for storing flagged data since it scales easily and integrates well with other AWS services.

Amazon SageMaker to build custom models for detecting inappropriate images, Amazon Comprehend for text analysis, and Amazon RDS for storing flagged data.

Explicación
While Amazon SageMaker can be used for custom model building, Amazon Rekognition already provides pre-trained models for image moderation, which is simpler and more cost-effective. Amazon Comprehend can handle text analysis, but Amazon RDS is more structured storage, while S3 is more commonly used in such moderation workflows for storing flagged data.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 35

A machine learning engineer is using SageMaker Data Wrangler to preprocess data from multiple sources. They want to analyze the relationships between different features in the dataset before selecting which features to include in model training. The engineer also needs to understand which features might have the greatest impact on the target variable. Which two functionalities of Data Wrangler will help the engineer achieve this? (Choose TWO)

Recursive feature elimination.

Explicación
Recursive feature elimination is not a feature provided directly by Data Wrangler; it’s a separate feature selection technique used in model development, often implemented using specific machine learning algorithms.

Feature selection through automatic feature scaling.

Explicación
Feature scaling is part of the preprocessing steps (like normalization), but it is not a method to analyze feature relationships or importance.

Selección correcta
Data visualization tools such as histograms and bar charts.

Explicación
Data Wrangler includes a variety of data visualization tools such as histograms and bar charts that allow the engineer to visually explore the relationships between features, helping in feature selection and understanding data distributions.

Exporting data directly to Amazon SageMaker for model deployment.

Explicación
Exporting data to SageMaker is for model training or deployment, but does not help with analyzing feature relationships or importance within the data preparation phase.

Selección correcta
Feature importance visualization using the Quick Model.

Explicación
Data Wrangler’s Quick Model feature provides a fast way to calculate feature importance, helping engineers understand the impact of each feature on the target variable. This can guide which features to include or exclude in the model.

Temática
Data Preparation for Machine Learning
Pregunta 36

A company is developing an AI solution that needs to reduce both underfitting and overfitting to ensure a balanced model. Which combination of strategies can the data scientist apply to mitigate bias and variance in the model? (Choose Two)

Increase the model complexity by adding more parameters.

Explicación
Increasing model complexity by adding more parameters could lead to overfitting, not reducing it.

Limit the dataset size to reduce noise in the model.

Explicación
Reducing the dataset size does not solve the problem of noise but may limit the model's ability to capture all relevant patterns in the data.

Ignore high variance to focus on reducing bias.

Explicación
High variance should not be ignored, as it leads to overfitting. Both bias and variance need to be managed in a balanced way (bias-variance trade-off).

Selección correcta
Use early stopping to prevent the model from overfitting during training.

Explicación
Early stopping helps prevent overfitting by halting the training process when the performance on the validation data begins to degrade, indicating the model is starting to memorize noise rather than learning useful patterns.

Selección correcta
Use cross-validation to assess model performance on multiple data subsets.

Explicación
Cross-validation helps in evaluating the model’s performance on different subsets of data, ensuring that it generalizes better and is not overly fitted to a specific portion of the dataset.

Temática
Data Preparation for Machine Learning
Pregunta 37

Which of the following features of SageMaker Jupyter Notebooks helps ensure that data is not lost during the machine learning workflow?

On-demand instance provisioning

Explicación
On-demand instance provisioning refers to the ability to quickly set up computing environments, but it doesn't directly address data preservation.

Respuesta correcta
Persistent storage

Explicación
Persistent storage in SageMaker Jupyter Notebooks ensures that data and work are not lost. Users can resume their work later without any concerns about losing data, providing a smooth and continuous development process.

Pre-installed libraries like TensorFlow and PyTorch

Explicación
Pre-installed libraries help streamline the development process but don't provide data persistence.

Real-time inference endpoints

Explicación
Real-time inference endpoints are used to deploy and serve models, but they don't have a role in data storage.

Temática
ML Model Development
Pregunta 38

A Machine Learning engineer is building an e-commerce chatbot using Amazon Bedrock. The chatbot must handle real-time customer queries, generate personalized recommendations, and operate at scale without infrastructure management. The engineer also needs to fine-tune the foundational model with historical customer data and ensure the security of this data.
Which combination of services should the engineer use to achieve these requirements? (Choose TWO)

Use Amazon SageMaker to deploy and fine-tune foundation models, ensuring customization of machine learning models

Explicación
While SageMaker can fine-tune models, Bedrock directly offers access to foundation models and includes customization, making it a better fit in this context.

Use AWS CloudTrail to monitor model inference processes and ensure fine-grained data access control

Explicación
AWS CloudTrail is useful for auditing and tracking user activities but does not directly contribute to model deployment or scaling. Domain: Deployment and Orchestration of ML Workflows

Use AWS Lambda for model deployment, which enables the scaling of real-time inference for customer queries

Explicación
AWS Lambda, though useful for serverless computing, does not handle large-scale inference workloads as efficiently as Bedrock's managed service.

Selección correcta
Use Amazon S3 for storing historical customer data and generated model outputs

Explicación
Amazon S3 is the recommended storage solution for historical customer data and generated outputs, which Bedrock can utilize for fine-tuning models. S3 is scalable and secure for such data requirements.

Selección correcta
Use Amazon Bedrock for deploying foundation models with serverless infrastructure and fine-tuning capabilities

Explicación
Amazon Bedrock provides pre-trained foundation models that can be fine-tuned with custom data and deployed in a fully managed, serverless environment, which meets the scalability and customization needs.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 39

A data engineering team needs to analyze large datasets stored in an Amazon S3 data lake using SQL queries. The data is in semi-structured JSON format, and the team wants to improve query performance by converting it into a columnar format. They also want to ensure the schema is automatically inferred and stored, so that the data can be queried directly from Amazon S3 without loading it into a relational database. Which of the following steps should the team implement using AWS Glue and Amazon Athena? (Choose Two)

Enable partitioning in Athena to automatically split large datasets by date, optimizing query performance.

Explicación
While partitioning helps optimize queries, Athena does not automatically create partitions for your data. You need to define and manage partitions manually or through Glue.

Use Amazon Athena to run a query that converts JSON data to Apache ORC format and saves it directly into Amazon Redshift.

Explicación
While Athena can query data, it is not designed for converting data formats into Redshift. AWS Glue handles this transformation better.

Selección correcta
Use AWS Glue ETL jobs to convert the JSON data into Apache Parquet format and store it back in S3.

Explicación
AWS Glue ETL jobs can efficiently transform semi-structured data like JSON into a columnar format such as Apache Parquet, which improves performance when queried by Amazon Athena.

Selección correcta
Set up an AWS Glue Crawler to automatically infer the schema from the JSON files and store the metadata in the Glue Data Catalog.

Explicación
AWS Glue Crawlers can automatically detect the schema from the JSON files and register the metadata in the Glue Data Catalog, making it available for querying with Amazon Athena.

Manually create an Amazon Athena schema for each dataset and register it in the Glue Data Catalog.

Explicación
Manually creating schemas for each dataset is inefficient and unnecessary, as Glue Crawlers automate this process.

Temática
Data Preparation for Machine Learning
Pregunta 40

A healthcare company wants to transcribe doctor-patient conversations into text for further analysis and store the results in Amazon S3. They need the solution to automatically remove any personally identifiable information (PII) to ensure compliance with HIPAA regulations. Which combination of AWS services should be used to achieve this goal?

Use Amazon Transcribe to convert speech to text, store the transcriptions in Amazon DynamoDB, and use AWS Glue to detect and redact PII from the data.

Explicación
Amazon DynamoDB is not suitable for storing large text files, and AWS Glue is not designed for real-time PII detection and redaction in this scenario

Use Amazon Transcribe to convert speech to text, store the transcriptions in Amazon RDS, and manually review the data for PII.

Explicación
Storing the data in Amazon RDS and performing manual review adds unnecessary complexity and does not automate PII detection or redaction.

Respuesta correcta
Use Amazon Transcribe Medical to convert speech to text, utilize AWS Lambda to trigger Amazon Comprehend Medical for PII detection and redaction, and store the processed text in Amazon S3.

Explicación
Amazon Transcribe Medical is designed for healthcare use cases and provides accurate speech-to-text conversion for doctor-patient conversations. AWS Lambda can trigger Amazon Comprehend Medical, which can detect and redact sensitive PII from the text, ensuring HIPAA compliance. The processed data can then be stored in Amazon S3.

Use Amazon Transcribe Medical to convert speech to text, store the transcriptions in Amazon S3, and enable Amazon Macie to scan and redact PII.

Explicación
While Amazon Transcribe Medical and Amazon Macie can work with data in Amazon S3, Macie is designed for general PII detection and lacks the medical-specific capabilities of Amazon Comprehend Medical.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 41

A pharmaceutical company is using machine learning to analyze clinical trial data. They need to ingest raw data from multiple sources, such as CSV files and relational databases, and clean and join this data. Afterward, the data should be used to create a set of features, including patient age, treatment history, and genetic markers. The company also wants to maintain a version-controlled feature repository that can be reused across different machine learning models.

Which AWS services should the company use to preprocess the data, create features, and maintain a version-controlled feature repository?

Use AWS Glue for data cleaning, store features in Amazon DynamoDB, and use SageMaker to train models.

Explicación
AWS Glue is primarily used for ETL (extract, transform, load) processes, but it does not provide the version-controlled feature storage needed for machine learning. DynamoDB is not optimal for storing machine learning features compared to SageMaker Feature Store.

Use Amazon QuickSight for data preprocessing, SageMaker Feature Store to store features, and SageMaker JumpStart to deploy a pre-built model.

Explicación
Amazon QuickSight is a data visualization tool and is not intended for preprocessing or feature engineering. SageMaker Data Wrangler would be a better fit for this task.

Respuesta correcta
Use SageMaker Data Wrangler for data preprocessing, SageMaker Feature Store to store and manage the features, and SageMaker Notebooks to train the models.

Explicación
SageMaker Data Wrangler allows for seamless data cleaning and feature engineering. SageMaker Feature Store provides version control and centralized management of features, making it easy to reuse features across multiple models. SageMaker Notebooks offer an integrated environment for training and evaluating models.

Use Amazon Athena to preprocess the data, store the features in Amazon S3, and use SageMaker Autopilot to automatically train models.

Explicación
Amazon Athena is not designed for complex data cleaning and feature engineering. Amazon S3 does not offer version-controlled feature storage like SageMaker Feature Store does.

Temática
Data Preparation for Machine Learning
Pregunta 42

Which of the following is the primary advantage of using the Hyperband algorithm for hyperparameter tuning over other methods?

Respuesta correcta
It allocates more resources to promising configurations while stopping underperforming configurations early

Explicación
Hyperband dynamically allocates more resources to promising hyperparameter configurations while early-stopping underperforming ones, balancing exploration and exploitation of different configurations.

It randomly selects hyperparameters to reduce training time

Explicación
Random search selects hyperparameters randomly, not Hyperband.

It guarantees that all hyperparameter combinations are tested

Explicación
Grid search tests all combinations, not Hyperband.

It uses prior training results to suggest the next set of hyperparameters to test

Explicación
Bayesian optimization, not Hyperband, uses prior training results to inform future hyperparameter choices.

Temática
ML Model Development
Pregunta 43

A company has deployed a machine learning model into production using Amazon SageMaker. Over time, the model's performance starts degrading due to changes in customer behavior. The team decides to monitor the model and retrain it periodically.

Which combination of AWS services should the team use to monitor and maintain the model’s performance?

Amazon CloudWatch for monitoring model performance and SageMaker Data Wrangler to retrain the model.

Explicación
While Amazon CloudWatch can monitor basic metrics, it’s not specifically designed for model performance monitoring. SageMaker Model Monitor is the correct service for this purpose.

SageMaker Feature Store to track changes in data distribution, and SageMaker Clarify to handle data bias in producti

Explicación
SageMaker Feature Store is used to store and manage features, not monitor data drift. SageMaker Clarify is primarily used to detect bias, not for monitoring overall model performance.

SageMaker Data Wrangler for data monitoring, and AWS Glue to retrain the model automatically.

Explicación
SageMaker Data Wrangler is for data preparation, not for continuous data monitoring or retraining.

Respuesta correcta
SageMaker Model Monitor for monitoring data drift, and SageMaker Pipelines to automate model retraining.

Explicación
SageMaker Model Monitor tracks changes in the input data distribution (data drift), and SageMaker Pipelines can be used to automate the process of model retraining based on the new data.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 44

A machine learning engineer is building a model to predict rare diseases based on a dataset where the majority of patients do not have the disease. To handle the imbalance in the dataset, the engineer is considering using SageMaker Data Wrangler to preprocess the data.

Which technique provided by SageMaker Data Wrangler is the BEST option to address class imbalance without increasing the risk of overfitting?

Random undersampling to reduce the number of samples in the majority class.

Explicación
While undersampling can address class imbalance, it risks losing important information from the majority class, which could negatively impact model performance.

Respuesta correcta
Synthetic Minority Oversampling Technique (SMOTE) to generate new synthetic samples in the minority class.

Explicación
SMOTE generates new, synthetic data points for the minority class, which helps balance the dataset while preserving diversity and reducing the risk of overfitting, compared to simple oversampling.

Simple oversampling to duplicate samples from the minority class.

Explicación
Simple oversampling can lead to overfitting since it duplicates existing data points, making the model more likely to learn noise from the minority class.



Feature scaling to normalize the feature values in the dataset.

Explicación
Feature scaling standardizes feature values but does not directly address class imbalance.

Temática
Data Preparation for Machine Learning
Pregunta 45

A data engineering team is tasked with setting up an automated ETL pipeline using AWS Glue. They have a semi-structured data file stored in Amazon S3 and need to extract, transform, and load this data into Amazon Redshift for further analysis. Additionally, they want to catalog the data schema and make it queryable via SQL. How should the team implement this solution efficiently using AWS Glue?

Manually create the schema in Redshift before loading the data to avoid unnecessary processing by AWS Glue crawlers and run AWS Glue ETL jobs to load the data.

Explicación
Manually creating the schema is not efficient, as AWS Glue crawlers can automatically handle this.

Respuesta correcta
Use AWS Glue crawlers to automatically infer the schema from the data in S3 and store the schema in the Glue Data Catalog. Then, run AWS Glue ETL jobs to extract, transform, and load the data into Amazon Redshift.

Explicación
AWS Glue crawlers are designed to automatically infer the schema from data stored in S3 and store the metadata in the Glue Data Catalog. This schema can then be used to run SQL-style queries using Athena or Redshift. After the cataloging, an ETL job can be configured to extract, transform, and load data into Amazon Redshift efficiently.

Use AWS Glue ETL jobs to extract the data from S3, transform it using pre-built transformations, and load it directly into Amazon Redshift without any further processing.

Explicación
This skips the Glue Data Catalog step, which is necessary for schema inference and SQL query capability.

Use AWS Lambda to manually trigger AWS Glue jobs and control costs, avoiding automatic cataloging and schema inference.

Explicación
While AWS Lambda could trigger Glue jobs, skipping the schema inference and cataloging would reduce functionality and efficiency.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 46

A machine learning team needs to regularly transform and load large datasets from Amazon S3 into a data warehouse for model training. They are considering using AWS Glue for this ETL process but want to ensure the process is scalable, cost-efficient, and able to handle incremental updates. Which features of AWS Glue should they leverage to meet these requirements? (Choose Two)

Pre-configure fixed Spark cluster resources to avoid unexpected scaling charges.

Explicación
Pre-configuring fixed Spark clusters can lead to inefficiencies and higher costs compared to using Glue’s auto-scaling capabilities.



Use AWS Glue crawlers to infer the schema and classify the data in real-time, ensuring all new files are processed immediately.

Explicación
AWS Glue crawlers are not designed for real-time schema inference; they are scheduled or manually triggered. Real-time processing requires a different approach.

Selección correcta
Utilize AWS Glue’s serverless architecture and auto-scaling capabilities to manage Spark clusters automatically, ensuring scalability.

Explicación
AWS Glue is serverless and automatically manages Spark clusters under the hood. This enables the system to scale based on the workload without manual intervention, making it both cost-efficient and scalable.

Selección correcta
Use AWS Glue to configure incremental ETL jobs that only process new or modified data.

Explicación
AWS Glue supports incremental loading, which ensures that only new or updated data is processed, significantly improving efficiency and reducing processing costs.

Use AWS Glue’s built-in scheduling to trigger ETL jobs at regular intervals and process the entire dataset each time for consistency.

Explicación
Processing the entire dataset every time is not cost-efficient or scalable, especially for large datasets.

Temática
Data Preparation for Machine Learning
Pregunta 47

A data scientist at a retail company needs to quickly build a machine learning model to predict customer churn. The data scientist has limited time and prefers to avoid the complexity of setting up custom infrastructure or algorithms. Which approach would be most suitable for this scenario using Amazon SageMaker?

Build a custom algorithm using TensorFlow on a SageMaker notebook instance, with manual setup for data processing and infrastructure management.

Explicación
Building a custom algorithm with TensorFlow adds complexity because it involves setting up infrastructure and managing the training process manually.

Set up a custom model using a proprietary algorithm on Amazon EC2, ensuring complete control over the infrastructure.

Explicación
Using a proprietary algorithm on EC2 gives complete control but introduces a high level of complexity in infrastructure setup, which is unnecessary for this task.

Use SageMaker Jumpstart to deploy a pre-built solution for customer churn prediction without customization.

Explicación
SageMaker Jumpstart is suitable for deploying pre-built solutions, but using built-in algorithms directly allows more control over the training process.

Respuesta correcta
Use SageMaker built-in algorithms, such as XGBoost, with the provided dataset, specifying only the hyperparameters and instance type.

Explicación
SageMaker built-in algorithms like XGBoost are optimized for common machine learning tasks, such as classification and regression. These algorithms are pre-packaged, making them easy to use without needing to handle the complexity of infrastructure management. The user only needs to specify data, hyperparameters, and instance types.

Temática
ML Model Development
Pregunta 48

A machine learning team is building an end-to-end workflow using Amazon SageMaker Pipelines to automate their ML tasks. They want to ensure that their workflow is reusable, scalable, and can handle concurrent execution of multiple machine learning jobs. What key features of SageMaker Pipelines make this possible?

The integration with SageMaker Studio, which only supports small-scale experimental workflows.

Explicación
SageMaker Pipelines can scale from small experiments to large-scale production workflows and is integrated with SageMaker Studio for development but not limited to small-scale tasks.

The ability to manually trigger each step in the pipeline for better control.

Explicación
SageMaker Pipelines is designed to automate the workflow, so manually triggering each step defeats the purpose of automation.

Pre-defined templates for building models with a user-friendly UI but limited scalability.

Explicación
While SageMaker Pipelines provides a user-friendly UI, it is highly scalable and not limited to small-scale tasks.

Respuesta correcta
Directed Acyclic Graph (DAG) representation with automatic step execution and scalability to tens of thousands of concurrent workflows.

Explicación
SageMaker Pipelines is designed with scalability in mind, capable of handling tens of thousands of concurrent ML workflows. It uses a Directed Acyclic Graph (DAG) to automatically manage the relationships and dependencies between steps, ensuring efficient execution.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 49

What does data drift refer to in the context of machine learning model monitoring, and why is it important to detect?

Respuesta correcta
A change in the input data distribution over time, making it different from the data used for model training.

Explicación
Data drift refers to the change in the distribution of input data in production, which can differ from the data the model was trained on. Detecting this is crucial because it can degrade the model’s performance.

The sudden failure of a model due to an error in the underlying algorithm.

Explicación
This is not related to data drift but rather a model or system failure.

A situation where the model's predictions deviate from expected outcomes in a systematic way.

Explicación
This describes model quality drift, not data drift. Data drift focuses on the input data, not the predictions.

An issue that arises when the system running the model experiences infrastructure downtime.

Explicación
Infrastructure downtime does not describe data drift.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 50

You are managing a machine learning project and want to reuse a data preprocessing step from a previous pipeline for a new model. What feature of SageMaker Pipelines allows you to achieve this?

Respuesta correcta
Steps in a pipeline can be reused across different projects, saving time and effort.

Explicación
SageMaker Pipelines allow for the reuse of steps across different models and pipelines, enabling the efficient replication of common tasks like data preprocessing.

Pipelines must be redefined for each new model.

Explicación
Pipelines don’t need to be redefined completely for every new model; steps can be reused.

Pipeline steps are not reusable and must be manually duplicated for each new model.

Explicación
SageMaker Pipelines provide reusable steps, which eliminate the need to manually duplicate processes for each new model.



Only parameter values can be reused in different pipelines.

Explicación
While parameters can be reused, SageMaker Pipelines also support the reuse of entire steps across different projects.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 51

A company is building a generative AI application using Amazon Bedrock. The company needs to quickly integrate a foundation model for generating product recommendations based on customer interactions. They also want to ensure the infrastructure is scalable and secure without managing it directly.
Which features of Amazon Bedrock make it the best choice for this use case? (Choose Two)

Selección correcta
Data encryption in transit and at rest

Explicación
Bedrock ensures that data is encrypted in transit and at rest, providing secure integration of generative AI models without compromising data security.

Selección correcta
Serverless environment with automatic scaling

Explicación
Amazon Bedrock offers a serverless environment that automatically scales based on the workload, making it suitable for applications that require dynamic scalability without manual intervention.

Custom instances for high-complexity models

Explicación
Custom instances are not required, as Bedrock manages infrastructure automatically.

Direct access to GPUs for model training

Explicación
Bedrock abstracts the underlying infrastructure, so direct access to GPUs for model training is not a feature.

On-premises infrastructure for enhanced security

Explicación
Bedrock is an AWS-managed service; it doesn’t support on-premises infrastructure for model deployment.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 52

A company is using AWS Glue to run ETL jobs on large datasets stored in Amazon S3. The data is organized into partitions based on the year the data was collected, with each year having its own folder structure. The team frequently runs queries that filter data by year, such as retrieving records only from 2023. How does this partitioning strategy benefit their data processing and querying?

It causes AWS Glue to process all partitions equally, regardless of the query conditions.

Explicación
AWS Glue processes partitions independently, not equally, which allows skipping irrelevant partitions.

It increases the storage costs due to more files being created in S3.

Explicación
While partitioning creates more files, the cost benefit comes from reduced scanning, not increased storage.

Respuesta correcta
It allows AWS Glue to skip irrelevant partitions, reducing data scanned and improving query performance.

Explicación
Partitioning allows AWS Glue to skip partitions that do not meet query conditions (e.g., data from 2023 only), reducing the amount of data scanned. This improves performance and reduces costs.

It increases the IOPS operations required, slowing down query response times.

Explicación
Partitioning reduces IOPS by limiting the data that needs to be read.

Temática
Data Preparation for Machine Learning
Pregunta 53

A machine learning engineer wants to ensure that their deployed model remains unbiased over time. They are concerned that the model may begin favoring a particular group in the predictions, which did not exist during training.

What specific type of drift should the engineer monitor using Amazon SageMaker Model Monitor to detect this issue?

Data drift

Explicación
Definition: Data drift refers to changes in the input data distribution over time compared to the data the model was trained on.

Example: If the characteristics of the data (e.g., customer age, income, or location) change over time, this might indicate data drift.

Relevance to the Question: Data drift could lead to bias if certain groups start appearing in the data that weren't present during training. However, data drift alone doesn't specifically measure or address fairness or group bias.

Feature attribution drift

Explicación
Definition: Feature attribution drift refers to changes in how the model weights or relies on certain features when making predictions.

Example: If a model starts relying heavily on a feature like "age" or "income" more than it did during training, it could indicate a shift in feature importance.

Relevance to the Question: This could be indirectly related to bias if the change in feature importance causes the model to favor certain groups. However, it does not directly measure bias or unfair treatment.

Respuesta correcta
Bias drift

Explicación
Definition: Bias drift refers to changes in the model's predictions that indicate it is starting to favor certain groups or individuals unfairly over time.

Example: If a model begins favoring a particular demographic group (e.g., males vs. females) that wasn't evident during training, it is an example of bias drift.

Relevance to the Question: This directly addresses the concern stated in the question. The engineer is specifically worried about the model starting to favor one group in predictions. Bias drift detection tools help ensure fairness and mitigate unintended discrimination.

Model quality drift

Explicación
Definition: Model quality drift refers to a decline in the model's predictive performance over time, such as lower accuracy, higher error rates, or reduced F1 scores.

Example: If the model starts predicting incorrectly more often, this could indicate a drift in quality.

Relevance to the Question: While a drop in performance could be a side effect of bias, this option focuses on general performance metrics rather than detecting bias directly.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 54

A manufacturing company is using machine learning to predict equipment failures. The company has data on equipment usage, temperature, and maintenance records stored in an Amazon S3 bucket. They need to clean and analyze this data, engineer features such as average machine temperature over time, and store these features for both batch and real-time predictions. The company also wants to utilize a pre-built machine learning model to accelerate the deployment process.

Which combination of services would BEST meet the company's needs?

Use SageMaker Data Wrangler to clean and preprocess the data, store the features in Amazon Redshift, and use SageMaker JumpStart to deploy a pre-built model.

Explicación
While SageMaker Data Wrangler is suitable for preprocessing, Amazon Redshift is not ideal for real-time feature storage. SageMaker Feature Store is a better fit for this use case.

Use Amazon EMR to preprocess the data, store the features in Amazon DynamoDB, and use SageMaker to train and deploy the model.

Explicación
Amazon EMR is typically used for large-scale distributed data processing, which may be overkill in this scenario. Additionally, Amazon DynamoDB is not optimized for machine learning feature storage, unlike SageMaker Feature Store.

Respuesta correcta
Use SageMaker Data Wrangler to clean and transform the data, store the features in SageMaker Feature Store, and use SageMaker JumpStart to deploy a pre-built predictive maintenance model.

Explicación
SageMaker Data Wrangler allows for easy data cleaning and feature engineering. SageMaker Feature Store is ideal for storing the engineered features, which can be used for both batch and real-time predictions. SageMaker JumpStart enables the use of pre-built models, reducing the time required for model development.

Use SageMaker Notebooks to preprocess the data, store the features in Amazon S3, and use SageMaker Autopilot to train and deploy a predictive model.

Explicación
While SageMaker Notebooks can be used for preprocessing, storing features in Amazon S3 is not as efficient for real-time feature serving as SageMaker Feature Store. Additionally, SageMaker Autopilot is more for custom model building rather than deploying pre-built models.

Temática
ML Model Development
Pregunta 55

A data scientist wants to optimize a machine learning model for deployment across multiple edge devices using SageMaker Neo. After training the model using TensorFlow in SageMaker, what are the next steps the data scientist should take to optimize the model for edge deployment while maintaining performance?

Set up SageMaker Debugger to capture real-time metrics during inference on edge devices.

Explicación
SageMaker Debugger is useful for monitoring and debugging during training but is not used for optimizing models for edge deployment.

Respuesta correcta
Use SageMaker Neo to compile the model for the target hardware platform and deploy it using AWS IoT Greengrass.

Explicación
After training the model in SageMaker, the data scientist can use SageMaker Neo to compile the model for the specific hardware (such as IoT or edge devices) and deploy it using AWS IoT Greengrass, ensuring optimized performance.

Use SageMaker Debugger to profile the edge device resource usage and optimize the model based on profiling results.

Explicación
While profiling can be useful, SageMaker Neo directly optimizes the model for hardware-specific deployments, providing a streamlined solution for edge devices.

Manually rewrite the model’s code to ensure it is compatible with the target edge hardware.

Explicación
Neo automates the optimization process, making manual rewriting of the model for hardware compatibility unnecessary.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 56

A security officer at a financial institution wants to ensure that all S3 buckets in the account are configured according to the company’s security standards, such as enabling server-side encryption and blocking public access. The institution also requires continuous monitoring and real-time alerts if any bucket becomes non-compliant.

Which AWS services should be used to meet these requirements?

Use AWS CloudTrail to track S3 API calls and alert if encryption or public access settings are changed.

Explicación
AWS CloudTrail logs API activity, but it doesn’t enforce compliance or continuously evaluate bucket configurations. It’s useful for auditing, not real-time compliance monitoring.

Set up CloudWatch Alarms for changes in S3 bucket configurations, including encryption and access control policies.

Explicación
CloudWatch Alarms do not monitor configuration changes. They are more suitable for tracking performance metrics like CPU utilization or error rates, not configuration compliance.

Respuesta correcta
Configure AWS Config to evaluate S3 bucket settings against security rules and set up CloudWatch Events for real-time alerts.

Explicación
AWS Config allows users to define compliance rules (e.g., S3 buckets must have encryption enabled, public access blocked). When an S3 bucket violates a rule, AWS Config can trigger real-time alerts via CloudWatch Events or SNS notifications, ensuring continuous monitoring and compliance enforcement.

Use CloudWatch Logs to monitor S3 bucket changes and deploy Lambda functions to automatically correct misconfigurations.

Explicación
While CloudWatch Logs can capture changes, they do not inherently monitor configurations or trigger compliance rules. Additionally, using Lambda to automatically correct misconfigurations would add complexity and might not provide as robust a solution as AWS Config.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 57

A data scientist is setting up a new SageMaker pipeline for automating an ML workflow. They need to define the steps in the pipeline, which include training a model, registering the model in the model registry, and deploying it. What structure should they follow to create a functional SageMaker pipeline?

Only define the steps, without specifying any parameters or names, since SageMaker Pipelines automatically handles these.

Explicación
Defining the steps, names, and parameters is necessary for proper configuration and execution of the pipeline.

Use AWS CloudFormation to define and automate all steps of the pipeline without utilizing SageMaker's pipeline features.

Explicación
While AWS CloudFormation can automate infrastructure, SageMaker Pipelines are purpose-built for automating ML workflows and provide better integration for model-related tasks

Respuesta correcta
Define the name of the pipeline, the steps for each action, and parameters for step configuration.

Explicación
SageMaker Pipelines require a structured definition that includes a unique name, the steps that define the actions (e.g., training, model registration), and parameters for configuration. The pipeline definition includes steps and relationships between steps.

Create a notebook in SageMaker Studio, then manually execute each step in the pipeline.

Explicación
The purpose of SageMaker Pipelines is to automate the execution of steps, not to require manual execution.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 58

A media company is ingesting high-velocity log data using Amazon Kinesis Data Firehose. They need to analyze this data in near real-time using SQL queries. The data is semi-structured and needs to be loaded into a data warehouse for fast querying. Which combination of services should the company use to accomplish this?

Use Amazon Kinesis Data Firehose to stream the data into an Amazon S3 bucket, then use Amazon Athena to run SQL queries on the data stored in S3.

Explicación
While this option could work for querying the data in Amazon S3 with Athena, it is less optimized than Redshift for near real-time data warehousing and analytics.



Stream data into Amazon Kinesis Data Streams, then use a Lambda function to transform and store the data in Amazon DynamoDB

Explicación
Although Kinesis Data Streams can be used for ingestion and transformation with Lambda, DynamoDB is not suitable for running complex SQL queries as it is a NoSQL database.

Respuesta correcta
Use Amazon Kinesis Data Firehose to load the data into Amazon Redshift using the native Firehose to Redshift integration for fast querying.

Explicación
Amazon Kinesis Data Firehose has a native integration with Amazon Redshift, which is a fully managed data warehouse that supports fast querying of large datasets. Firehose can automatically load the streaming data into Redshift tables, making it suitable for this use case.

Use Amazon Kinesis Data Firehose to stream the data into Amazon RDS with a PostgreSQL database for querying.

Explicación
Amazon RDS is a managed relational database, but it is not optimized for large-scale data warehousing like Redshift. It also lacks the near real-time analytics capabilities required.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 59

A company needs to create a machine learning model to detect fraudulent transactions in real-time. They plan to use Amazon SageMaker for model training and deployment. The data for training is stored in Amazon S3, and they require the ability to preprocess the data, perform hyperparameter tuning, and deploy the model to an endpoint for real-time inference. The company also wants to automate the entire workflow from data preprocessing to model deployment.

Which solution will meet these requirements MOST effectively?

Use Amazon EMR for data preprocessing, SageMaker Script Mode for custom training scripts, perform hyperparameter tuning manually, and deploy the model using Amazon SageMaker Hosting Services.

Explicación
While Amazon EMR can be used for data preprocessing, it may not offer the same level of integration and automation as SageMaker Processing. Using SageMaker Script Mode for custom training scripts may require more manual effort compared to using SageMaker built-in algorithms. Performing hyperparameter tuning manually can be time-consuming and less effective than using SageMaker Automatic Model Tuning. Deploying the model using Amazon SageMaker Hosting Services may not provide the same level of scalability and real-time inference capabilities as using a SageMaker Endpoint.
Respuesta correcta
Use Amazon SageMaker Processing for data preprocessing, SageMaker built-in algorithms for training, SageMaker Automatic Model Tuning for hyperparameter tuning, and SageMaker Endpoint for deployment.

Explicación
Using Amazon SageMaker Processing for data preprocessing allows the company to preprocess the data efficiently before training the model. SageMaker built-in algorithms provide a variety of pre-built algorithms for training the model. SageMaker Automatic Model Tuning automates the process of hyperparameter tuning, saving time and effort. Finally, deploying the model to a SageMaker Endpoint enables real-time inference, meeting all the requirements effectively.
Use AWS Glue for data preprocessing, train the model on a custom EC2 instance, use Amazon SageMaker for hyperparameter tuning, and deploy it using Amazon API Gateway.

Explicación
While AWS Glue can be used for data preprocessing, training the model on a custom EC2 instance may not be as efficient as using SageMaker for training. Using Amazon API Gateway for deployment may not provide the same level of integration and automation as deploying to a SageMaker Endpoint. Additionally, hyperparameter tuning manually may be time-consuming and less effective compared to using SageMaker Automatic Model Tuning.
Use Amazon SageMaker Data Wrangler for data preprocessing, SageMaker built-in algorithms for training, SageMaker Experiments for tracking, and deploy the model using AWS Lambda.

Explicación
While Amazon SageMaker Data Wrangler can be used for data preprocessing, it may not offer the same level of flexibility and customization as using SageMaker Processing. Using SageMaker Experiments for tracking may not provide the same level of automation and integration as using SageMaker Automatic Model Tuning for hyperparameter tuning. Deploying the model using AWS Lambda may not offer the same level of scalability and real-time inference capabilities as deploying to a SageMaker Endpoint.
Temática
Data Preparation for Machine Learning
Pregunta 60

A company is managing multiple AWS accounts and wants to implement a centralized user management system. They need to ensure that users can seamlessly access resources across all accounts without managing multiple credentials. The company also requires the ability to control access through policies and track user activities.

Which solution will meet these requirements MOST effectively?

Use AWS Directory Service to create a single directory and link it to all AWS accounts.

Explicación
Using AWS Directory Service to create a single directory and linking it to all AWS accounts can provide centralized user management, but it may not offer the ability to control access through policies or track user activities effectively.
Create IAM users in each AWS account and assign the necessary permissions.

Explicación
Creating IAM users in each AWS account and assigning necessary permissions may lead to credential management issues and make it difficult for users to seamlessly access resources across multiple accounts without switching credentials.
Respuesta correcta
Set up AWS Organizations and enable Single Sign-On (SSO) for centralized access management across all accounts.

Explicación
Setting up AWS Organizations with Single Sign-On (SSO) provides a centralized management system for multiple AWS accounts. SSO allows users to log in with a single set of credentials and access resources across all linked accounts based on their permissions. This solution simplifies user management, enhances security by reducing the need for multiple credentials, and enables effective access control through policies. The other options either involve managing multiple credentials or do not provide centralized user management.

Configure IAM roles in each account and allow users to switch roles as needed.

Explicación
Configuring IAM roles in each account and allowing users to switch roles as needed may provide some level of access control, but it may not offer the same level of centralized management and tracking capabilities as using AWS Organizations with SSO.
Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 61

A financial institution's machine learning model in production shows a significant drop in prediction accuracy over the last month. Upon investigation, the data science team suspects that the real-world data used for inference has shifted from the training dataset. They want to continuously monitor the quality of the input data to ensure it meets the expected constraints.

Which steps should the team take to monitor the data quality using Amazon SageMaker Model Monitor? (Select TWO)

Selección correcta
Enable data capture on the SageMaker endpoint to store incoming requests and predictions.

Explicación
This step allows the collection of both incoming data and predictions, which can be used for monitoring and comparison with the baseline.

Use AWS CloudTrail to log and evaluate prediction errors.

Explicación
CloudTrail logs API activity, not model data or predictions.

Selección correcta
Define a baseline of the training data to set expected statistical properties.

Explicación
A baseline is necessary for understanding what constitutes normal data, enabling the model monitor to detect deviations.



Set up a batch transform job to retrain the model when data drift is detected.

Explicación
Batch transforms are for inference, not for detecting data drift.

Manually review each data point for drift after the model has made predictions.

Explicación
SageMaker Model Monitor automates this process.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 62

A media company wants to build an automated video transcription and translation pipeline. They need to transcribe speech from videos, translate the text into multiple languages, and create audio versions of the translated text. Which AWS services would fulfill these requirements?

Use AWS Glue for transcription, Amazon Translate for text translation, and AWS Lambda to convert the translated text to speech.

Explicación
AWS Glue is not intended for transcription purposes; it's primarily used for ETL (Extract, Transform, Load) jobs. AWS Lambda can invoke functions but cannot handle speech-to-text or text-to-speech by itself.

Use Amazon Transcribe to convert speech to text, Amazon SageMaker for text translation, and Amazon Rekognition for speech synthesis.

Explicación
While Amazon Transcribe is suitable for speech-to-text conversion, Amazon SageMaker is designed for machine learning tasks, not direct text translation. Amazon Rekognition does not provide speech synthesis capabilities.

Respuesta correcta
Use Amazon Transcribe to convert speech to text, Amazon Translate for text translation, and Amazon Polly to convert translated text to speech.

Explicación
Amazon Transcribe can automatically convert the speech in videos into text. Amazon Translate can then translate the text into different languages, and Amazon Polly can convert the translated text back into speech, forming an automated pipeline for transcription, translation, and audio synthesis.

Use Amazon Rekognition for transcription, AWS Lambda for text translation, and Amazon Polly for speech synthesis.

Explicación
Amazon Rekognition is used for image and video analysis but does not handle speech transcription or text translation.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 63

A data scientist is running multiple machine learning experiments using Amazon SageMaker to determine the best hyperparameter configurations for their model. They want to track each experiment's metadata, compare the results, and ensure that the process is fully reproducible. Which SageMaker feature would BEST meet these requirements?

Respuesta correcta
SageMaker Experiments

Explicación
SageMaker Experiments is designed to track and manage experiments, allowing the user to compare different configurations and reproduce results. It records metadata and provides a structure to organize trials and components.

SageMaker Autopilot

Explicación
SageMaker Autopilot is an automated model training tool but does not offer the detailed tracking and comparison of multiple configurations like SageMaker Experiments.

SageMaker Debugger

Explicación
SageMaker Debugger is used for monitoring model training jobs and capturing debugging information, but it does not provide the structured experiment comparison functionality required here.

SageMaker Pipelines

Explicación
SageMaker Pipelines automate end-to-end workflows but do not provide the fine-grained experiment tracking that SageMaker Experiments offers.

Temática
ML Model Development
Pregunta 64

A financial services company needs to monitor its application infrastructure hosted on AWS. The company wants to set up an alarm that triggers an action if the average CPU utilization of their EC2 instances exceeds 80% over a 5-minute period. Additionally, they want to automatically stop the instance if the alarm triggers.

Which steps should the machine learning engineer take to meet these requirements?

Respuesta correcta
Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Configure the alarm action to stop the instances directly.

Explicación
This choice is correct because it directly addresses the requirement of automatically stopping the instances when the CPU utilization exceeds the threshold. Configuring the CloudWatch alarm action to stop the instances directly is the most efficient way to meet the company's requirements.
Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Use the alarm action to trigger an AWS Lambda function that stops the instances.

Explicación
Setting up a CloudWatch alarm to monitor CPU utilization is necessary, but using an AWS Lambda function as the alarm action to stop the instances adds unnecessary complexity to the solution. Directly configuring the alarm action to stop the instances would be more straightforward.
Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Configure an alarm action to send an SNS notification when the alarm state is triggered.

Explicación
Creating a CloudWatch alarm to monitor CPU utilization is correct, but configuring an SNS notification as the alarm action will only send a notification without taking any automated action to stop the instances.
Create a CloudWatch alarm to monitor the average CPU utilization of the EC2 instances. Use the alarm action to trigger an Auto Scaling policy that stops the instances.

Explicación
While creating a CloudWatch alarm to monitor CPU utilization is a step in the right direction, triggering an Auto Scaling policy to stop the instances may not be the most direct and efficient way to achieve the requirement of automatically stopping the instances.
Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 65

An organization is using SageMaker to train and deploy a reinforcement learning (RL) model that optimizes a portfolio of financial assets. Which of the following features of SageMaker RL would benefit their use case? (Select Two)

Selección correcta
SageMaker supports pre-built RL toolkits like Intel Coach and Ray RLlib for state-of-the-art algorithms.

Explicación
SageMaker provides pre-built RL toolkits like Intel Coach and Ray RLlib, which allow users to leverage advanced reinforcement learning algorithms for applications like portfolio optimization.

Selección correcta
SageMaker allows monitoring and adjusting the training through Python SDK integration.

Explicación
SageMaker’s integration with the Python SDK allows users to monitor and manage training jobs, view metrics, and adjust parameters directly from their notebooks.

The RL model must always be deployed on real-world environments; simulated environments are not supported.

Explicación
Simulated environments are supported in SageMaker and are often used to accelerate training before moving to real-world environments.

Training jobs can only run on external instances and not locally on notebook instances.

Explicación
Training can be done in local mode on notebook instances, which is a key feature for ease of development and experimentation.

Temática
Deployment and Orchestration of ML Workflows