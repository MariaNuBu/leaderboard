{
  "questions": [
    {
      "id": 5,
      "question": "When using masked self-attention in GPT models, what is the purpose of the mask?",
      "options": [
        "To reduce the model size",
        "To prevent overfitting",
        "To allow tokens to focus only on past tokens",
        "To speed up the training process"
      ],
      "correct_answers": [
        "To allow tokens to focus only on past tokens"
      ],
      "references": [],
      "topic": "Generative AI Model Fundamentals",
      "Source": "https://rgitsc.udemy.com/course/aws-certified-machine-learning-engineer-associate-mla-c01/learn/quiz/6496233#overview",
      "Practice test": "AWS Certified Machine Learning Engineer Associate: Hands On!"
    }
  ]
}