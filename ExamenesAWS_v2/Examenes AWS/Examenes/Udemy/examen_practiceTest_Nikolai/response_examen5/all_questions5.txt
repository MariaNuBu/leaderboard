Pregunta 1

A data scientist is using SageMaker Data Wrangler for feature engineering and needs to determine which features contribute most to the model's predictions. Which feature of Data Wrangler helps visualize and estimate the importance of different features?

One-hot Encoding

Explicación
One-hot encoding is a technique for transforming categorical data into binary vectors, not for evaluating feature importance.

Respuesta correcta
Quick Model

Explicación
The Quick Model feature in Data Wrangler allows users to train a simple model quickly and visualize the feature importance. This helps in understanding which features contribute most to the model's predictions.

Recursive Feature Elimination

Explicación
Recursive Feature Elimination is a feature selection technique but is not part of Data Wrangler’s quick visualizations.

Min-Max Scaling

Explicación
Min-Max Scaling is a normalization method that transforms feature values but does not provide feature importance information.

Temática
Data Preparation for Machine Learning
Pregunta 2

A financial services company records customer service calls and needs to perform sentiment analysis on these calls to assess customer satisfaction. They also require the ability to review the transcripts of these calls for compliance purposes. Which combination of AWS services will BEST achieve these requirements?

Use Amazon Transcribe to convert the call recordings into text, and Amazon Lex to analyze the sentiment of the calls.

Explicación
While Amazon Transcribe can transcribe the recordings, Amazon Lex is a conversational interface service and does not perform sentiment analysis.

Use AWS Lambda to convert the call recordings into text, store the transcripts in Amazon S3, and use Amazon Rekognition for sentiment analysis.

Explicación
AWS Lambda is a compute service and not intended for transcription. Amazon Rekognition is focused on image and video analysis, not sentiment analysis.

Use Amazon Polly to convert the call recordings into text, store the transcripts in Amazon DynamoDB, and use Amazon SageMaker to perform sentiment analysis.

Explicación
Amazon Polly is used to convert text to speech, not speech to text. Additionally, Amazon DynamoDB is not optimized for storing large text data, and while SageMaker can perform sentiment analysis, Amazon Comprehend is a more straightforward service for this purpose.

Respuesta correcta
Use Amazon Transcribe to convert the call recordings into text, store the transcripts in Amazon S3, and use Amazon Comprehend for sentiment analysis on the transcripts.

Explicación
Amazon Transcribe can convert the audio recordings into text, which can be stored in Amazon S3. Amazon Comprehend can then be used to perform sentiment analysis on the transcripts, providing insights into customer satisfaction.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 3

A company wants to use Amazon SageMaker to deploy a real-time machine learning model for high-traffic online predictions. The model has moderate memory and CPU requirements, but scalability and low latency are critical due to unpredictable traffic spikes.

Which of the following deployment options is best suited for this scenario?

Deploy the model on an Amazon EC2 instance to manually manage scaling.

Explicación
Using Amazon EC2 for model deployment would require manual scaling and management, which can be less responsive to unpredictable traffic.

Use AWS Lambda to serve predictions from the model, scaling as per incoming requests.

Explicación
While AWS Lambda offers auto-scaling, it may not be suitable for models that require extensive CPU or memory resources, especially with the high-traffic demands described. SageMaker endpoints are optimized for such workloads.

Respuesta correcta
Deploy the model on an Amazon SageMaker endpoint with auto-scaling enabled.

Explicación
Deploying the model on an Amazon SageMaker endpoint with auto-scaling ensures that the endpoint can automatically adjust resources in response to traffic spikes, providing scalability and low latency.

Use Amazon SageMaker batch transform to process requests in large batches.

Explicación
SageMaker batch transform is for batch predictions, which are processed in large batches and not suitable for real-time, low-latency requirements.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 4

A financial services company is using Amazon SageMaker to develop a machine learning model for fraud detection. The dataset contains transaction records, customer profiles, and behavioral data, stored in Amazon S3. The data is updated in real-time, and new fraudulent patterns emerge frequently, requiring regular model updates. To ensure that the model remains effective, the company wants to automate the model retraining process based on new data, detect data drift in real-time, and perform hyperparameter tuning for optimal performance. Additionally, the company needs to ensure that model predictions are explainable to regulators.

Which is the most appropriate solution to meet these requirements?

Respuesta correcta
Use SageMaker Model Monitor to detect data drift, SageMaker Pipelines to automate the retraining process, and SageMaker Clarify to explain model predictions.

Explicación
This option provides a complete solution for the company's needs: SageMaker Model Monitor detects data drift in real-time, which helps determine when the model should be retrained. SageMaker Pipelines is designed to automate end-to-end workflows, including the retraining process based on new data. SageMaker Clarify is essential for generating explainable model predictions, which is important in industries like finance to meet regulatory requirements.

Use SageMaker Feature Store to automatically update features as new data comes in, SageMaker Model Monitor to automate model retraining, and SageMaker Autopilot to handle hyperparameter tuning.

Explicación
Although SageMaker Feature Store is useful for managing features, Model Monitor does not automate model retraining, it only tracks data drift. SageMaker Autopilot does automate training, but this option lacks the automation of model retraining based on data drift detection and explainability for predictions.

Use SageMaker Clarify to detect data drift, SageMaker Feature Store to manage real-time feature updates, and SageMaker Autopilot to retrain the model based on new data.

Explicación
SageMaker Clarify is used to explain model predictions, not to detect data drift. Furthermore, while Autopilot can handle retraining, this option does not provide a method for automating the retraining process based on drift.

Use SageMaker Data Wrangler to preprocess new data, SageMaker Pipelines to automate retraining and hyperparameter tuning, and SageMaker Ground Truth to explain model predictions.

Explicación
SageMaker Data Wrangler is used for data preparation, but it does not detect data drift or retrain models automatically. Additionally, Ground Truth is used for labeling datasets, not for explaining model predictions.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 5

A machine learning engineer is using SageMaker Data Wrangler to understand relationships between features in the dataset. The engineer wants to visualize the distribution of numerical data and identify relationships between features. Which of the following visualization methods should be used in Data Wrangler to achieve these objectives? (Choose TWO)

Selección correcta
Line plots for analyzing trends over time

Explicación
Line plots are helpful for identifying trends over time, which can be useful for analyzing relationships in time series data.

Selección correcta
Histograms for understanding data distribution

Explicación
Histograms are an effective way to visualize the distribution of numerical data and are available in SageMaker Data Wrangler to help understand the spread of values in a dataset.

Partial Dependence Plots (PDP) for feature impact

Explicación
PDPs help assess feature impact on predictions but are not used for basic distribution or trend analysis in Data Wrangler.

ROC curves for model evaluation

Explicación
ROC curves are used for evaluating model performance in classification tasks, not for exploring data distributions.

Confusion matrix for evaluating model predictions

Explicación
Confusion matrices are used to evaluate the performance of classification models, not for understanding feature distributions or relationships.

Temática
Data Preparation for Machine Learning
Pregunta 6

A company is planning to utilize AWS Kinesis Data Firehose for the ingestion of large log files from various devices. Their goal is to ensure data consistency and optimize API call usage. What configuration would best meet their requirements?

Respuesta correcta
Configure Kinesis Data Firehose to buffer data based on maximum time intervals.

Explicación
Configuring Kinesis Data Firehose to buffer data based on time intervals helps in batch processing, which reduces API calls and helps maintain data consistency.

Enable immediate data processing in AWS Lambda without buffering.

Explicación
Immediate processing without buffering does not optimize API usage and might not handle data consistency efficiently.

Directly send unbuffered data to Amazon S3 for cost efficiency.

Explicación
Directly sending unbuffered data does not leverage the buffering benefits for optimizing API usage and could lead to inefficiency in handling large data volumes.

Use AWS Glue to handle data ingestion without buffering.

Explicación
AWS Glue is primarily an ETL service and does not inherently provide real-time data ingestion like Kinesis Data Firehose.

Temática
Data Preparation for Machine Learning
Pregunta 7

You are working on a machine learning project where your model frequently encounters issues such as vanishing gradients and overfitting during training. You decide to use Amazon SageMaker Debugger to monitor and debug the model in real-time to resolve these issues before deployment.

Which of the following actions should you take when configuring SageMaker Debugger for this task? (Select Two)

Selección correcta
Configure a Debugger hook in the training script to capture and log tensors, which provide insights into the model's performance.

Explicación
The Debugger hook captures tensors (model states) that provide real-time insights into training progress and issues.

Enable SageMaker Model Monitor to visualize and track resource utilization, such as CPU and GPU, during training.

Explicación
SageMaker Model Monitor is used for data quality monitoring after deployment, not during training.

Selección correcta
Set up built-in or custom debug rules to monitor training metrics and detect issues such as vanishing gradients or overfitting.

Explicación
SageMaker Debugger allows the use of predefined or custom debug rules that help identify issues like vanishing gradients and overfitting.

Use SageMaker Profiler to automatically correct hyperparameters such as learning rate and batch size during training.

Explicación
SageMaker Profiler helps with resource utilization, not hyperparameter tuning. Debugger does not automatically correct hyperparameters.

Temática
ML Model Development
Pregunta 8

You are training a machine learning model using PyTorch on Amazon SageMaker and want to ensure that the system resources, such as CPU and GPU, are being effectively utilized. You plan to use SageMaker Profiler to track these metrics and improve the efficiency of your training job. Which of the following steps should you follow to set up and use SageMaker Profiler correctly? (Select Two)

Configure the SageMaker Profiler to automatically tune your hyperparameters, such as learning rate, based on the resource utilization data.

Explicación
SageMaker Profiler monitors system resource utilization but does not automatically tune hyperparameters such as learning rate. Hyperparameter tuning is a separate process in SageMaker, managed through SageMaker's automatic model tuning capabilities.

Selección correcta
Define the profiler configuration in your SageMaker Estimator, specifying the duration of the profiling in seconds for the CPU and GPU.

Explicación
You need to configure the SageMaker Estimator with profiling settings to capture CPU and GPU utilization data at specific intervals during the training job. This allows for effective monitoring and optimization of resource usage.

Manually enable automatic SageMaker Profiler optimization in the SageMaker console to reduce memory overhead during training.

Explicación
There is no feature in the SageMaker console to automatically reduce memory overhead using the SageMaker Profiler. Resource monitoring requires manual setup in the training script or Estimator configuration.

Selección correcta
Import the necessary SageMaker Profiler modules and add start and stop profiling commands in your PyTorch training script.

Explicación
To track and monitor system resource usage, the SageMaker Profiler modules need to be imported, and start/stop profiling commands must be incorporated into the training script. This ensures that the profiler is actively capturing performance data during training.

Use SageMaker Neo to compile your model for optimized CPU and GPU performance after training is completed.

Explicación
SageMaker Neo is used for compiling models post-training to optimize their performance for specific deployment hardware environments. It is not used during the training process to track system resources.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 9

A data scientist is using SageMaker Script Mode to train a machine learning model. They need to modularize their code by separating model definition, training logic, and inference logic into different Python files. They also need to include several external dependencies. How should the data scientist proceed to ensure SageMaker can correctly run their custom model?

Respuesta correcta
Split the code into multiple Python files, package them, and specify a single entry point script in the SageMaker estimator. Use a requirements.txt file to install dependencies.

Explicación
In SageMaker Script Mode, the data scientist can modularize their code by splitting it into multiple Python files. They need to package these files together and specify a single entry point (the main training script) in the estimator. Any additional dependencies can be specified in a requirements.txt file, ensuring they are installed in the container during training.

Build a custom Docker container that includes all the Python files and dependencies, and then push it to Amazon ECR for SageMaker to use.

Explicación
While custom Docker containers offer control over the environment, they are unnecessary when SageMaker Script Mode already allows modularization of code and handling dependencies in a more streamlined way.

Write all code into a single Python file and include dependencies directly within the script.

Explicación
Writing all code into a single Python file is not a good practice for complex models and tasks, and it does not leverage the flexibility provided by SageMaker Script Mode.

Use SageMaker’s built-in algorithms and ignore the need for custom modularized code.

Explicación
Using SageMaker’s built-in algorithms would not allow the flexibility needed for custom models with external dependencies and modularized code.

Temática
ML Model Development
Pregunta 10

A robotics company is using SageMaker to train a reinforcement learning agent to navigate through a dynamic warehouse environment. During training, the agent sometimes moves farther away from the goal instead of closer. What is the most likely cause of this behavior?

Negative rewards or penalties

Explicación
Penalties are used to discourage undesirable actions. If the agent were receiving the correct negative rewards when moving away from the goal, it would eventually learn to avoid that behavior. The fact that it continues moving in the wrong direction suggests that negative rewards are either missing, too weak, or overpowered by incorrect positive rewards, but they are not the direct cause of the issue.

Adjustments to the action space

Explicación
Adjustments to the action space refer to changes in the possible actions the agent can take, not to the feedback it receives for its actions.

Adjustments to the policy function

Explicación
While the policy function determines how the agent chooses actions based on rewards, it is a consequence of learning rather than a direct reason for the incorrect movement. The agent's policy is learned based on the feedback it receives, meaning that if it is behaving incorrectly, the root cause is more likely to be a flawed reward function rather than an issue with the policy itself.

Respuesta correcta
Positive rewards

Explicación
Agent’s behavior is primarily shaped by the rewards it receives. In reinforcement learning, an agent learns by associating actions with rewards or penalties. If the agent is moving farther away from the goal instead of closer, the most likely explanation is that it is being incorrectly rewarded for moving in the wrong direction. This means that the reward function may be flawed, reinforcing undesirable behavior and misleading the agent into thinking that moving away is beneficial.

Temática
ML Model Development
Pregunta 11

A retail company uses Amazon SageMaker to forecast sales by training a machine learning model on historical data. The data includes columns such as product_id, date, price, promotion, and units_sold. The company wants to improve its forecast accuracy by incorporating new features, such as holidays and competitor pricing.

What is the most effective way to include these new features in the model training workflow?

Use Amazon SageMaker’s hyperparameter tuning to adjust the model based on new features.

Explicación
Hyperparameter tuning adjusts model parameters for optimization but doesn’t add new features.

Enable Amazon SageMaker’s AutoPilot feature to automatically detect and add holiday and competitor pricing features.

Explicación
SageMaker AutoPilot does not automatically add external features such as holidays or competitor pricing. It automates feature engineering but doesn't add these specific contextual features unless they’re included in the dataset.

Use Amazon Forecast, which automatically incorporates external variables, such as holidays, into the forecasting model.

Explicación
Amazon Forecast does incorporate external factors, but in this case, the company is already using Amazon SageMaker. Migrating to Amazon Forecast would require significant reconfiguration and may not provide immediate improvements.

Respuesta correcta
Add the new features to the dataset, retrain the model, and compare the new model’s performance.

Explicación
Adding relevant features (like holidays and competitor pricing) and retraining the model is an effective approach to enhance forecast accuracy. After adding these features, the model’s performance should be evaluated to confirm any improvement.

Temática
Data Preparation for Machine Learning
Pregunta 12

A company is training a large neural network model with billions of parameters on Amazon SageMaker. The model is too large to fit into the memory of a single GPU. What technique should the company use to distribute the training across multiple instances in this scenario?

Gradient Parallelism

Explicación
Gradient parallelism is not a relevant term in SageMaker for distributed training. It is not a recognized parallelism method in this context.

Data Parallelism

Explicación
Data parallelism is used when the model can fit into memory, but the dataset is large. Here, each machine trains the model on a different portion of the data, not splitting the model.

Respuesta correcta
Model Parallelism

Explicación
When a model is too large to fit into a single machine's memory, model parallelism is used to split the model itself across multiple machines or GPUs. Each machine handles a portion of the model, and they communicate during training to share intermediate outputs.

Hybrid Model and Data Parallelism

Explicación
Hybrid Model and Data Parallelism is useful for extremely large models and datasets, but in this case, only model parallelism is necessary to handle the large neural network model.

Temática
ML Model Development
Pregunta 13

A data scientist is designing a SageMaker pipeline to automate an ML workflow. The pipeline needs to train a model, process data, and register the model for future deployments. Which features of SageMaker Pipelines help automate and manage the workflow effectively?

SageMaker Pipelines use a manual step-by-step execution, allowing better control over each task.

Explicación
SageMaker Pipelines automate the workflow, reducing the need for manual intervention.

Each step in the pipeline is manually executed, but results are stored for reuse in future workflows.

Explicación
Pipeline execution is automated, not manual. Reusability is achieved through the pipeline definition and parameters, not manually stored results.

Pipelines can only handle small-scale projects due to limitations in scaling concurrent workflows.

Explicación
SageMaker Pipelines are highly scalable, capable of handling tens of thousands of concurrent ML workflows, making them suitable for both small and large projects.

Respuesta correcta
Pipelines are defined using a directed acyclic graph (DAG), specifying the sequence of steps, and include reusable parameters for customization.

Explicación
SageMaker Pipelines use a directed acyclic graph (DAG) to define the sequence and relationships of steps. Parameters in the pipeline enable reusable, customizable workflows, improving automation and flexibility.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 14

A company uses AWS Glue to automate their ETL processes for data stored in various formats. They are planning to optimize the performance of their Glue ETL jobs which currently handle large volumes of Parquet files. What would be the most cost-effective way to optimize these jobs without compromising on performance?

Manually trigger ETL jobs instead of scheduling them to control when resources are utilized.

Explicación
Manually triggering jobs reduces the ability to automate and optimize usage of resources, potentially leading to higher costs and less efficiency.

Increase the number of DPUs in each ETL job to process data faster.

Explicación
Increasing DPUs would increase costs rather than optimize them.

Respuesta correcta
Implement job bookmarking to track data processed between job runs and reduce data redundancy.

Explicación
Implementing job bookmarking in AWS Glue allows the ETL jobs to track the state of the data processed between job runs, reducing redundancy by processing only the new or changed data since the last job run. This significantly enhances efficiency and cost-effectiveness without sacrificing performance.

Convert all Parquet files into CSV format to simplify processing.

Explicación
Converting Parquet files, which are already efficient for performance due to their columnar storage format, to CSV would actually degrade performance and increase costs.



Temática
Data Preparation for Machine Learning
Pregunta 15

A machine learning engineer wants to collaborate with a team on a project in AWS SageMaker. The team needs to share notebooks, manage multiple users, and ensure that they can easily scale resources based on their deep learning model's GPU requirements. They also prefer to work in an integrated environment that allows seamless transition between data preparation, training, and model deployment. Which SageMaker tool would BEST meet these collaboration and scaling requirements?



Amazon EMR with SageMaker integration

Explicación
Amazon EMR is focused on big data processing and would not provide the fully managed and integrated machine learning environment that SageMaker Studio offers.

Respuesta correcta
SageMaker Studio

Explicación
SageMaker Studio offers a collaborative environment where multiple users can manage and share notebooks. It provides integrated support for scaling resources (e.g., adding GPUs) and allows seamless transitions between data preparation, model training, and deployment—all within a unified interface.

SageMaker Notebook Instances

Explicación
SageMaker Notebook Instances are standalone and not optimized for collaboration with multiple users or managing multiple notebooks within a shared environment.

AWS Lambda with integrated Jupyter notebooks

Explicación
AWS Lambda is used for serverless compute functions and is not designed for managing Jupyter notebooks or collaboration in machine learning workflows.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 16

A company wants to streamline their machine learning workflow by using AWS SageMaker. They need an environment that allows them to handle every step of the machine learning lifecycle, from data preparation to model deployment and monitoring. The company prefers a fully managed environment where infrastructure concerns, such as managing servers, can be avoided.
Which statement BEST describes how AWS SageMaker meets these needs?

Respuesta correcta
SageMaker simplifies the machine learning workflow by providing a fully managed service, handling data preparation, model training, and deployment without requiring manual server management.

Explicación
AWS SageMaker provides a fully managed environment for every step of the machine learning lifecycle, including data preparation, model training, hyperparameter tuning, deployment, and monitoring. This allows users to focus on machine learning tasks without worrying about server management or infrastructure setup.

SageMaker integrates with Amazon EC2, which must be manually configured for machine learning tasks, while SageMaker handles data preparation and monitoring.

Explicación
EC2 is not required for SageMaker, as SageMaker handles the entire infrastructure needed for machine learning workflows.

SageMaker only assists in the model training phase, and the company must use additional AWS services for data preparation and model deployment.

Explicación
SageMaker handles the entire ML workflow, not just model training. It also manages data preparation, deployment, and monitoring.

SageMaker requires manual setup of servers and other infrastructure for model training but simplifies model deployment with managed endpoints.

Explicación
SageMaker does not require manual server setup. The infrastructure for model training and deployment is fully managed.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 17

A machine learning team is using SageMaker Pipelines to automate the training and deployment of multiple machine learning models. They want to scale the pipeline to handle thousands of workflows simultaneously while ensuring that steps, such as model training and evaluation, are executed in a specific sequence. Which feature of SageMaker Pipelines ensures that steps are executed in a defined order?

Model Package

Explicación
A Model Package is a versioned model in SageMaker's Model Registry, not related to pipeline step ordering.

Respuesta correcta
Directed Acyclic Graph (DAG)

Explicación
The Directed Acyclic Graph (DAG) defines the sequence and dependencies between the steps of the pipeline, ensuring that steps are executed in a specific order.

SageMaker Model Monitor

Explicación
SageMaker Model Monitor is used for monitoring models in production, not for defining step order in a pipeline.

Pipeline Parameters

Explicación
Pipeline parameters are used to set variables, not to define execution order.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 18

A retail company wants to use machine learning to predict customer churn. The company does not have enough expertise to build and train a model from scratch, but they need a reliable model that can quickly be deployed and customized with their customer data. The company is considering Amazon SageMaker JumpStart as a solution.
Which of the following actions should the company take to achieve its goal using SageMaker JumpStart? (Choose TWO.)

Train a custom model on SageMaker from scratch using their own algorithms to meet the customer churn prediction requirements.

Explicación
The company doesn't need to build and train a model from scratch, especially when they have access to pre-built models via JumpStart.

Manually configure the infrastructure, storage, and compute resources required to deploy the pre-trained model from SageMaker JumpStart.

Explicación
SageMaker JumpStart simplifies the deployment of pre-trained models without requiring manual configuration of infrastructure, compute, or storage.

Selección correcta
Fine-tune a pre-built customer churn model available on SageMaker JumpStart using the company's proprietary data.

Explicación
SageMaker JumpStart allows users to fine-tune pre-built models using their own data. This enables the model to adapt better to the company's specific dataset, improving prediction accuracy.

Deploy a SageMaker endpoint without any modifications or fine-tuning to ensure model accuracy on their data.

Explicación
Deploying a pre-trained model without fine-tuning may not yield the best results since the model might not be optimized for the company's specific data.

Selección correcta
Browse the SageMaker JumpStart model hub to find a pre-built customer churn prediction model and deploy it directly.

Explicación
SageMaker JumpStart offers a library of pre-built, state-of-the-art models, including customer churn prediction models, which can be directly deployed for use. This allows the company to skip the process of developing a model from scratch.

Temática
ML Model Development
Pregunta 19

An ML engineer wants to build a sentiment analysis model using pre-built solutions in SageMaker JumpStart. They need a model that can quickly be deployed and then customized using their own dataset.

What features of SageMaker JumpStart would be most useful for the ML engineer's needs?

Respuesta correcta
The ability to fine-tune foundation models with custom datasets and perform hyperparameter tuning.

Explicación
SageMaker JumpStart allows the ML engineer to quickly deploy pre-trained foundation models, which can then be fine-tuned using the engineer's custom dataset. This feature is ideal for adapting the pre-trained model to specific use cases like sentiment analysis, and hyperparameter tuning helps optimize model performance.

Only the use of prompt engineering to modify the inputs without needing to retrain the model.

Explicación
While prompt engineering is useful for modifying input formats, it is not a complete solution for tailoring models to specific datasets or for sentiment analysis. Fine-tuning the model is necessary for best results.

Pre-built solutions that automatically perform both data preprocessing and model training without user intervention.

Explicación
Pre-built solutions in SageMaker JumpStart streamline the process but still require user input, such as providing datasets and tuning hyperparameters. The process is not completely automatic.

The option to use custom-built proprietary models with full infrastructure control.

Explicación
SageMaker JumpStart focuses on pre-built solutions and pre-trained models. Custom-built proprietary models would require more complex setup and control over infrastructure.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 20

A data analyst wants to extract meaningful insights from unstructured customer feedback stored in Amazon S3. The insights should include key entities like people, organizations, and locations mentioned in the feedback, as well as the overall sentiment of the feedback. Which combination of Amazon Comprehend features should the analyst use to achieve this goal?

Keyphrase Extraction and Topic Modeling

Explicación
Keyphrase Extraction identifies significant phrases, but it does not identify entities or determine sentiment. Topic Modeling groups documents by common themes, which isn’t directly relevant for analyzing individual feedback.

Respuesta correcta
Entity Recognition and Sentiment Analysis

Explicación
Entity Recognition identifies specific entities (like people, organizations, and locations), and Sentiment Analysis determines the overall sentiment (positive, negative, neutral, or mixed) of the customer feedback.

Topic Modeling and Keyphrase Extraction

Explicación
While Topic Modeling and Keyphrase Extraction are useful for grouping documents and identifying key concepts, they do not provide entity recognition or sentiment analysis.

Text Classification and Entity Recognition

Explicación
Text Classification helps categorize text into predefined categories, but it does not directly extract entities or analyze sentiment.

Temática
Data Preparation for Machine Learning
Pregunta 21

A retail company uses Amazon SageMaker to train machine learning models for product recommendation. The team stores raw customer behavior data, such as clicks and purchases, in an Amazon S3 bucket. They want to preprocess this data to remove duplicates, normalize numerical features, and encode categorical variables before training their model. Additionally, the preprocessing needs to scale to handle increasing data volumes.

Which solution will most effectively meet their requirements?

Write a preprocessing script in Python and run it on Amazon EC2 instances with auto-scaling enabled.

Explicación
Running custom scripts on EC2 instances introduces complexity and lacks the managed, scalable environment of SageMaker Processing Jobs.

Respuesta correcta
Use Amazon SageMaker Processing Jobs with a pre-built container for data preprocessing and store the processed data back in S3.

Explicación
Amazon SageMaker Processing Jobs provide a managed environment to preprocess data at scale, leveraging pre-built containers for tasks like normalization and encoding. This approach is scalable, integrated with SageMaker, and ideal for ML pipelines.

Leverage AWS Glue for ETL to preprocess the data, and then load the transformed data into Amazon SageMaker.

Explicación
AWS Glue is powerful for ETL tasks but less tailored for ML-specific preprocessing needs compared to SageMaker Processing Jobs.

Use Amazon Athena to query the data and perform all preprocessing using SQL, then export the processed data back to Amazon S3.

Explicación
Athena is primarily used for querying data, not preprocessing tasks like feature scaling or encoding.

Temática
Data Preparation for Machine Learning
Pregunta 22

A machine learning model deployed into production has started underperforming due to the introduction of new data, resulting in the model making less accurate predictions. The team is concerned about both data drift (changes in input data distribution) and model drift (declining model accuracy).
Which AWS service combination is most suitable for addressing these issues?

Respuesta correcta
SageMaker Model Monitor to track data drift and SageMaker Pipelines to automate model retraining.

Explicación
SageMaker Model Monitor can track data drift by monitoring changes in input data distribution. SageMaker Pipelines automates the process of retraining models when performance issues like model drift are detected.

AWS CloudTrail to monitor model drift and AWS Lambda to retrain the model.

Explicación
AWS CloudTrail tracks API calls, not data or model drift. AWS Lambda is useful for serverless compute but not for managing model retraining workflows.

Amazon S3 to store new data and SageMaker Model Monitor to manage model drift.

Explicación
While Amazon S3 can store new data, it doesn’t address model drift or data drift. SageMaker Model Monitor tracks data drift, but it doesn’t directly manage model retraining without SageMaker Pipelines.

SageMaker Feature Store to track data distribution and Amazon Kinesis to monitor model predictions.

Explicación
SageMaker Feature Store helps manage and reuse features, but it is not designed for monitoring data distribution. Amazon Kinesis is for real-time data streaming, not model monitoring.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 23

A data scientist is building a fraud detection model using Amazon SageMaker. The dataset is highly imbalanced, with only 2% of the data representing fraudulent cases. The scientist wants to ensure the model can effectively detect fraud while minimizing false negatives.

Which techniques should the scientist use to handle this issue? (Choose TWO.)

Train the model on the original dataset and rely on evaluation metrics for adjustments.

Explicación
Training on the original dataset without adjustments can lead to a biased model that underperforms on the minority class.

Use Amazon SageMaker Random Cut Forest (RCF) algorithm for anomaly detection.

Explicación
Random Cut Forest is used for anomaly detection but may not be effective for structured fraud detection tasks.

Perform dimensionality reduction to reduce the number of features and make the dataset more manageable.

Explicación
Dimensionality reduction does not directly address class imbalance and may lead to the loss of important features.

Selección correcta
Use weighted loss functions during training to penalize misclassification of the minority class.

Explicación
Weighted loss functions assign a higher penalty for misclassifying the minority class, addressing the imbalance during model training.

Selección correcta
Use oversampling techniques, such as SMOTE, to balance the dataset.

Explicación
Oversampling methods like SMOTE can help balance the dataset by generating synthetic examples for the minority class, improving the model's ability to detect fraud.

Temática
ML Model Development
Pregunta 24

A DevOps team needs to monitor the CPU and memory utilization of their containerized applications running in Amazon ECS. They also need to monitor custom metrics such as the number of active users. Which combination of AWS services and features is the MOST appropriate for this scenario?

Use AWS Lambda to regularly query ECS task performance and push CPU and memory metrics into CloudWatch for analysis.

Explicación
Using Lambda to query ECS metrics manually is unnecessary because CloudWatch can automatically collect these metrics directly from ECS without the need for custom solutions.

Respuesta correcta
Use AWS CloudWatch to monitor ECS task-level metrics such as CPU and memory utilization. Install the CloudWatch Agent to push custom metrics for active users.

Explicación
CloudWatch can monitor ECS task-level metrics such as CPU and memory utilization automatically. By installing the CloudWatch Agent, the team can also push custom metrics, such as the number of active users, into CloudWatch for comprehensive monitoring.

Configure CloudTrail to capture ECS API calls and use EventBridge to push the CPU and memory metrics into CloudWatch.

Explicación
CloudTrail logs API calls but does not capture performance metrics such as CPU and memory utilization.

Enable AWS Config to track ECS task performance and set up CloudWatch Alarms for CPU and memory metrics.

Explicación
AWS Config does not track performance metrics. It is useful for tracking changes in configuration but not for monitoring CPU or memory utilization in ECS tasks.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 25

A team at a media company wants to use Amazon SageMaker Jumpstart to quickly deploy a pre-trained model for text generation. They are considering whether to customize the model to better suit their specific use case. Which of the following customization methods available in Jumpstart should the team use to adapt the pre-trained model to their needs? (Choose TWO)

Perform manual hyperparameter tuning using SageMaker Studio, as Jumpstart models do not support automatic hyperparameter tuning.

Explicación
Jumpstart allows for automatic hyperparameter tuning; manual tuning is not necessary.

Selección correcta
Fine-tune the pre-trained model using their own dataset to improve performance for their specific task.

Explicación
Fine-tuning a pre-trained model with their own dataset enables the team to improve the model’s performance for their specific task, making it more suitable for their use case.

Use SageMaker Autopilot to automatically generate new models from scratch based on their dataset, bypassing the need for pre-trained models.

Explicación
SageMaker Autopilot generates new models rather than customizing pre-trained models, which is not what the team needs.

Selección correcta
Use prompt engineering to design and refine the input prompts so that the model output aligns better with their business requirements.

Explicación
Prompt engineering allows the team to fine-tune the inputs provided to the model, guiding it to produce more relevant responses for their specific use case.

Customize the pre-trained model by changing the underlying model architecture to better suit text analysis tasks.

Explicación
Customizing the architecture of the pre-trained model is not a feature supported by Jumpstart; fine-tuning and prompt engineering are the preferred customization methods.

Temática
ML Model Development
Pregunta 26

Which of the following best describes how Amazon SageMaker handles data parallelism during distributed training?

Respuesta correcta
Splits the dataset across multiple machines, each training the same model on a portion of the data

Explicación
In data parallelism, the dataset is divided into chunks, and each chunk is assigned to a different machine (or worker). Each worker trains the same model on its portion of the data, and then SageMaker synchronizes the gradients across all workers to update the model.

Splits the model across multiple machines and synchronizes the model parameters

Explicación
This describes model parallelism, not data parallelism. In model parallelism, the model is split across multiple machines.

Splits the dataset across multiple machines, each training a different model

Explicación
Each machine trains the same model on a portion of the data, not a different model.

Splits the dataset and model across multiple machines, ensuring both are distributed equally

Explicación
Hybrid parallelism refers to splitting both the dataset and model, but in data parallelism, only the dataset is divided, not the model.

Temática
ML Model Development
Pregunta 27

A machine learning engineer at a healthcare company has deployed a model on Amazon SageMaker and wants to monitor the data quality to ensure that the data used in production maintains high integrity.

Which of the following actions is MOST appropriate for monitoring data quality using Amazon SageMaker?

Enable SageMaker Feature Store to log and validate all incoming data used for predictions and correct data quality issues in real-time.

Explicación
SageMaker Feature Store is used for storing and serving features but is not responsible for real-time validation of data quality.

Use a custom SageMaker algorithm to filter out low-quality data before making predictions in the production environment.

Explicación
Using a custom algorithm to filter data would not be as effective as monitoring and addressing issues after they occur. SageMaker Model Monitor is specifically designed for this purpose.

Set up Amazon CloudWatch to automatically retrain the model whenever the incoming data has quality issues.

Explicación
CloudWatch helps with notifications and monitoring but does not handle automatic retraining based on data quality.

Respuesta correcta
Use SageMaker Model Monitor to evaluate incoming data for missing values, outliers, and deviations from the baseline training data.

Explicación
SageMaker Model Monitor allows for the evaluation of incoming data for various quality issues such as missing values, outliers, and drift by comparing it to the baseline generated from training data.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 28

A healthcare organization needs to analyze large volumes of patient medical forms and identify important entities such as patient names, diagnoses, and medications. These forms include both text and handwritten content. The organization also needs to extract and categorize information to automate the documentation process and store the extracted metadata for further querying. Which combination of services would BEST meet the organization’s requirements?

Respuesta correcta
Amazon Textract for extracting both printed and handwritten text, Amazon Comprehend for entity recognition, and Amazon S3 for storing the extracted metadata.

Explicación
Amazon Textract can extract both printed and handwritten text from documents, and Amazon Comprehend can perform entity recognition (like identifying patient names, diagnoses, and medications). Amazon S3 can be used to store the structured metadata, enabling further analysis or querying.



Amazon Rekognition for detecting text and images, Amazon Comprehend for analyzing the extracted text, and Amazon RDS for storing structured data.

Explicación
Amazon Rekognition is used for detecting objects and facial analysis in images, but it is not as effective as Textract for extracting text from documents, particularly handwritten content. Amazon RDS is a good option for storing structured data, but S3 is better suited for this use case, especially when combined with other querying tools like Athena.

Amazon SageMaker for handwriting recognition, Amazon Comprehend for sentiment analysis, and Amazon Kendra for querying the extracted metadata.

Explicación
Amazon SageMaker is not a handwriting recognition service. For extracting text from handwritten documents, Amazon Textract is a more appropriate service. Amazon Comprehend can be used for entity recognition, but Amazon Kendra is more appropriate for natural language search, not for storing and querying extracted metadata.

Amazon Comprehend for entity extraction, Amazon Textract for analyzing images and text, and Amazon Elasticsearch for indexing and querying the results.

Explicación
While Amazon Elasticsearch (or OpenSearch) is good for indexing and searching text, Amazon Textract should be used for extracting both text and handwriting, and Amazon S3 is more commonly used for scalable storage in such workflows.

Temática
Data Preparation for Machine Learning
Pregunta 29

Which of the following strategies is MOST effective in reducing overfitting when training an AI model, according to responsible AI practices?

Reducing the size of the training dataset to speed up training.

Explicación
Reducing the size of the training dataset can lead to underfitting, not overfitting, as the model will have fewer data points to learn from.

Utilizing a more complex model with a larger number of parameters to improve accuracy.

Explicación
A more complex model with too many parameters is likely to overfit, as it may capture noise and irrelevant patterns in the training data.

Increasing the training data without any validation steps to capture more patterns.

Explicación
While increasing training data can help, it is important to validate the model to ensure it is generalizing well, rather than relying solely on more data.

Respuesta correcta
Using regularization techniques that add penalties to extreme model parameters.

Explicación
Regularization helps prevent overfitting by penalizing extreme parameters, thereby encouraging the model to focus on general patterns rather than noise in the data.

Temática
ML Model Development
Pregunta 30

You are a data scientist tasked with developing a model for a large e-commerce platform. The company needs a solution that minimizes development time and operational complexity. The team is considering using Amazon SageMaker's built-in algorithms instead of building custom models.
Which of the following advantages do built-in SageMaker algorithms offer for this use case? (Choose TWO)

Built-in algorithms can only be used for simple tasks like linear regression and are not suitable for complex machine learning tasks.

Explicación
Built-in algorithms are versatile and optimized for both simple and complex tasks, including text analysis, image classification, and unsupervised learning.

Selección correcta
Built-in algorithms like XGBoost and Linear Learner abstract away infrastructure setup and allow quick model deployment.

Explicación
Algorithms like XGBoost and Linear Learner are among the built-in algorithms, designed to perform well across a variety of tasks, and they abstract away the complexity of the infrastructure, allowing fast deployment.

Selección correcta
Pre-configured environments and infrastructure management are handled by AWS, eliminating the need for custom containers.

Explicación
Built-in SageMaker algorithms are pre-packaged in containers, meaning AWS manages the infrastructure, simplifying the training process and eliminating the need for custom container setups.

Custom algorithms provide faster training times and more efficient resource usage compared to built-in algorithms.

Explicación
Custom algorithms may offer more flexibility, but they do not necessarily offer faster training times or better resource management compared to built-in algorithms.

Custom frameworks such as PyTorch and TensorFlow must be used with SageMaker to enable hyperparameter tuning.

Explicación
Hyperparameter tuning can be applied to both custom and built-in algorithms in SageMaker, so using PyTorch or TensorFlow is not required for this.

Temática
ML Model Development
Pregunta 31

An ML engineer is using Amazon SageMaker Data Wrangler to preprocess data for a machine learning project. They need to import customer transaction data from various sources, clean and transform the data, and engineer features for model training. Which TWO of the following steps should the engineer take to prepare the data using Data Wrangler? (Choose TWO.)

Selección correcta
Use Data Wrangler to apply one-hot encoding on categorical variables.

Explicación
SageMaker Data Wrangler provides built-in features like one-hot encoding to transform categorical variables into a format suitable for machine learning.

Selección correcta
Import data from Amazon S3, Redshift, or SageMaker Feature Store into Data Wrangler.

Explicación
Data Wrangler can import data from multiple sources such as Amazon S3, Redshift, and SageMaker Feature Store, making it easy to preprocess diverse datasets.

Manually write custom Python scripts to handle missing data and normalization tasks.

Explicación
Data Wrangler has a user-friendly visual interface that eliminates the need for manual Python scripting.

Perform data transformations using SageMaker Studio's in-built Jupyter notebooks.

Explicación
While SageMaker Studio allows Jupyter notebooks for custom transformations, Data Wrangler simplifies the process by offering pre-built transformations in a visual interface.

Export the transformed data directly to Amazon Athena for further analysis.

Explicación
Data Wrangler can export the processed data to S3 or directly integrate with SageMaker for training, but Athena is not commonly used for this.

Temática
Data Preparation for Machine Learning
Pregunta 32

A data scientist is facing the challenge of high variance in a machine learning model, where the model performs well on training data but poorly on new, unseen data. Which of the following methods would MOST LIKELY improve the model's ability to generalize, adhering to responsible AI principles?

Remove cross-validation steps to speed up model training and testing.

Explicación
Cross-validation helps to ensure the model generalizes better to new data, so removing it is not recommended.

Focus on training the model using a smaller subset of the data to prevent overfitting.

Explicación
Training on a smaller subset can lead to underfitting, where the model is unable to capture important patterns due to lack of data.

Respuesta correcta
Use early stopping to halt the training once the model's performance starts to degrade on the validation set.

Explicación
Early stopping prevents overfitting by stopping the training process when the model's performance begins to degrade on the validation set, indicating it is starting to overfit the training data.

Increase the number of parameters in the model to capture more complex patterns in the data.

Explicación
Increasing the number of parameters can worsen overfitting, as the model may focus too much on the training data's noise.

Temática
ML Model Development
Pregunta 33

Suppose you have a relatively small model that trains very quickly, but you want to systematically guarantee coverage of the hyperparameter space. Which method is most suitable in this case?

Random Search

Explicación
Random Search samples from the search space rather than covering it completely. It’s typically better if the search space is large, but it does not guarantee testing all configurations.

Respuesta correcta
Grid Search

Explicación
Grid Search enumerates every possible combination from a pre-defined “grid” of hyperparameter values. When training is cheap or the parameter space is small, it provides a comprehensive view of performance across all combinations.

Bayesian Optimization

Explicación
Bayesian Optimization is valuable when each individual evaluation is expensive and you want to minimize the number of trials. It’s not necessarily the best choice if you explicitly want full coverage of a small grid.

Hyperband

Explicación
Hyperband is designed for larger search spaces and relies on early stopping. It’s not specifically intended to fully enumerate the space; its strength is in pruning unpromising configurations quickly.

Temática
ML Model Development
Pregunta 34

An ML engineer is using SageMaker Debugger's profiling feature to optimize resource usage during training. They notice that the GPU utilization is very low, causing longer training times. Which of the following actions could help improve GPU utilization and optimize the training process?

Use SageMaker Neo to optimize the model for better GPU performance during training.

Explicación
SageMaker Neo optimizes models for deployment and inference, not for improving GPU utilization during training.

Migrate the model to CPU-only instances to avoid GPU underutilization.

Explicación
Moving to CPU-only instances would likely reduce performance further, as GPUs are generally more suited for deep learning tasks.

Respuesta correcta
Adjust the batch size and data loading pipeline to ensure that the GPU is being used efficiently throughout the training.

Explicación
Low GPU utilization can often be caused by inefficient data loading or small batch sizes. By adjusting the batch size and optimizing the data pipeline, the GPU can be kept busy, improving overall utilization and reducing training time.

Use SageMaker Debugger to monitor for vanishing gradients, which will improve GPU utilization automatically.

Explicación
Monitoring vanishing gradients with SageMaker Debugger is useful for debugging model convergence issues but won't directly improve GPU utilization.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 35

A machine learning engineer is using Amazon SageMaker Data Wrangler to prepare data for training a model. The dataset has missing values, inconsistent scales across features, and categorical data. Which of the following actions can the engineer perform using Data Wrangler to resolve these issues? (Choose THREE)

Manually code all the transformations using Python and upload the final dataset to SageMaker.

Explicación
While custom transformations can be coded, Data Wrangler offers a wide array of built-in visual tools and does not require manual coding for standard tasks like normalization and encoding.

Selección correcta
Normalize feature values to ensure all features are on a similar scale.

Explicación
Normalization and scaling are supported within Data Wrangler to ensure that features are on the same scale, which is crucial for training machine learning models.

Selección correcta
One-hot encode categorical data to convert it into a numerical format.

Explicación
Data Wrangler supports one-hot encoding for categorical data, which transforms categories into binary vectors, a necessary step for many machine learning models.

Use Amazon EMR to pre-process the data for missing values and scaling.

Explicación
Amazon EMR is not necessary for this purpose since Data Wrangler can handle pre-processing tasks such as missing value imputation and scaling directly.



Selección correcta
Perform imputation to fill in missing values using strategies like mean or median.

Explicación
SageMaker Data Wrangler allows filling in missing values using imputation methods like mean, median, or mode, making it a built-in feature for handling incomplete data.

Temática
Data Preparation for Machine Learning
Pregunta 36

A company wants to optimize the data ingestion process for a machine learning pipeline in Amazon SageMaker. The data, stored in Amazon S3, consists of large JSON files, and the current ETL job takes a long time to process the data before training. To improve performance, they want to minimize read and I/O times, especially for downstream analysis using Amazon Athena.

Which format should the company use to store their data in S3 for better processing and analytics performance?

Convert the data to JSON Lines format to reduce file size.

Explicación
JSON Lines format may reduce file size compared to traditional JSON but does not improve performance significantly for large-scale queries in comparison to Parquet.

Respuesta correcta
Use Apache Parquet to store the data in S3.

Explicación
Apache Parquet is a columnar storage format, which optimizes read and I/O times, especially for analytical workloads like those processed by Amazon Athena. Parquet also offers compression, reducing storage costs and improving performance.

Convert the data to CSV format and store it in S3.

Explicación
While CSV is a common format, it is not columnar and does not provide efficient I/O performance for large-scale analytics, as each row has to be fully read even if only certain columns are needed.

Compress the data as a ZIP file and upload it to S3.

Explicación
Compressing data in ZIP format might reduce storage space, but it does not optimize data processing performance in analytical workloads.

Temática
Data Preparation for Machine Learning
Pregunta 37

A company is using Amazon S3 to store sensitive business documents. To enhance security, they want to grant access to the S3 bucket only to certain IAM users, while denying access to others. Additionally, the company requires the S3 bucket to be accessed by an EC2 instance running in a different AWS account, but only for reading data.

Which combination of AWS IAM features should be used to meet these requirements? (Choose TWO)

Use a customer-managed IAM policy attached to the S3 bucket to define granular access for the EC2 instance and the IAM users.

Explicación
Customer-managed IAM policies should be attached to IAM users or roles, not to S3 buckets; a bucket policy could be used but is less flexible in this cross-account scenario.

Use AWS Lambda to check the IAM users’ permissions before allowing access to the S3 bucket.

Explicación
AWS Lambda is unnecessary for permission checks, as IAM policies can directly control access without introducing a compute layer.

Selección correcta
Create a role in the other AWS account with a trust policy allowing the EC2 instance to assume the role and read from the S3 bucket.

Explicación
A trust policy in a role allows the EC2 instance from another AWS account to assume the role and gain the necessary permissions to access the S3 bucket.

Selección correcta
Use an identity-based policy to attach to IAM users that grants or denies access to the S3 bucket.

Explicación
Identity-based policies are ideal for granting or denying access to individual IAM users for specific actions on resources like S3 buckets.

Apply a resource-based bucket policy in Amazon S3 to explicitly deny access from all users except the EC2 instance and required IAM users.

Explicación
Resource-based policies are more commonly used to define permissions on the resource itself, but in this case, a combination of identity-based policies and roles with trust policies provides better control.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 38

A company wants to ensure that all its EC2 instances are monitored for both CPU utilization and disk read/write operations. They also need to track configuration changes to these EC2 instances over time, such as changes to security groups, instance type, and AMI IDs. Which combination of AWS services will best meet these requirements?

Use CloudWatch Logs to capture both CPU utilization and configuration changes and create custom log filters for disk operations.

Explicación
CloudWatch Logs captures log data, not resource metrics or configuration changes. It would require additional agents and custom filters, making it unnecessarily complex.

Set up CloudWatch Alarms for CPU and disk operations and AWS CloudTrail for tracking configuration changes on EC2 instances.

Explicación
CloudWatch Alarms monitor performance metrics but do not track configuration changes. CloudTrail logs API calls but is not ideal for tracking detailed configuration state changes.

Respuesta correcta
Use AWS CloudWatch to monitor CPU utilization and disk read/write operations, and AWS Config to track configuration changes to the EC2 instances.

Explicación
AWS CloudWatch provides built-in metrics for CPU utilization and disk read/write operations on EC2 instances, while AWS Config tracks configuration changes (e.g., security group, instance type, AMI ID) for compliance and audit purposes.

Enable AWS CloudTrail to monitor CPU and disk usage and use AWS Config to capture configuration changes.

Explicación
AWS CloudTrail logs API calls but does not monitor resource metrics like CPU and disk operations. It also does not track configuration changes in the way AWS Config does.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 39

A machine learning engineer is conducting multiple model training sessions in SageMaker using different sets of hyperparameters and algorithms to find the best configuration for their model. They want to keep track of every trial and ensure the reproducibility of their experiments. Which AWS service feature would help them organize and compare these configurations effectively?

SageMaker Ground Truth

Explicación
This is used for labeling data for machine learning models, not for tracking and comparing experiments.

SageMaker Autopilot

Explicación
Autopilot automates model development but does not specifically handle organizing and comparing trials like SageMaker Experiments does.

Respuesta correcta
SageMaker Experiments

Explicación
Saker Experiments allows users to organize and track different model training configurations and trials, ensuring that each run's parameters and metrics are logged. It provides a way to compare various trials and identify the best-performing model configuration.

Amazon EC2 Spot Instances

Explicación
Spot instances are used for cost-effective computing but are unrelated to experiment tracking or comparison.

Temática
ML Model Development
Pregunta 40

A financial firm wants to use reinforcement learning to optimize trading strategies using Amazon SageMaker. They want to train an agent that interacts with a simulated trading environment and receives feedback in the form of profits or losses. Which of the following elements must be included to formulate the reinforcement learning problem in this context?

Training the model using a supervised learning algorithm

Explicación
Supervised learning is not used in reinforcement learning. RL uses a feedback loop to adjust the agent’s behavior rather than learning from labeled

Configuring manual data labeling for the agent's learning

Explicación
Reinforcement learning does not require manual data labeling, unlike supervised learning.

Respuesta correcta
Defining the agent, environment, actions, and rewards

Explicación
To formulate a reinforcement learning problem, the agent, environment, actions, and rewards must be defined. These are essential components for any RL setup. In this case, the agent would make trading decisions, the environment would simulate the market, actions represent trading actions, and rewards would be the profits or losses from those trades.

Creating static rules for decision-making

Explicación
Reinforcement learning does not rely on static rules but rather learns through trial and error, dynamically adjusting based on feedback.

Temática
ML Model Development
Pregunta 41

A financial services company is using Amazon SageMaker Model Monitor to track the feature attribution drift of their credit risk prediction model. They notice that the importance of a particular feature, "annual income," has shifted significantly over time, making it a more dominant factor in the model's predictions.

What is the BEST course of action to address this shift in feature importance?

Switch to a different model algorithm that automatically reduces the importance of "annual income" in predictions.

Explicación
Switching to a different algorithm is not required for addressing feature attribution drift. This drift can typically be managed with data updates and retraining.

Retrain the model with a larger dataset that excludes the "annual income" feature to prevent bias in predictions.

Explicación
Excluding the "annual income" feature is drastic and unnecessary. Adjustments to the model can be made without removing a potentially important feature.

Fine-tune the hyperparameters of the existing model to lower the weight of the "annual income" feature.

Explicación
Fine-tuning hyperparameters may not address the root cause, which is changes in the data or feature importance over time.

Respuesta correcta
Use SageMaker Model Monitor to alert the team about this shift and retrain the model with updated data to ensure feature importance remains balanced.

Explicación
Feature attribution drift means that the importance of certain features has changed over time, which could lead to poor generalization or bias. Retraining the model with updated data will help realign the feature importance.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 42

You are evaluating the performance of a machine learning model and want to ensure that it generalizes well on unseen data. You decide to partition your dataset into three distinct sets: training, validation, and testing. However, your model performs exceptionally well on the training set but poorly on the validation and test sets. What is the likely cause of this performance discrepancy?

The model is underfitting the data.

Explicación
Underfitting leads to poor performance on both the training and test datasets, which is not the case here.

The model has a high bias and low variance.

Explicación
A high bias indicates underfitting, where the model is too simplistic, and a low variance suggests it doesn’t capture the complexity of the data, which would result in poor performance across all datasets.

Respuesta correcta
The model is overfitting the training data.

Explicación
Overfitting occurs when the model performs well on the training data but poorly on unseen data, such as the validation and test sets. This happens because the model learns the noise and intricate details of the training data, reducing its generalizability.

The training set is too small for meaningful evaluation.

Explicación
While the size of the training set can influence performance, this scenario is primarily due to overfitting, not the dataset size.

Temática
ML Model Development
Pregunta 43

A company is using Amazon Kinesis to monitor IoT sensor data for real-time security threats. The system needs to detect anomalies in the data stream and trigger alerts immediately, minimizing latency. They plan to have multiple consumers process the data streams concurrently.

What approach should the company take to ensure optimal scalability and low latency?

Use Kinesis Firehose to scale automatically and reduce latency for each consumer.

Explicación
Kinesis Firehose is for delivering data to storage systems, not for optimizing read throughput or reducing latency.

Respuesta correcta
Enable enhanced fan-out for each consumer to achieve dedicated read throughput and lower latency.

Explicación
Enhanced fan-out ensures that each consumer has its own dedicated throughput, which helps reduce latency and increase scalability when multiple consumers are involved.

Use AWS Glue for ETL and real-time data analysis of the streaming data.

Explicación
AWS Glue is designed for batch ETL tasks and not suitable for real-time streaming or low-latency data analysis.

Use standard Kinesis consumers and scale the number of shards as needed.

Explicación
Standard consumers share throughput, leading to potential bottlenecks and increased latency when scaling to multiple consumers.

Temática
Data Preparation for Machine Learning
Pregunta 44

A Machine Learning Engineer is using Amazon SageMaker to build and train a binary classification model. The training dataset contains a severe class imbalance, with 95% of the data belonging to the negative class and only 5% to the positive class. After training, the model achieves high accuracy but performs poorly on the positive class.

Which two techniques should the engineer consider to address this issue? (Choose TWO.)

Use the area under the ROC curve (AUC) as the evaluation metric.

Explicación
AUC provides an overall measure of model performance but does not directly tackle the issue of class imbalance during training.

Decrease the size of the negative class through undersampling.

Explicación
Undersampling the negative class can lead to data loss and may reduce model performance if too many data points are removed.

Selección correcta
Increase the size of the positive class through oversampling.

Explicación
Oversampling the positive class can help mitigate class imbalance, as it increases the presence of minority class instances, making the model more likely to learn patterns in this class.

Selección correcta
Apply class weights to penalize misclassification of the minority class.

Explicación
Applying class weights to penalize misclassification of the minority class helps the model pay more attention to the positive class, improving its sensitivity toward it.

Use precision as the primary evaluation metric instead of accuracy.

Explicación
Precision focuses on the ratio of true positives to false positives but does not address the underlying issue of class imbalance directly.

Temática
ML Model Development
Pregunta 45

A company wants to integrate generative AI capabilities into their applications using foundation models from multiple AI providers. They need a solution that offers a serverless environment, flexibility in model customization, and ensures that data remains within a specific AWS region for compliance reasons.
Which AWS service would best meet these requirements?

Amazon SageMaker

Explicación
Amazon SageMaker is focused on building, training, and deploying machine learning models but doesn't offer direct integration with third-party foundation models for generative AI like Bedrock.

Respuesta correcta
Amazon Bedrock

Explicación
Amazon Bedrock is designed for generative AI applications and provides access to foundation models from various providers. It operates in a serverless environment, allowing users to deploy models without managing infrastructure. It also ensures data stays within a specific AWS region, adhering to compliance and security needs.

Amazon Rekognition

Explicación
Amazon Rekognition is a service for image and video analysis, not generative AI.

Amazon Lex

Explicación
Amazon Lex is used for building conversational interfaces such as chatbots and voice agents, not for deploying foundation models for generative AI.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 46

An e-commerce company needs to build a recommendation engine using a machine learning model trained on customer activity data, including purchase history, browsing patterns, and demographics. The team wants to streamline their model development process by creating a centralized repository of preprocessed features that can be reused across different projects. The system needs to support real-time recommendations and batch training.

What is the MOST appropriate setup for the team to ensure consistent feature reuse, while also optimizing for real-time and batch processing?

Use SageMaker Ground Truth to label the data and store the features in an offline store for training only.

Explicación
SageMaker Ground Truth is for labeling data, not for managing features or optimizing for real-time/batch processes.

Respuesta correcta
Use the SageMaker Feature Store to create an offline store for batch training and an online store for real-time inference.

Explicación
The SageMaker Feature Store supports both an online and offline store. The online store is optimized for low-latency, real-time access (ideal for recommendations), while the offline store is used for training models and batch inference.

Use the SageMaker Feature Store to create an online store for batch processing and an offline store for real-time recommendations.

Explicación
Batch processing is better suited for the offline store, while real-time recommendations require low-latency access from the online store.

Store all features in Amazon S3 and manually preprocess them for each model training run.

Explicación
Manually storing and preprocessing features in Amazon S3 introduces unnecessary complexity and does not benefit from the centralized and reusable nature of the SageMaker Feature Store.

Temática
Data Preparation for Machine Learning
Pregunta 47

A data scientist is setting up an environment to develop, train, and deploy machine learning models using AWS SageMaker. The scientist wants a fully managed Jupyter notebook environment that simplifies infrastructure setup and provides pre-installed libraries such as TensorFlow and PyTorch. Additionally, they want to ensure that their work is automatically saved and can be accessed later.
Which AWS SageMaker feature would BEST meet these requirements?

AWS Glue with integrated Jupyter environment

Explicación
AWS Glue is designed for ETL processes and does not include a Jupyter notebook environment for machine learning tasks.

Respuesta correcta
SageMaker Notebook Instances

Explicación
SageMaker Notebook Instances provide fully managed Jupyter notebooks that simplify machine learning workflows. These notebooks automatically handle infrastructure setup, including server configuration, and come with popular ML libraries like TensorFlow and PyTorch pre-installed. They also offer persistent storage, allowing work to be saved and accessed later.

SageMaker Studio with Jupyter Lab spaces

Explicación
SageMaker Studio is a more comprehensive environment but is not explicitly tied to individual Jupyter notebook instances. It is designed for larger-scale, integrated workflows with multiple notebooks and users.

Amazon EC2 with Jupyter notebook manually installed

Explicación
Amazon EC2 would require manual setup and maintenance, including installing and configuring Jupyter notebooks and the required ML libraries, which is more complex and not fully managed.

Temática
ML Model Development
Pregunta 48

A data scientist is building a machine learning pipeline using Amazon SageMaker Pipelines. The pipeline includes data preprocessing, model training, and model registration. The data scientist wants to ensure that parameters used for model training can be easily adjusted without modifying the pipeline itself. Which feature of SageMaker Pipelines should the data scientist use to achieve this?

Respuesta correcta
Pipeline Parameters

Explicación
Pipeline parameters allow the user to introduce variables that can be overridden when executing the pipeline, providing flexibility without needing to modify the pipeline itself.

Directed Acyclic Graph (DAG)

Explicación
The Directed Acyclic Graph (DAG) defines the flow of the pipeline but is unrelated to parameter flexibility.

Pipeline Steps

Explicación
Pipeline Steps define actions in the pipeline but do not offer parameter flexibility.

SageMaker Model Registry

Explicación
SageMaker Model Registry is used for registering and managing models, not for setting parameters in a pipeline.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 49

An ML engineer wants to automate the hyperparameter tuning of their machine learning model using Amazon SageMaker. To minimize costs, they also want to utilize spot instances during the tuning process. Which combination of factors must the engineer specify to initiate the automatic tuning process in SageMaker?

Dataset, algorithm, performance metric

Explicación
The performance metric is not included in the initial inputs for setting up the tuning job.

Algorithm, performance metric, model parameters

Explicación
Model parameters are learned during training, not set before the tuning process.

Algorithm, hyperparameter range, performance metric

Explicación
While a performance metric is important for evaluating the model, it is not one of the initial factors required to start the tuning job.

Respuesta correcta
Hyperparameter range, dataset, algorithm

Explicación
To initiate automatic tuning in SageMaker, the engineer must provide the dataset, the algorithm to be used, and the range of hyperparameters to explore.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 50

A company needs to ensure that their data storage solutions on AWS comply with industry regulations that require encryption of data at rest. Which AWS service and feature should be implemented to meet this compliance requirement across multiple storage services?

Implement AWS Shield Advanced for automatic data encryption across all AWS services.

Explicación
AWS Shield is primarily for DDoS protection, not for data encryption.

Use AWS Lambda to apply encryption to data as it is stored in each service.

Explicación
This is not efficient or scalable as Lambda would require custom scripts for each service.

Respuesta correcta
Enable Amazon S3 server-side encryption (SSE) with AWS KMS-managed keys (SSE-KMS).

Explicación
Amazon S3 server-side encryption (SSE) with AWS KMS-managed keys (SSE-KMS) provides robust encryption for data at rest, meeting compliance requirements across services that integrate with S3 for storage, such as Amazon Redshift, Amazon EMR, and others.

Configure AWS IAM policies to enforce encryption on data storage services.

Explicación
It is useful for access control but does not directly handle encryption.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 51

A data scientist is using SageMaker Experiments to track and compare model training runs. They want to ensure that detailed metadata, such as hyperparameters, datasets, and code changes, are logged automatically for each trial. Which feature of SageMaker Experiments provides this functionality?

SageMaker Feature Store

Explicación
SageMaker Feature Store is used to store and manage features used in training, but it is not responsible for tracking experiment metadata.

Model Monitor

Explicación
Model Monitor is used to monitor models in production for data and model drift but does not handle logging trial metadata during the experimentation phase.

Trial Components

Explicación
Trial Components refer to different stages of an individual trial, but they do not handle automatic logging of metadata.

Respuesta correcta
Trackers

Explicación
Trackers in SageMaker Experiments automatically log metadata for each trial, including hyperparameters, datasets, and changes to the code. This ensures reproducibility and traceability for every experiment.

Temática
ML Model Development
Pregunta 52

Which of the following scenarios best describes the use of supervised learning?

A financial institution segments its customers based on spending patterns without predefined categories.

Explicación
This describes unsupervised learning (clustering) where customer segments are formed without predefined categories or labels.

Respuesta correcta
A company develops a predictive model to classify emails as either spam or not spam using historical labeled data.

Explicación
Supervised learning involves using labeled data to train a model to predict outcomes. In this case, the model uses labeled historical data (spam or not spam) to classify emails, making it a supervised learning task.

A retail business uses machine learning to automatically group products based on their similarity without providing labeled data.

Explicación
This is another example of unsupervised learning, where products are grouped without labeled data, such as customer preferences.

A gaming company implements a system where an AI learns to improve its strategy by playing games and receiving feedback from wins and losses.

Explicación
This describes reinforcement learning, where the AI interacts with an environment and learns through rewards or penalties, not labeled data.

Temática
Data Preparation for Machine Learning
Pregunta 53

A company deployed a binary classification model on a real-time Amazon SageMaker endpoint. To ensure long-term performance, they set up a schedule for SageMaker Model Monitor to evaluate model quality based on actual predictions and outcomes over time.
Which of the following steps is required to effectively monitor model quality using SageMaker Model Monitor?

Set up custom metrics for feature attribution and monitor the bias drift for predictions from the real-time endpoint.

Explicación
Feature attribution and bias drift are part of explainability and fairness monitoring but do not directly relate to overall model quality monitoring.

Respuesta correcta
Create baseline statistics from the model’s training data and enable data capture to record requests and predictions.

Explicación
To monitor model quality, it is necessary to create baseline statistics from the training data, enable data capture, and compare predictions to actual outcomes using SageMaker Model Monitor.

Enable scheduled retraining on SageMaker to automatically improve model accuracy without monitoring metrics.

Explicación
Scheduled retraining is not required for monitoring. Monitoring provides insights into when retraining may be necessary, but it doesn't involve automatic retraining.

Enable automatic hyperparameter tuning for the endpoint and log model predictions to Amazon CloudWatch.

Explicación
Hyperparameter tuning is unrelated to model monitoring. Logging predictions is useful, but it does not replace the need for monitoring based on actual outcomes.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 54

An e-commerce company uses a Lambda function triggered by Amazon SNS to preprocess customer transaction data before sending it to an Amazon SageMaker model for fraud detection. The transaction data is then stored in Amazon RDS for long-term analytics. The company wants to ensure that only valid transactions (based on a predefined schema) are passed to the SageMaker model and that invalid transactions are logged and stored in Amazon S3 for further analysis.

Which of the following solutions BEST fits the company's requirements?

Respuesta correcta
Use Lambda to validate the schema, send valid data to SageMaker, and forward invalid data to an Amazon SQS queue, which will trigger another Lambda function to log and store it in S3.

Explicación
Using an SQS queue to handle invalid transactions is a scalable and decoupled solution. The SQS-triggered Lambda function can log and store the invalid transactions in S3, allowing the first Lambda function to focus on validation and fraud detection.

Use Lambda to validate the schema of each transaction, send valid data to SageMaker for inference, and write invalid data directly to Amazon S3.

Explicación
This solution can become inefficient if invalid data logging happens in the same Lambda function, potentially slowing down the fraud detection process.

Use Amazon Kinesis Data Firehose to preprocess and validate the data, then trigger Lambda to send valid data to SageMaker, while invalid data is redirected to S3.

Explicación
Kinesis Data Firehose is typically used for continuous data streaming, and using it for schema validation and transaction filtering is not a common or cost-effective approach in this context.

Set up a Step Functions workflow to manage validation and invoke Lambda for processing valid data and storing invalid data in S3.

Explicación
Step Functions might be overkill for managing simple validation tasks, and this approach adds unnecessary complexity for processing invalid data.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 55

A financial services company is building a fraud detection system to monitor transactions in real-time. They plan to use Amazon Kinesis Data Streams to ingest transaction data and want the system to scale automatically based on demand. They require low-latency for fast fraud detection responses.

Which feature of Amazon Kinesis Data Streams should the company implement to meet these requirements?

Use Kinesis Firehose to automatically scale the stream based on traffic.

Explicación
Kinesis Firehose is used for delivering data to storage destinations, not for scaling data ingestion and real-time processing.

Manually scale shards based on throughput requirements.

Explicación
Manually scaling shards may not meet dynamic scaling needs in real-time scenarios, and it introduces management overhead.

Respuesta correcta
Use enhanced fan-out to reduce consumer latency and ensure dedicated throughput per consumer.

Explicación
Enhanced fan-out provides dedicated throughput to each consumer, ensuring faster data processing with reduced latency, which is crucial for real-time fraud detection.

Enable provisioned mode and dynamically add shards.

Explicación
While provisioned mode allows manual shard scaling, it’s less efficient than enhanced fan-out for handling multiple consumers with low latency.

Temática
Data Preparation for Machine Learning
Pregunta 56

A technology company uses AWS CloudFormation to manage its infrastructure as code. The team wants to create a CloudFormation stack that includes an Amazon S3 bucket and an Amazon EC2 instance. The S3 bucket should only be created if the EC2 instance is successfully launched. If the EC2 instance creation fails, the stack creation should be rolled back, and the S3 bucket should not be created.

Which configuration will meet these requirements?

Use a CreationPolicy on the EC2 instance resource and an UpdatePolicy on the S3 bucket resource.

Explicación
A CreationPolicy helps in specifying how AWS CloudFormation waits for a resource to be created successfully. UpdatePolicy is used for updating resources. These do not directly ensure that the S3 bucket creation is dependent on the successful creation of the EC2 instance.

Use a custom resource to check the status of the EC2 instance creation and create the S3 bucket based on that status.

Explicación
Using a custom resource could achieve the desired result, but it is more complex and unnecessary when the DependsOn attribute can directly fulfill the requirement.

Use the Condition attribute to create a condition that checks if the EC2 instance is created and use that condition in the S3 bucket resource.

Explicación
The Condition attribute is used for conditional creation of resources based on parameters or other conditions, but it cannot directly check the success status of another resource’s creation.

Respuesta correcta
Use the DependsOn attribute in the S3 bucket resource to specify the EC2 instance resource.

Explicación
The DependsOn attribute ensures that the specified resource (in this case, the S3 bucket) is created only after the specified dependent resource (the EC2 instance) has been successfully created. If the EC2 instance creation fails, the stack will roll back, and the S3 bucket will not be created.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 57

What is the main difference between model parameters and hyperparameters in machine learning?

Respuesta correcta
Hyperparameters are set before training, while model parameters are learned during training.

Explicación
Hyperparameters are external configurations that are set before training, such as learning rate or depth of a decision tree. Model parameters, on the other hand, are learned during training as the model adjusts to the data.

Both model parameters and hyperparameters are learned during training.

Explicación
Hyperparameters are not learned during training; they must be defined beforehand.

Both model parameters and hyperparameters are set before training.

Explicación
Model parameters are learned during training, unlike hyperparameters, which are set prior to it.

Model parameters are set before training, while hyperparameters are learned during training.

Explicación
This option reverses the roles of hyperparameters and model parameters.

Temática
ML Model Development
Pregunta 58

A company is building a multi-lingual customer support system that allows users to communicate both via text and voice. The system should transcribe customer support calls into text, translate the text into the user’s preferred language, and generate audio responses in the same language. Which combination of AWS services is MOST appropriate for this use case?

Use AWS Lambda to convert speech to text, Amazon Translate to handle text translations, and Amazon SageMaker to generate speech responses.

Explicación
AWS Lambda is not used for transcription or speech generation, and Amazon SageMaker is a machine learning service, not a speech synthesis tool.

Use Amazon Transcribe to convert speech to text, Amazon Lex to handle translations, and Amazon Polly to generate the voice responses.

Explicación
Amazon Lex is a service for building conversational interfaces but does not perform translations. Amazon Translate is the appropriate service for translating text.

Respuesta correcta
Use Amazon Transcribe to convert speech to text, Amazon Translate to translate the text, and Amazon Polly to convert the translated text to speech.

Explicación
Amazon Transcribe is designed to convert speech to text, Amazon Translate can handle the text translation into the user’s preferred language, and Amazon Polly can convert the translated text back into speech, creating a fully integrated multi-lingual system.

Use Amazon Polly to convert speech to text, AWS Glue to manage translations, and Amazon Transcribe to generate speech responses.

Explicación
Amazon Polly is used for text-to-speech, not speech-to-text conversion, and AWS Glue is an ETL service, not designed for managing translations.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 59

You are working with large datasets stored in an Amazon S3 bucket and need to optimize the performance of querying and processing the data in AWS Glue. The data is frequently queried for specific years. Which of the following strategies will MOST effectively reduce query execution time and cost in this scenario?

Store the data in an Amazon RDS database and run queries directly from there.

Explicación
Storing the data in an RDS database introduces higher storage and query costs compared to using S3 and partitioning with Glue. It’s also not necessary for large-scale distributed data.

Store all data in a single folder and apply a custom filter during each query.

Explicación
Storing all data in a single folder would force all data to be scanned during each query, increasing execution time and cost.

Respuesta correcta
Partition the data by year and define Glue crawlers to automatically recognize these partitions.

Explicación
Partitioning the data by year allows queries to skip irrelevant partitions (such as other years), reducing the amount of data scanned. Glue crawlers can automatically recognize these partitions, optimizing the query performance and reducing costs (since services like Athena charge based on the amount of data scanned).

Use AWS Glue to compress the data, but do not partition it.

Explicación
Compressing data without partitioning may reduce storage costs but won’t optimize query execution. Partitioning is more effective in reducing query time and cost.

Temática
Data Preparation for Machine Learning
Pregunta 60

A law firm wants to create an intelligent search system for internal legal documents. These documents are stored in different formats (PDFs, Word documents, and scanned images), and the firm needs to extract and index key information like case names, legal terms, and dates. The system should also allow the legal team to perform natural language searches to quickly retrieve relevant documents. Which combination of AWS services should the firm use to achieve this goal?

Amazon Rekognition to analyze the scanned images, Amazon Comprehend to detect entities and extract text from documents, and Amazon Kendra to provide natural language search capabilities.

Explicación
While Amazon Rekognition can analyze images, Amazon Textract is more suited for extracting text from scanned documents, which is critical in this scenario. Amazon Comprehend can be used for entity recognition, and Amazon Kendra is an excellent choice for creating a natural language search system.

Amazon Comprehend to categorize and index the documents, Amazon Lex for voice interaction with the system, and Amazon S3 for storing the document metadata.

Explicación
Amazon Lex is a service for building conversational interfaces (e.g., chatbots), which is not required in this use case. Amazon Comprehend is useful, but Kendra is the appropriate tool for building a search system.

Respuesta correcta
Amazon Textract for extracting text from scanned images, Amazon Comprehend for entity recognition, and Amazon Kendra for building an intelligent search interface.

Explicación
Amazon Textract is perfect for extracting text from scanned documents, and Amazon Comprehend can recognize key legal terms and entities like case names. Amazon Kendra is designed specifically for intelligent search across unstructured data and can provide the needed natural language search functionality.

Amazon Kendra to store the documents, Amazon Rekognition for facial recognition in the scanned images, and Amazon Textract for analyzing legal terms.

Explicación
Amazon Kendra does not store documents—it indexes them for search. Additionally, Amazon Rekognition is more suited for facial recognition and not necessary in this legal document context. Amazon Textract can handle text extraction from documents, but this option doesn't fully address the need for entity recognition and search.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 61

A data engineer is tasked with creating an Extract, Transform, Load (ETL) pipeline using AWS Glue to process sales data from various sources. The data, in CSV format, is stored in Amazon S3. The processed data needs to be stored in a columnar format for faster querying using Amazon Athena. The engineer also needs to ensure the schema is automatically detected and stored for future use.

Which of the following steps should the data engineer take to meet these requirements? (Choose TWO)

Set up manual scripts to define the schema for each data source and store it in Amazon RDS.

Explicación
Writing manual scripts for schema management is not necessary when AWS Glue provides built-in automation for schema detection and storage.

Configure AWS Lambda to process and load the data into Athena for schema inference and storage.

Explicación
While AWS Lambda is useful for other event-driven processes, AWS Glue is specifically designed for ETL and schema detection.

Selección correcta
Use an AWS Glue crawler to automatically detect the schema of the CSV files and store the metadata in the AWS Glue Data Catalog.

Explicación
AWS Glue crawlers can automatically detect the schema of files and store it in the Glue Data Catalog, allowing for easy future querying.

Selección correcta
Use AWS Glue's built-in transformation to convert the CSV data into Apache Parquet before saving it back to S3.

Explicación
AWS Glue allows for seamless conversion of CSV files into columnar formats such as Apache Parquet, which improves query performance in tools like Amazon Athena.

Manually create a schema in Amazon Athena and upload it to the Glue Data Catalog to enable querying.

Explicación
You don’t need to manually create schemas in Athena; the Glue Crawler can infer and store them in the Data Catalog automatically.

Temática
Data Preparation for Machine Learning
Pregunta 62

A transportation company is analyzing GPS data from its fleet of vehicles to optimize delivery routes. The company needs to preprocess and clean this data, create relevant features like average speed and distance traveled, and store these features in a scalable feature store. The team wants to ensure the features are available for real-time predictions on delivery times and wants to train a machine learning model using these features in an easily managed notebook environment.

Which services should the company use to preprocess the data, store the features, and train the model?

Respuesta correcta
Use SageMaker Data Wrangler for data preprocessing, SageMaker Feature Store to store features, and SageMaker Notebooks to train and deploy the machine learning model.

Explicación
SageMaker Data Wrangler provides a no-code solution for data preprocessing and feature engineering. SageMaker Feature Store is perfect for storing and managing features in real-time and batch inference. SageMaker Notebooks allow for a seamless environment to train and deploy models.

Use SageMaker Data Wrangler for preprocessing, Amazon Redshift for feature storage, and SageMaker Notebooks to train and deploy the machine learning model.

Explicación
While SageMaker Data Wrangler is ideal for preprocessing, Amazon Redshift is primarily used for analytical queries and is not as well-suited for real-time feature storage compared to SageMaker Feature Store.

Use SageMaker Notebooks for data preprocessing, Amazon DynamoDB for feature storage, and SageMaker Feature Store to train the machine learning model.

Explicación
Amazon DynamoDB is not designed for feature storage in machine learning workflows. SageMaker Feature Store is a more suitable choice for storing and managing features for both real-time and batch predictions.

Use Amazon QuickSight to preprocess and clean the data, SageMaker Feature Store for feature storage, and SageMaker Autopilot to train the machine learning model.

Explicación
Amazon QuickSight is a business intelligence tool, not a preprocessing tool for machine learning data. SageMaker Data Wrangler would be a better fit for preprocessing and feature engineering tasks.

Temática
Data Preparation for Machine Learning
Pregunta 63

Which of the following statements about AWS IAM roles is correct?

IAM roles do not support inline policies and can only be defined with AWS managed policies.

Explicación
IAM roles support both managed and inline policies, giving flexibility to define permissions as needed.

IAM roles are used only by AWS services and cannot be assumed by IAM users or external identities.

Explicación
IAM roles can indeed be assumed by IAM users, groups, or external identities, not just AWS services.

IAM roles are associated directly with an IAM user and always tied to a specific user account.

Explicación
Roles are not directly tied to specific users but can be assumed by any user, service, or account that has permission, making them flexible for various scenarios.



Respuesta correcta
IAM roles can be assumed by AWS services or external identities and can have multiple policies attached to manage permissions.

Explicación
IAM roles are used to delegate permissions to AWS services or external identities and allow multiple policies to manage specific permissions.



Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 64

A company is using Amazon SageMaker Debugger to monitor and debug their machine learning model during training. They encounter an issue where the model's performance is no longer improving, and they suspect it might be due to disappearing gradients. Which of the following steps should they take to resolve this issue?

Manually stop the training job and restart it with more training data to solve the disappearing gradient issue.

Explicación
Restarting the training job with more data doesn’t address disappearing gradients, which is related to how the gradients are computed and propagated during training.

Use SageMaker Neo to optimize the model performance for deployment on edge devices to avoid disappearing gradients.

Explicación
SageMaker Neo is used for model optimization for inference and deployment but does not solve issues related to disappearing gradients during training.

Increase the batch size in the training script to capture disappearing gradients more effectively.

Explicación
Increasing batch size may not address disappearing gradients directly; other techniques such as adjusting the learning rate are more effective.

Respuesta correcta
Set up a custom rule in SageMaker Debugger to detect vanishing gradients and adjust the learning rate or change the optimizer.

Explicación
SageMaker Debugger allows real-time monitoring, and users can set up custom rules or use built-in rules to detect issues like vanishing gradients. Adjusting the learning rate or using different optimizers can help mitigate disappearing gradients.

Temática
ML Model Development
Pregunta 65

A machine learning engineer is using Amazon SageMaker Studio to perform exploratory data analysis on a large dataset stored in Amazon S3. The analyst needs to create visualizations, run data preprocessing workflows, and share the results with team members. The analyst also wants to avoid setting up infrastructure manually. Considering these requirements, which SageMaker features should the analyst leverage to achieve these tasks efficiently? (Select THREE)

Store the dataset in the SageMaker notebook instance's local storage to avoid the complexity of using external services like Amazon S3.

Explicación
Storing data locally on a SageMaker notebook instance would complicate the process. Instead, Amazon S3 offers better scalability, persistence, and integration for managing datasets.

Use SageMaker Automatic Model Tuning to automatically handle hyperparameter optimization during the preprocessing and analysis steps.

Explicación
SageMaker Automatic Model Tuning is useful during model training, but it does not apply to preprocessing or data analysis tasks.

Use Amazon SageMaker's Real-Time Inference Endpoints to deploy the model and generate immediate predictions on the dataset.

Explicación
Real-Time Inference Endpoints are used for deploying models to serve predictions, but this is not relevant for preprocessing and visualization tasks.

Selección correcta
Utilize SageMaker Studio's JupyterLab environment to interactively run code cells, visualize data, and tweak preprocessing workflows without worrying about underlying infrastructure.

Explicación
SageMaker Studio's JupyterLab environment provides an interactive space for running code, visualizing data, and modifying workflows without requiring manual infrastructure setup, making it ideal for data exploration and preprocessing tasks.

Selección correcta
Share SageMaker Studio notebooks directly with team members for collaboration, ensuring that they can access the same environment and results.

Explicación
SageMaker Studio allows sharing notebooks with other team members, enabling collaboration and shared access to results and workflows. This is especially useful for teams working together on the same dataset.

Selección correcta
Take advantage of the built-in integrations with popular data visualization libraries such as Matplotlib and Seaborn to create interactive data visualizations.

Explicación
SageMaker Studio supports data visualization libraries such as Matplotlib and Seaborn, which can be used to create rich, interactive visualizations for the analysis.

Temática
ML Model Development