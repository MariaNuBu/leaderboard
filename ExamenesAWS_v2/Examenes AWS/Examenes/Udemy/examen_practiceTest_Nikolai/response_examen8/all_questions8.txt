Pregunta 1

A machine learning engineer is using Amazon SageMaker to develop a model. They want to use a managed Jupyter notebook environment that will allow them to write and execute code, visualize data, and train models without needing to configure or maintain the underlying infrastructure.
Which of the following options provides the most suitable solution?

AWS Lambda

Explicación
AWS Lambda is a serverless compute service, but it does not offer a managed Jupyter notebook environment.

Amazon EMR

Explicación
Amazon EMR is used for big data processing and is not designed for managing Jupyter notebooks.

Respuesta correcta
SageMaker Notebook Instances

Explicación
SageMaker Notebook Instances provide a fully managed Jupyter notebook environment specifically designed for machine learning tasks. These notebooks automatically handle infrastructure setup, including the server environment, software, and library installation.

AWS Glue

Explicación
AWS Glue is a serverless ETL service and is not related to Jupyter notebooks or interactive model development.

Temática
ML Model Development
Pregunta 2

A machine learning team wants to identify the most important features in their dataset using Amazon SageMaker Data Wrangler's "Quick Model" feature. What is the primary purpose of using this feature?

To apply advanced deep learning techniques for feature engineering.

Explicación
The Quick Model feature is not designed for deep learning or advanced feature engineering; it's meant to provide a quick, initial evaluation of feature importance.

Respuesta correcta
To automatically split the dataset, train a simple model, and evaluate feature importance.

Explicación
The "Quick Model" feature in SageMaker Data Wrangler automatically splits the dataset, trains a simple model, and provides feature importance scores to evaluate how much each feature contributes to the target variable.

To visualize the correlation matrix of the dataset to identify highly correlated features.

Explicación
While correlation matrices help identify relationships between features, the Quick Model provides direct feature importance estimates rather than visualizing correlations.

To perform automated hyperparameter tuning for the best model performance.

Explicación
Quick Model does not perform hyperparameter tuning; its purpose is to quickly assess the impact of different features, not to optimize model performance.

Temática
ML Model Development
Pregunta 3

You are working with SageMaker built-in algorithms and need to perform image classification. Which of the following statements correctly describes the process of using a SageMaker built-in algorithm for training and deployment?

SageMaker built-in algorithms require the use of custom containers where you manually install the necessary libraries.

Explicación
Custom containers are not necessary for SageMaker's built-in algorithms, as AWS provides pre-packaged containers.

Respuesta correcta
You need to bring your dataset and specify hyperparameters, but the algorithm is pre-packaged and managed by AWS, including the necessary infrastructure setup.

Explicación
With SageMaker built-in algorithms, you need to provide the dataset and define hyperparameters, but AWS handles the infrastructure and packaging of the algorithm, streamlining the training process.

The built-in algorithms come fully trained and do not require further training on your specific dataset, ensuring immediate deployment.

Explicación
Built-in algorithms still require training on your specific dataset, as they are not pre-trained models.

SageMaker built-in algorithms cannot be used for image classification tasks, and custom algorithms must be used for these purposes.

Explicación
SageMaker built-in algorithms support a wide variety of tasks, including image classification.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 4

A data scientist at a financial institution wants to train a machine learning model using Amazon SageMaker's built-in algorithms. They need to optimize for ease of use and infrastructure management, while still having the flexibility to define their hyperparameters and perform automatic hyperparameter tuning. The data scientist is considering the built-in "XGBoost" algorithm for a classification task. Which of the following steps are necessary to properly set up and train the model using the built-in algorithm?

Customize the XGBoost algorithm container and manage infrastructure manually before training.

Explicación
The built-in algorithms come with pre-packaged containers. There is no need to manually customize the container or manage the infrastructure.

Respuesta correcta
Use the pre-packaged XGBoost algorithm container, specify the data location, define the hyperparameters, and select an appropriate compute instance.

Explicación
SageMaker's built-in algorithms, such as XGBoost, provide pre-packaged containers that abstract away infrastructure management. The user only needs to specify the data, define hyperparameters, and select compute resources, simplifying the setup process.

Develop a custom XGBoost container to allow for proprietary data processing pipelines.

Explicación
SageMaker's built-in XGBoost container already provides optimized infrastructure. Developing a custom container is unnecessary unless a highly specialized use case requires it.

Install the XGBoost framework locally, develop the model, and upload the trained model to SageMaker for inference.

Explicación
SageMaker's built-in algorithms allow direct training on the platform. There is no need to develop the model locally and upload it separately.

Temática
ML Model Development
Pregunta 5

When using Amazon SageMaker's automatic hyperparameter tuning, which of the following must be specified to start the tuning job? (Select TWO)

Selección correcta
Performance metric

Explicación
A performance metric, such as accuracy or mean squared error, must be specified to evaluate the quality of different hyperparameter combinations.

Selección correcta
Training algorithm

Explicación
The training algorithm is a key input, as SageMaker uses it to apply the hyperparameter combinations and train the model.

Batch size for model training

Explicación
Batch size can be a hyperparameter but isn’t a mandatory input for starting a SageMaker hyperparameter tuning job.

Number of EC2 spot instances

Explicación
EC2 spot instances help reduce costs but are not required to define or start a tuning job.

A fixed set of hyperparameter values

Explicación
Rather than a fixed set, a range of hyperparameters is provided for exploration.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 6

Which of the following scenarios would be most appropriate for using Amazon SageMaker Script Mode over creating a custom Docker container?

The machine learning team needs to use a custom version of TensorFlow that has specialized libraries not supported by SageMaker’s built-in containers.

Explicación
This scenario would require a custom Docker container rather than using SageMaker's Script Mode. The reason is that SageMaker Script Mode is designed for easier use of SageMaker's pre-configured environments for popular machine learning frameworks (like TensorFlow, PyTorch), but it doesn't allow you to fully customize those environments beyond a few parameters. If you need a custom version of TensorFlow with specialized libraries, creating a custom Docker container would provide full control over the environment.

Respuesta correcta
A data scientist wants to focus on model development using a pre-configured PyTorch environment while avoiding the complexity of managing container infrastructure.

Explicación
This scenario is ideal for Amazon SageMaker Script Mode. Script Mode allows users to bring their own training or inference scripts to SageMaker without needing to manage the infrastructure or create custom containers. The pre-configured PyTorch environment would handle the underlying setup, letting the data scientist focus purely on model development without worrying about managing or configuring Docker containers.

A developer requires full control over the operating system, libraries, and dependencies for an advanced C++ model.

Explicación
SageMaker Script Mode is not suitable here. If the developer needs full control over the operating system, libraries, and dependencies, this requires creating a custom Docker container. SageMaker Script Mode allows you to write scripts in Python for supported frameworks, but it does not provide low-level control over the environment like a Docker container would.

The model needs to be trained and deployed in a completely different programming language, such as Rust, which SageMaker does not support natively.

Explicación
Amazon SageMaker Script Mode is not appropriate for this scenario because it supports only Python scripts for specific frameworks (TensorFlow, PyTorch, etc.). If the model is written in a language like Rust, which SageMaker does not natively support, the team would need to create a custom Docker container that includes the Rust environment, libraries, and any necessary dependencies.

Temática
ML Model Development
Pregunta 7

A machine learning team is using Amazon SageMaker Experiments to run multiple training jobs with different hyperparameter configurations and feature sets. They want to organize, track, and compare the results to identify the best-performing model. Which of the following components in SageMaker Experiments allows the team to track and compare each individual training run within the experiment?

SageMaker Pipelines

Explicación
SageMaker Pipelines is used to orchestrate and automate machine learning workflows but does not specifically track individual training runs.

Respuesta correcta
Trials

Explicación
In SageMaker Experiments, a trial refers to an individual training run with a specific configuration of hyperparameters, feature sets, or algorithms. Each trial can be tracked and compared to other trials within the same experiment.

SageMaker Autopilot

Explicación
SageMaker Autopilot automates the process of selecting algorithms and tuning hyperparameters but is not a core component of SageMaker Experiments.

Trial Components

Explicación
Trial Components are the stages of each training run (e.g., data preprocessing, model training), but they are part of an individual trial, not the entire training job.

Temática
ML Model Development
Pregunta 8

A startup working on autonomous driving technology needs to label a large dataset of road images for training their object detection model. The team wants to minimize the cost of labeling while maintaining high-quality results. They aim to use a combination of human labelers and automation to label straightforward objects while having human reviewers handle more complex cases.

Which of the following strategies BEST fits their needs?

Respuesta correcta
Use SageMaker Ground Truth with active learning to automate labeling of simple cases while sending complex cases to human labelers for review.

Explicación
SageMaker Ground Truth offers active learning, which automates the labeling of simpler cases based on previously labeled data while allowing human labelers to handle more complex instances. This reduces the cost of labeling while maintaining accuracy.

Use SageMaker Feature Store to preprocess and label the images before passing them to the machine learning model for training.

Explicación
SageMaker Feature Store is for managing features used in model training, not for data labeling tasks.

Use a fully automated labeling approach without human intervention to reduce labeling costs.

Explicación
Fully automated labeling may reduce costs but is likely to result in lower quality and less accurate labels for complex cases.

Use SageMaker Experiments to run multiple trials of different labeling approaches to determine which strategy reduces costs the most.

Explicación
SageMaker Experiments helps manage model training configurations, not labeling tasks.

Temática
Data Preparation for Machine Learning
Pregunta 9

During a model training session, you notice that your training job is taking significantly longer than expected. To identify the root cause, you decide to use the SageMaker Profiler to analyze resource bottlenecks, such as CPU and GPU underutilization. Which of the following actions should you take to correctly set up SageMaker Profiler for this task?

Respuesta correcta
Add SageMaker Profiler start and stop commands in the training script to collect system resource metrics like CPU and GPU utilization.

Explicación
To track system resources, SageMaker Profiler requires the start and stop profiling commands to be added to the training script, which collects metrics such as CPU, GPU, and memory usage.

Define custom debug rules to monitor vanishing gradients and early stopping criteria during training.

Explicación
Custom debug rules focus on model-specific metrics like vanishing gradients, not system resources.

Use SageMaker Model Monitor to track and visualize the training job's input data distribution changes.

Explicación
SageMaker Model Monitor is for post-deployment data quality, not resource monitoring during training.

Automatically correct detected bottlenecks by reducing the training batch size using SageMaker Debugger’s real-time insights.

Explicación
SageMaker Profiler helps identify bottlenecks but does not automatically change batch sizes or hyperparameters.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 10

A machine learning team has deployed a model to production using Amazon SageMaker. They set up SageMaker Model Monitor to detect any data drift. After several weeks, they notice that the predictions of the model are becoming less accurate compared to the actual outcomes, and SageMaker Model Monitor has flagged significant differences in the input data distribution over time.

What is the MOST likely cause of the declining model performance, and what is the recommended action?

Data drift has occurred, causing the model to receive input data that is different from the training data. The team should update the model's hyperparameters to resolve the issue.

Explicación
Updating hyperparameters may not fix the root cause, which is data drift, not model architecture.

Respuesta correcta
Data drift has occurred, causing the input data distribution to deviate from the training data. The team should retrain the model with updated training data to resolve the issue.

Explicación
Data drift refers to a change in the distribution of the input data over time, which can cause the model to perform poorly. Retraining the model with updated data reflecting the new distribution can resolve this issue.

Model quality drift has occurred because the model’s predictions are becoming less accurate. The team should switch to a different algorithm to improve performance.

Explicación
While model quality drift is possible, the description points to data drift as the main cause of reduced performance. Switching algorithms without addressing data drift won't necessarily fix the issue.

Bias drift has occurred, leading the model to favor certain outcomes. The team should retrain the model with a new algorithm and set stricter constraints in SageMaker Model Monitor.

Explicación
Bias drift typically affects fairness or bias-related outcomes, not overall accuracy. The problem here is more related to the data distribution.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 11

A data science team deployed a model using Amazon SageMaker real-time endpoints. They are concerned that the distribution of data in production may change over time, potentially leading to data drift. To mitigate this risk, they want to set up continuous monitoring of the model in production. Which of the following does Amazon SageMaker Model Monitor allow the team to track to address their concerns? (Select TWO)

Model Latency Drift

Explicación
Model Monitor does not track latency; it's primarily for data and model quality.

Selección correcta
Data Drift

Explicación
This occurs when the production data becomes significantly different from the training data, and SageMaker Model Monitor can track this to detect changes in the input data patterns over time.

Selección correcta
Feature Attribution Drift

Explicación
SageMaker Model Monitor can track feature attribution drift, which indicates changes in the relative importance of features in model predictions.

AWS CloudFormation Drift

Explicación
This is unrelated to SageMaker Model Monitor, as it pertains to infrastructure as code.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 12

A machine learning engineer is tasked with automating the registration, versioning, and deployment of models as part of a workflow using SageMaker Pipelines. The goal is to track model development, including data preprocessing, training runs, and version history. The engineer needs to ensure that the pipeline can log the necessary details for auditing and compliance.
Which SageMaker feature should the engineer use to track the history of the model’s development?

Model Group

Explicación
Model Group organizes versions of models but does not track development history.

Approval Status

Explicación
Approval Status controls deployment of models but does not track development history.

Respuesta correcta
Model Lineage

Explicación
Model Lineage tracks the history of model development, including data used, pre-processing steps, and training runs, making it crucial for auditing and compliance.

Model Metrics

Explicación
Model Metrics store model performance data but do not track the history of model development.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 13

An organization uses SageMaker Pipelines to automate its ML workflow. After training a model, they need to register it with the SageMaker Model Registry, which tracks different versions of the model. They want to ensure that only approved models are deployed to production.

Which feature of SageMaker Model Registry should be used to control the deployment of models?

Model Group

Explicación
Model Group organizes model versions, but it does not control deployment approval.

Respuesta correcta
Approval Status

Explicación
The approval status feature in SageMaker Model Registry allows users to control which model versions are approved for deployment to production.

Model Package

Explicación
A Model Package represents a version of a model but does not manage deployment approval.

Model Lineage

Explicación
Model Lineage tracks the origin and development of models but does not manage deployment control.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 14

A retail company wants to predict if a customer will purchase a product based on historical data, including past purchases, browsing behavior, and demographic details. Which type of machine learning model is best suited for this task?

K-Means Clustering

Explicación
This is an unsupervised learning algorithm used to group similar data points into clusters, not for making a binary prediction.

Regression

Explicación
Regression is used to predict continuous values, such as house prices or sales, but not for binary outcomes.

Respuesta correcta
Binary Classification

Explicación
The problem involves predicting whether a customer will make a purchase (yes or no), which makes it a binary classification problem. Supervised learning models like classification are used when the target variable is categorical, such as whether or not a customer will make a purchase.

Reinforcement Learning

Explicación
Reinforcement learning is used for problems that involve learning through interaction with an environment to maximize rewards, not for binary classification tasks.

Temática
ML Model Development
Pregunta 15

A data scientist is training a machine learning model to classify emails as either spam or not spam. They want to evaluate the model using various metrics. If the model correctly classifies 85% of the emails as spam or not spam, which metric is being used in this scenario?

Precision

Explicación
Precision measures how many of the predicted positives (spam emails) are actually positive, which is not mentioned in the question.

Recall

Explicación
Recall (or sensitivity) measures how many of the actual positive instances (spam emails) were correctly identified. The question refers to the overall classification rate, not just the positive cases.

Respuesta correcta
Accuracy

Explicación
Accuracy measures the proportion of correctly classified instances (in this case, emails) out of the total number of instances. In this scenario, 85% of the emails were correctly classified, which is an indication of accuracy.

F1 Score

Explicación
The F1 Score is the harmonic mean of precision and recall, which balances both metrics. The question does not mention precision or recall, so the F1 score does not apply here.

Temática
ML Model Development
Pregunta 16

Which of the following are key features of Amazon Bedrock that simplify the management and scaling of generative AI applications? (Choose TWO)

Selección correcta
Provisioned throughput mode for handling large, steady workloads.

Explicación
Amazon Bedrock offers a provisioned throughput mode, which is useful for large, steady workloads, ensuring reliable performance by allowing users to purchase throughput units for token processing.

Ability to directly modify the foundation models' source code for deeper customization.

Explicación
Bedrock does not allow direct modification of the foundation models' source code; customization is done through fine-tuning.

Selección correcta
Serverless infrastructure with automatic scaling and built-in data encryption.

Explicación
Bedrock operates in a serverless environment, which handles automatic scaling and infrastructure management. Additionally, it ensures data encryption both at rest and in transit.

Requirement to configure and manage underlying EC2 instances for model hosting.

Explicación
Bedrock does not require manual management of EC2 instances, as it is fully serverless.

Fine-tuning models with custom data to improve performance for specific tasks.

Explicación
While fine-tuning is available, this option alone doesn’t fully describe the infrastructure simplification that Bedrock offers, unlike provisioned throughput and serverless infrastructure.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 17

A retail company plans to use Amazon Bedrock to build a product recommendation system that generates personalized suggestions for users. The company wants to enhance the accuracy of the recommendations using real-time data and integrate additional knowledge sources to improve model responses.
Which features of Amazon Bedrock should the company use to meet these needs? (Choose TWO)

Selección correcta
Use agents in Amazon Bedrock to break down tasks and connect to real-time data via APIs.

Explicación
Agents in Amazon Bedrock can help by breaking down complex tasks and connecting to APIs for real-time data, improving the accuracy of the recommendation system by integrating up-to-date information.

Leverage AWS Lambda to trigger model updates after each user interaction.

Explicación
While AWS Lambda can trigger actions, it is not a direct feature of Bedrock for enhancing model accuracy in real-time.

Selección correcta
Use Amazon Bedrock's knowledge bases to incorporate private data into the recommendation model.

Explicación
Knowledge bases in Amazon Bedrock can be used to incorporate additional private data, making the model responses more personalized and relevant for the recommendation engine.

Configure EC2 instances to run additional AI models for better performance.

Explicación
Amazon Bedrock is a serverless solution and does not require configuring EC2 instances for running additional models.

Enable cross-region data sharing for better model accuracy.

Explicación
Bedrock ensures data remains within the AWS region for security and compliance, and cross-region sharing is generally not recommended for data privacy.

Temática
Data Preparation for Machine Learning
Pregunta 18

A company is setting up an AI-powered assistant using Amazon Q Business to streamline internal operations. They want to generate summaries of meetings and ensure that the assistant can retrieve relevant information from a knowledge base to provide accurate responses.
What key feature of Amazon Business should be implemented to meet these requirements?

Respuesta correcta
Use the Retrieval Augmented Generation (RAG) system to pull information from the knowledge base.

Explicación
Retrieval Augmented Generation (RAG) allows the AI assistant to enhance responses by retrieving relevant information from a knowledge base before generating answers. This ensures the responses are accurate and contextual.

Set up a custom plugin to generate responses based on Jira tickets.

Explicación
While plugins like Jira are useful for task management, they are not directly related to generating accurate summaries or retrieving relevant knowledge.

Deploy the assistant with access to only Amazon S3 as the primary data source.

Explicación
Limiting the data source to just Amazon S3 restricts the assistant’s ability to provide comprehensive answers across multiple data sources.

Disable the knowledge base integration to ensure faster response times.

Explicación
Disabling the knowledge base would negatively impact the accuracy and relevance of the assistant's responses.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 19

An organization has large volumes of log data stored in Amazon S3. They plan to process this data using AWS Glue ETL jobs, which will run daily to extract, transform, and load the data into Amazon Redshift. The organization wants to ensure that only the newly added log files are processed in each ETL run to reduce cost and optimize performance.

Which AWS Glue feature can be leveraged to achieve this goal most cost-effectively?

AWS Glue Crawler with automatic schema detection

Explicación
AWS Glue Crawler detects schema changes but does not handle incremental data loading on its own.

Respuesta correcta
Incremental loading using partitioning

Explicación
AWS Glue supports incremental loads by processing only data that has been newly added since the last ETL run. This can be achieved by partitioning data or by tracking the data load status.

Using AWS Lambda to trigger ETL jobs on every new file upload

Explicación
AWS Lambda can trigger jobs based on file uploads, but it does not inherently manage incremental data loads.

Defining custom transformations in AWS Glue to identify new data

Explicación
Custom transformations can be applied in AWS Glue, but incremental loading is achieved more effectively through built-in partitioning and tracking.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 20

A machine learning team is using AWS Glue to preprocess large datasets for training models. They need to ensure that the Glue ETL jobs are optimized for cost without sacrificing performance. The team wants to apply custom transformations and only pay for the compute resources they use.

What are the best practices the team should follow to manage AWS Glue costs? (Choose TWO)

Use the AWS Glue visual interface to build ETL jobs instead of custom scripts to save on costs.

Explicación
While the visual interface is convenient, cost savings come from managing resources effectively, not necessarily the interface choice.

Selección correcta
Configure the Glue job to run with a minimal number of Data Processing Units (DPUs) that meet performance requirements.

Explicación
AWS Glue jobs are charged based on the DPUs consumed, so configuring the job to use the minimum required DPUs for performance optimization helps in managing costs.

Selección correcta
Use AWS Glue Crawlers to run on a schedule and detect changes in the data schema, reducing the need for constant job execution.

Explicación
By scheduling AWS Glue Crawlers to detect schema changes, the team can avoid unnecessarily rerunning ETL jobs, reducing costs.

Implement manual ETL processes using EC2 Spot Instances to lower the compute costs.

Explicación
EC2 Spot Instances can reduce costs, but using AWS Glue's serverless infrastructure is more efficient for this use case and aligns with Glue's managed ETL service.

Use AWS Glue's built-in auto-scaling feature to automatically scale up DPUs based on workload.

Explicación
Glue does not support automatic scaling of DPUs; you need to manually configure the required DPU settings.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 21

A data engineer is tasked with building an ETL pipeline using AWS Glue to process semi-structured data stored in Amazon S3. The pipeline should automatically infer the schema of the input data, perform transformations, and store the output back to S3 in a different format. The process must be efficient and scalable, without the need for infrastructure management. Which of the following AWS Glue features best fulfill this requirement? (Choose Two)

Selección correcta
AWS Glue Crawler

Explicación
AWS Glue Crawlers are used to automatically infer the schema of semi-structured data in S3. They scan the data, detect the structure (such as CSV or Parquet), and populate the Glue Data Catalog with metadata.

Selección correcta
AWS Glue Data Catalog

Explicación
AWS Glue Data Catalog stores the schema and metadata, making the data queryable through services like AWS Athena, Amazon Redshift, and Amazon QuickSight.

AWS Glue Auto Scaling

Explicación
AWS Glue is a serverless, fully-managed service, so manual auto-scaling isn't required. Glue automatically manages resource allocation.

Amazon EC2-based Spark Cluster

Explicación
An EC2-based Spark cluster would require manual management and doesn't align with the serverless nature of AWS Glue.

AWS Glue Triggers

Explicación
AWS Glue Triggers are used to automate the execution of ETL jobs but do not handle schema inference or metadata management.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 22

A machine learning is preparing a large dataset for machine learning model training using AWS Glue and AWS Glue Databrew. The goal is to clean and preprocess the data, including removing duplicates and converting text columns to uppercase. What is the BEST way to achieve this using AWS Glue Databrew?

Respuesta correcta
Use Databrew’s visual interface to apply transformation steps, such as removing duplicates and converting text to uppercase, then save these transformations as a recipe.

Explicación
AWS Glue Databrew provides a no-code, visual interface to apply various data transformations such as removing duplicates and converting text values to uppercase. These transformations can be saved as a recipe, allowing easy reuse and automation for future jobs.

Manually write code to perform the transformations, and run it on an Amazon EC2 instance.

Explicación
Writing custom code and running it on an EC2 instance is unnecessary when Databrew offers a visual, no-code interface designed for these tasks.

Use AWS Lambda functions to apply text transformations and remove duplicates before importing the data into Databrew.

Explicación
Using Lambda functions adds complexity and manual effort compared to Databrew’s built-in transformation tools.

Store the dataset in Amazon RDS and use SQL queries to preprocess the data.

Explicación
While SQL in RDS can handle such transformations, it lacks the flexibility and ease of use offered by Databrew, especially for non-technical users.

Temática
Data Preparation for Machine Learning
Pregunta 23

A data scientist is using AWS Lambda to preprocess incoming JSON data from an IoT device stream. The Lambda function needs to validate the data, enrich it by calling an external API, and forward the processed data to an Amazon Kinesis stream for further machine learning processing. The solution should be cost-effective, minimize latency, and ensure the Lambda function does not exceed its timeout limit during the API call.

Which of the following approaches should the data scientist take?

Use AWS Lambda with asynchronous invocation to call the external API, validate the data, and forward the processed data to Kinesis.

Explicación
Asynchronous invocations in Lambda don't help with external API calls, as the Lambda function might still time out during long-running operations.

Use AWS Lambda with the built-in AWS SDK to call the external API synchronously, validate the data, and send the result to Kinesis.

Explicación
Synchronous API calls inside Lambda are prone to timeouts and increase the risk of the Lambda function not completing its task within the set execution time.

Respuesta correcta
Use AWS Step Functions to manage the workflow, including the API call and data validation, and then use a separate Lambda function to forward data to Kinesis.

Explicación
AWS Step Functions is a better approach for handling long-running tasks, such as external API calls, without exceeding Lambda's timeout limits. Step Functions can orchestrate the workflow and Lambda can still be used to send data to Kinesis after enrichment.

Use Lambda to invoke an AWS Fargate container for handling the external API call, and forward the enriched data to Kinesis from the Lambda function after validation.

Explicación
Using AWS Fargate introduces unnecessary complexity and may not be cost-effective for relatively simple API calls, especially if Step Functions can handle the task.

Temática
ML Model Development
Pregunta 24

A healthcare company needs to ingest and store real-time health monitoring data from wearable devices. The data must be compressed and transformed into a format that supports fast querying in Amazon Athena. They also need to ensure the processed data is securely stored in Amazon S3.

Which of the following solutions would meet the company's requirements using Amazon Kinesis Data Firehose?

Ingest data using Amazon Kinesis Data Streams, transform the data using AWS Lambda, compress it with Gzip, and store it in Amazon S3 in CSV format.

Explicación
Although Amazon Kinesis Data Streams can be used for ingestion, the scenario mentions Firehose. Moreover, storing data in CSV format is less efficient for querying in Athena compared to columnar formats like Parquet.

Respuesta correcta
Ingest data using Amazon Kinesis Data Firehose, compress the data using Gzip, convert it to Parquet format using the built-in data transformation feature, and store it in Amazon S3 with server-side encryption.

Explicación
Amazon Kinesis Data Firehose natively supports compressing data (e.g., Gzip or Snappy) and transforming it into columnar formats like Apache Parquet or ORC. This meets the requirement for fast querying with Amazon Athena, which performs better on columnar formats like Parquet. Additionally, Firehose can write the data to Amazon S3 with server-side encryption for security.

Ingest data using Amazon Kinesis Data Firehose, use a Lambda function for data transformation, compress the data with Snappy, and store it in Amazon Redshift.

Explicación
Although Firehose supports Lambda functions for transformation, the question requires data to be stored in Amazon S3, not Redshift.

Ingest data using Amazon Kinesis Data Streams, compress the data with Zlib, transform it to JSON format, and store it in Amazon S3.

Explicación
Storing data in JSON format is not as optimized for Athena queries as Parquet.

Temática
Data Preparation for Machine Learning
Pregunta 25

A Machine Learning Engineer is preparing to train a deep learning model on Amazon SageMaker using a custom algorithm. The dataset is very large and stored across multiple S3 buckets. Due to the data size, it needs to be streamed directly into SageMaker for training rather than loaded all at once.

What is the recommended approach to efficiently manage and stream this data during training in Amazon SageMaker?

Respuesta correcta
Use SageMaker Pipe mode to stream the data directly from S3 to the training instances.

Explicación
SageMaker’s Pipe mode allows data to be streamed directly from S3 to the training instances without fully loading it into memory. This is optimal for large datasets, as it minimizes memory usage and speeds up the training process.

Use AWS Glue to join the data into a single file before loading it into SageMaker.

Explicación
AWS Glue can be used to preprocess data but would not help with streaming data directly into SageMaker for training. Glue’s primary function here would be for ETL, not real-time streaming.

Manually download and preprocess the data on an Amazon EC2 instance before uploading it to SageMaker.

Explicación
Manually downloading and preprocessing data on an EC2 instance is inefficient and does not leverage SageMaker’s optimized data handling capabilities for large datasets.

Use the SageMaker batch transform feature to prepare the data before training.

Explicación
SageMaker batch transform is for inference, not for streaming data into training. It would not be suitable for directly loading data during training.

Temática
Data Preparation for Machine Learning
Pregunta 26

A data scientist is using Amazon SageMaker to develop a machine learning model for customer behavior prediction. The scientist is working in a SageMaker Notebook Instance and wants to collaborate with a team of data analysts on this project. Additionally, they need to ensure that their work, including data visualizations and intermediate results, is saved and can be accessed even after the notebook instance is stopped.

Which configuration should the data scientist consider to achieve these goals? (Select TWO)

Selección correcta
Use SageMaker Studio to enable collaboration, as it allows multiple users to access shared resources within a Jupyter Lab environment.

Explicación
SageMaker Studio provides a collaborative environment, allowing multiple users to share and access notebooks in a persistent, scalable environment. This is ideal for teams working together on the same project.

Selección correcta
Enable persistent storage in the SageMaker Notebook Instance to automatically save data and code, ensuring that all work is retained even after stopping the instance.

Explicación
SageMaker Notebook Instances offer persistent storage, ensuring that data, code, and results are saved even after the notebook instance is stopped. This eliminates the risk of data loss during development.

Install all necessary libraries manually in the SageMaker Notebook Instance to ensure the team has access to the correct dependencies for their machine learning project.

Explicación
SageMaker comes pre-configured with popular libraries like TensorFlow and PyTorch, reducing the need for manual installation of dependencies.

Use SageMaker notebook instances to share code directly, which allows for real-time collaboration between multiple users.

Explicación
SageMaker notebook instances are standalone and do not support real-time collaboration across multiple users directly. For collaboration, SageMaker Studio should be used.

Configure a custom EC2 instance with an EBS volume for persistent storage, ensuring data is saved when the instance is stopped.

Explicación
While configuring an EC2 instance with EBS provides persistent storage, SageMaker's built-in persistent storage is a simpler and more appropriate solution within the managed notebook environment.

Temática
ML Model Development
Pregunta 27

Machine learning team is tasked with preparing a dataset for a model that will predict customer churn. They decide to use SageMaker Data Wrangler to streamline the data preparation. The dataset contains time-based information, and they want to engineer new features such as "day of the week" from timestamps and assess the importance of these features.
What TWO features of SageMaker Data Wrangler will assist the team in this process? (Choose TWO)

Time series analysis and forecasting.

Explicación
Data Wrangler does not provide built-in time series analysis or forecasting capabilities, though it supports feature extraction from time-based data.

Selección correcta
Feature engineering options to create new features like "day of the week" from timestamps.

Explicación
SageMaker Data Wrangler provides tools for feature engineering, such as extracting information from timestamps (e.g., "day of the week") to create more meaningful features for the model.

Selección correcta
The Quick Model feature to automatically compute feature importance.

Explicación
The Quick Model feature in Data Wrangler can quickly compute the feature importance, allowing the team to assess how valuable the newly engineered features are for predicting customer churn.

Integrating data directly from Amazon RDS for real-time predictions.

Explicación
While Data Wrangler can import data from multiple sources (like S3 and Redshift), real-time integration from Amazon RDS is not part of the workflow for feature engineering or importance calculation.

Data export to Amazon Athena for advanced SQL-based transformations.

Explicación
Exporting to Athena is unnecessary for basic feature extraction and importance analysis since Data Wrangler can handle these tasks directly.

Temática
Data Preparation for Machine Learning
Pregunta 28

You are tasked with training a machine learning model for image classification using Amazon SageMaker's built-in algorithms.
Which of the following steps accurately describe the process of using SageMaker's built-in algorithms?

Set up a custom container, upload the algorithm, and configure infrastructure.

Explicación
There is no need to set up a custom container or upload the algorithm for built-in algorithms since they are pre-packaged by AWS.

Respuesta correcta
Select the algorithm from the SageMaker console, upload your dataset, configure hyperparameters, and launch the training job.

Explicación
SageMaker's built-in algorithms abstract away the complexity of infrastructure management. Users only need to select the algorithm, upload the dataset, set the hyperparameters, and select the compute resources to launch the training job.

Create a custom script for model training, manually configure hyperparameter tuning, and deploy the model on an EC2 instance.

Explicación
Manually configuring scripts and hyperparameter tuning is not necessary when using SageMaker built-in algorithms. Hyperparameter tuning can be done automatically.

Use SageMaker built-in image generation algorithms and skip hyperparameter configuration to deploy a pre-trained model.

Explicación
While image classification is supported, hyperparameters still need to be configured for training. Pre-trained models are available, but they are not part of the built-in algorithm suite.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 29

A machine learning engineer is training a decision tree model and is tasked with tuning the model’s hyperparameters. Which of the following is an example of a hyperparameter that the engineer can configure before training begins?

The weights assigned to each node in the decision tree

Explicación
The weights assigned to nodes are learned during training and are not considered hyperparameters.

Respuesta correcta
The depth of the tree

Explicación
The depth of the tree is a hyperparameter in a decision tree that controls how many levels the tree can have and is configured before training begins. It directly affects how complex the model can become.

The data points used for training

Explicación
The data points used for training are part of the dataset, not a model hyperparameter.

The split criteria for each node learned from the data

Explicación
The split criteria at each node are learned during training based on the data and are not set prior to training.

Temática
ML Model Development
Pregunta 30

A machine learning engineer is working with Amazon SageMaker to train a decision tree model and needs to optimize the hyperparameters. The model's performance needs to be optimized by tuning the depth of the tree and the minimum samples required for a split.

Which of the following describes the difference between hyperparameters and model parameters?

Hyperparameters are learned from the training data, while model parameters are set before training.

Explicación
This is the inverse of the correct explanation; model parameters are learned from data, while hyperparameters are set before training.

Respuesta correcta
Hyperparameters are external configurations set before training begins, while model parameters are learned during training.

Explicación
Hyperparameters are external settings like tree depth and minimum samples for a split that must be defined before training. Model parameters, such as weights in neural networks, are learned during training.

Hyperparameters control the size of the data input, while model parameters control the training speed.

Explicación
This misrepresents both hyperparameters and model parameters.

Model parameters influence how quickly the model learns, while hyperparameters control the training algorithm.

Explicación
While hyperparameters can influence training speed, the explanation is incomplete and incorrect concerning model parameters.

Temática
ML Model Development
Pregunta 31

A machine learning team needs to train a model on a large dataset that can fit into memory, but the training would be too slow on a single machine. Which distributed training method should they use in Amazon SageMaker to speed up the process?

Model parallelism, where the model is split across multiple machines.

Explicación
Model parallelism is used when the model itself is too large to fit on a single machine, not when the dataset is large.

Respuesta correcta
Use data parallelism to split the dataset across several machines, with each machine training a copy of the model.

Explicación
Data parallelism is the most effective method when the dataset is large but can still fit into memory. It splits the dataset across multiple machines, allowing each machine to train the same model on a different part of the data, improving training speed.

Manually manage data distribution and gradient synchronization across multiple machines.

Explicación
Manually managing data distribution and gradient synchronization is unnecessary since SageMaker handles this automatically with data parallelism.

Train the model on a single large instance with more compute power.

Explicación
Training on a single larger instance may still be inefficient for large datasets. Distributing the data across multiple machines is a better solution.

Temática
ML Model Development
Pregunta 32

A media company is developing a machine learning model to categorize a large library of video content. Since manually labeling thousands of videos would be costly, the team wants to reduce labeling costs by automating part of the labeling process while ensuring that complex cases are reviewed by human annotators. They also want the labeling process to improve over time based on the labeled data.

What is the BEST strategy for optimizing labeling costs while ensuring high-quality results?

Respuesta correcta
Use SageMaker Ground Truth’s active learning to automate simple labeling tasks and send more complex labeling tasks to human annotators.

Explicación
SageMaker Ground Truth’s active learning can automate labeling of simpler cases while sending complex tasks to human annotators, effectively balancing cost and accuracy. Over time, the system learns from human input, reducing the need for manual labeling.

Rely solely on human workers to manually label all video content, ensuring maximum accuracy but at a higher cost.

Explicación
Relying solely on human labor would ensure accuracy but incur high costs, which the company aims to avoid.

Use SageMaker Ground Truth to automate all labeling tasks without human involvement to reduce costs.

Explicación
Fully automating labeling without human intervention would reduce costs but likely compromise the quality of the labels, especially for more complex video content.

Use SageMaker Experiments to track labeling performance and automatically adjust labeling strategies based on accuracy metrics.

Explicación
SageMaker Experiments is designed for tracking model configurations and metrics, not for managing data labeling strategies.

Temática
Data Preparation for Machine Learning
Pregunta 33

A data scientist is developing a machine learning model for a healthcare application. During model evaluation, they observe that the model has high bias and is underfitting the data. They decide to implement strategies to address the bias and variance issues to ensure the model generalizes well on new data.

Which combination of strategies would be MOST effective to balance bias and variance and avoid both underfitting and overfitting?

Use early stopping during training to prevent overfitting and add regularization to simplify the model.

Explicación
While early stopping and regularization help in reducing overfitting, these alone do not address underfitting, especially if the data size is insufficient or the model lacks capacity.

Respuesta correcta
Add more training data and implement cross-validation to reduce overfitting and ensure better generalization.

Explicación
Adding more training data helps the model learn from a broader range of examples, reducing both underfitting and overfitting risks. Cross-validation ensures the model generalizes well to new data by testing it on different subsets of the training data.

Implement early stopping and dimensionality reduction to reduce the complexity of the model and prevent underfitting.

Explicación
Early stopping helps prevent overfitting, but dimensionality reduction might lead to loss of important information, which could exacerbate underfitting.

Increase the number of model parameters and reduce the training data size to reduce variance.

Explicación
Increasing model parameters without sufficient data can lead to overfitting, and reducing training data would further increase this risk rather than reducing variance.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 34

A financial company is concerned about bias in its AI models and wants to assess the influence of individual features on model predictions. The company aims to build trust with stakeholders and ensure regulatory compliance by explaining how certain features impact predictions.
Which feature of Amazon SageMaker Clarify would best meet these needs?

Bias Detection

Explicación
Bias detection in SageMaker Clarify helps identify biases in datasets and models but does not explain feature importance.

Respuesta correcta
SHAP (Shapley Additive Explanations) values for feature attribution

Explicación
SHAP values are a method used in SageMaker Clarify for feature attribution, providing explanations on how individual features contribute to model predictions. This method is crucial for explainability, which is necessary for building trust and meeting regulatory compliance.

Data Wrangling

Explicación
Data Wrangling is used to balance and preprocess data, not explain individual model predictions.

Early stopping

Explicación
Early stopping is a training technique to prevent overfitting, unrelated to explaining model predictions or feature importance.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 35

A company has developed a machine learning model using XGBoost and plans to deploy it on edge devices with limited compute resources. The model needs to be optimized for performance without significantly affecting accuracy. The company decides to use SageMaker Neo for this task.

Which of the following steps should the company take to optimize and deploy the model using SageMaker Neo?

Respuesta correcta
Use SageMaker Neo to automatically compile the model for the target hardware and deploy it using AWS IoT Greengrass.

Explicación
SageMaker Neo is designed to compile and optimize models for specific hardware platforms, making it ideal for deploying models on edge devices with limited resources. It integrates with AWS IoT Greengrass for seamless deployment on edge.

Deploy the unoptimized model directly on the edge devices and monitor the performance through SageMaker Debugger.

Explicación
Deploying the unoptimized model directly without using Neo would not benefit from the performance improvements and could result in inefficient inference.

Retrain the model with a simplified architecture to reduce compute demand before deploying on edge devices.

Explicación
Simplifying the model architecture may reduce accuracy, which is not ideal. Neo optimizes performance without sacrificing accuracy significantly.

Use SageMaker Profiler to analyze edge device resource utilization before optimizing the model with SageMaker Neo.

Explicación
Profiler is useful for analyzing resource utilization but does not directly optimize model performance for edge devices.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 36

A machine learning engineer is responsible for deploying a model in Amazon SageMaker. After deployment, the engineer needs to monitor the model to detect data drift, model quality drift, and feature attribution drift over time. What are the most appropriate steps to take to monitor the deployed model effectively?

Respuesta correcta
Implement a baseline job, define constraints based on training data, and schedule monitoring jobs.

Explicación
SageMaker model monitoring allows the engineer to set up a baseline job to define constraints from the training data, followed by scheduling monitoring jobs to track data drift, model quality drift, and feature attribution drift automatically.

Monitor only the model predictions for accuracy and retrain the model when performance declines.

Explicación
While monitoring model predictions is important, this option does not mention setting up monitoring jobs or detecting various types of drift, such as data or feature attribution drift.

Use Amazon S3 to store logs manually and monitor changes in the input data via manual inspection.

Explicación
Manually inspecting logs is not efficient for continuous monitoring. SageMaker model monitoring provides automated detection and alerts.

Perform model re-training without any active monitoring to avoid unnecessary processing costs.

Explicación
Active monitoring is critical to detecting drift early and avoiding model performance degradation. Simply retraining without monitoring can lead to undetected issues in production.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 37

You are deploying a model in production and want to ensure that only the most trusted model versions are used. Which feature of the SageMaker Model Registry will help you achieve this?

Model lineage

Explicación
Model lineage tracks the development history of the model but does not control its deployment.

Respuesta correcta
Model approval status

Explicación
SageMaker Model Registry's model approval status allows you to control which models are approved for deployment. This helps ensure that only trusted and validated models are used in production environments.

Model group

Explicación
A model group is a collection of related model versions, but it does not determine deployment approval.

Model metrics

Explicación
Model metrics store performance-related data, but they don’t directly manage which model is deployed.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 38

Which of the following scenarios best describes the use of supervised learning?

A financial institution segments its customers based on spending patterns without predefined categories.

Explicación
This describes unsupervised learning (clustering) where customer segments are formed without predefined categories or labels.

Respuesta correcta
A company develops a predictive model to classify emails as either spam or not spam using historical labeled data.

Explicación
Supervised learning involves using labeled data to train a model to predict outcomes. In this case, the model uses labeled historical data (spam or not spam) to classify emails, making it a supervised learning task.

A retail business uses machine learning to automatically group products based on their similarity without providing labeled data.

Explicación
This is another example of unsupervised learning, where products are grouped without labeled data, such as customer preferences.

A gaming company implements a system where an AI learns to improve its strategy by playing games and receiving feedback from wins and losses.

Explicación
This describes reinforcement learning, where the AI interacts with an environment and learns through rewards or penalties, not labeled data.

Temática
Data Preparation for Machine Learning
Pregunta 39

A financial firm is using Amazon SageMaker to develop a reinforcement learning (RL) model for optimizing portfolio management. The agent is tasked with adjusting the portfolio by buying, selling, or holding assets in a fluctuating market. To ensure the agent maximizes the long-term return while minimizing risk, how should the reward function be structured?

The reward function should penalize any change in the portfolio and reward only large gains in asset value.

Explicación
Penalizing any portfolio change would discourage exploration, which is essential in RL for finding optimal strategies.

The reward function should reward every transaction made by the agent to encourage exploration of different strategies.

Explicación
Rewarding all transactions without considering performance can lead to erratic and suboptimal behavior, as the agent may focus on activity rather than results.

Respuesta correcta
The reward function should provide small rewards for gains in asset value and larger penalties for significant losses, to encourage both risk management and profit maximization.

Explicación
In a portfolio optimization problem, rewarding gains while penalizing losses encourages the agent to balance risk and reward, promoting sustainable long-term profitability.

The reward function should only reward short-term profit increases, ensuring the agent takes immediate action for portfolio growth.

Explicación
Focusing solely on short-term gains overlooks the importance of long-term portfolio health, leading to riskier strategies.

Temática
ML Model Development
Pregunta 40

A large e-commerce company is using Amazon Fraud Detector to prevent fraudulent account signups. The company wants to automate the detection process using machine learning models and receive fraud scores in real-time. What steps should the company take to implement this solution efficiently? (Choose TWO)

Selección correcta
Ingest historical data such as past account signups and user details to train the Amazon Fraud Detector model

Explicación
Ingesting historical data such as past account signups and user details is essential for training the Amazon Fraud Detector model. By using historical data, the model can learn patterns of fraudulent behavior and improve its accuracy in detecting fraudulent account signups in real-time.

Use Amazon Personalize to generate fraud scores based on user interaction data and account creation activities

Explicación
Using Amazon Personalize for generating fraud scores based on user interaction data and account creation activities is not the correct approach for automating the fraud detection process using machine learning models. Amazon Fraud Detector is specifically designed for fraud detection tasks and should be utilized for this purpose.

Set up Amazon Fraud Detector to automatically extract relevant features, such as user location and device type, for model training

Explicación
Setting up Amazon Fraud Detector to automatically extract relevant features for model training is important, but it is not sufficient on its own to implement the solution efficiently. Ingesting historical data and using the Fraud Detector to assign fraud scores in real-time are crucial steps for automating the fraud detection process effectively.

Deploy AWS Glue to periodically retrain the fraud detection model on newly ingested data

Explicación
Deploying AWS Glue to periodically retrain the fraud detection model on newly ingested data is a good practice for model maintenance and improvement. However, it is not directly related to automating the fraud detection process in real-time, which is the main goal of the company in this scenario.

Selección correcta
Use Amazon Fraud Detector to assign a fraud score in real-time, allowing the company to take immediate actions based on the score

Explicación
Using Amazon Fraud Detector to assign a fraud score in real-time is a key step in automating the fraud detection process efficiently. By receiving fraud scores in real-time, the company can take immediate actions to prevent fraudulent account signups and protect its platform from malicious activities.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 41

A retail company is planning to use Amazon Q Business to automate customer support by integrating with third-party applications like ServiceNow and Zendesk. They want to ensure smooth communication between Amazon Business and these tools.
What feature of Amazon Business enables this integration?

Use IAM Identity Center to control access to ServiceNow and Zendesk.

Explicación
IAM Identity Center is used for managing user access, not for integrating with third-party tools.

Respuesta correcta
Set up plugins to enable interaction with third-party applications.

Explicación
Amazon Business supports plugins, which allow seamless integration with third-party applications like ServiceNow and Zendesk, enabling the AI assistant to interact with these tools for tasks like ticket creation.

Configure pre-built connectors to synchronize all data between Amazon S3 and the third-party tools.

Explicación
Pre-built connectors are used to connect data sources, not specifically for integrating applications like Zendesk or ServiceNow.

Manually code the integration between Amazon Business and ServiceNow or Zendesk.

Explicación
Amazon Business is designed to offer no-code integration through plugins, making manual coding unnecessary.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 42

A financial firm needs to create a scheduled ETL process using AWS Glue that extracts transaction data from Amazon Redshift, transforms it, and loads it into another Redshift cluster for reporting. The process should avoid reprocessing the same data repeatedly and only process newly added transactions.
Which AWS Glue feature will allow the firm to achieve this?

AWS Glue Crawler

Explicación
AWS Glue Crawlers are used to infer the schema and populate the Glue Data Catalog but do not perform incremental processing.

Respuesta correcta
AWS Glue Incremental Processing

Explicación
AWS Glue Incremental Processing allows the ETL jobs to process only the data that has not been processed before, preventing duplication and making the ETL job more efficient.

AWS Glue Triggers

Explicación
AWS Glue Triggers automate job execution but are not responsible for incremental data processing.

AWS Glue Development Endpoint

Explicación
AWS Glue Development Endpoints are for developing and debugging Glue scripts and aren't related to managing incremental data loads.

Temática
Data Preparation for Machine Learning
Pregunta 43

A company is preparing to automate the transformation and loading of log data from an Amazon S3 bucket using AWS Glue. The data should be transformed into a columnar format to enhance query performance and stored back into S3 for subsequent analysis using business intelligence tools. Considering cost optimization and effective data transformation, which TWO of the following strategies should the company implement? (Choose Two)

Configure an AWS Glue crawler to detect and update the data schema, enabling automatic partitioning by adding new partitions as data grows.

Explicación
While configuring a crawler to update schema and partition data is useful, it does not directly influence the transformation into a columnar format, which is the primary need in this scenario.

Directly use Amazon S3 to perform ETL operations on the data, bypassing AWS Glue to minimize costs and avoid complex configurations.

Explicación
Amazon S3 does not have built-in ETL capabilities; it's a storage service, so this approach is technically unfeasible.

Selección correcta
Utilize AWS Glue's built-in capability to transform the data format to Apache Parquet for optimized analytical querying before storing it back to S3.

Explicación
AWS Glue inherently supports converting data into Parquet, a columnar format beneficial for analytics, without manual intervention, which streamlines processing and enhances performance for BI tools.

Implement manual ETL processes that rely on EC2 instances to periodically transform and overwrite data in S3 to ensure data is up-to-date.

Explicación
Using EC2 instances for manual ETL processes introduces higher costs and complexity compared to leveraging AWS Glue's serverless architecture, which is designed specifically for such operations.

Selección correcta
Set up AWS Glue ETL jobs to run on a minimal number of Data Processing Units (DPUs) to effectively balance cost and performance.

Explicación
Running Glue ETL jobs on a minimal number of DPUs is a cost-effective method. It maintains necessary performance for tasks without provisioning unnecessary resources, adhering to the pay-as-you-go model effectively.

Temática
Data Preparation for Machine Learning
Pregunta 44

A media company uses AWS Glue for processing and analyzing streaming video metadata stored in Amazon S3, with a requirement to make the processed data available in a SQL-like queryable format for quick insights. Given the need to regularly update the metadata catalog and the integration with analytics services, which TWO actions should be implemented to optimize the processing workflow in AWS Glue? (Choose Two)

Selección correcta
Utilize AWS Glue crawlers to automatically infer schema from new data and update the Glue Data Catalog, ensuring data remains queryable with updated schema.

Explicación
AWS Glue crawlers automate schema detection and updates to the Data Catalog, critical for maintaining accessibility and queryability of streaming video metadata as it evolves.

Schedule AWS Glue ETL jobs to perform transformations only during off-peak hours to reduce impact on operational costs and resource utilization.

Explicación
While scheduling jobs during off-peak hours can save costs, it does not specifically address the processing or data availability needs directly related to the company’s operational requirements in the question.

Enable AWS Glue Studio to manage ETL jobs and transformations visually, thereby simplifying the modification and deployment of data pipelines.

Explicación
AWS Glue Studio enhances usability but the question focuses on optimizing the data processing workflow in terms of performance and cost, not simplifying job management.

Rely on Amazon Redshift Spectrum to handle the ETL processes directly from the S3 data lake, eliminating the need for AWS Glue.

Explicación
Amazon Redshift Spectrum is useful for querying data in S3, but it does not replace the ETL capabilities of AWS Glue which are needed for preprocessing and cataloging the metadata as described.

Selección correcta
Configure incremental data loads in Glue ETL jobs to ensure only newly added data is processed, thus minimizing compute resources and processing time.

Explicación
Configuring incremental loads significantly reduces unnecessary data processing and compute resource utilization, focusing only on new or changed data, which enhances efficiency and lowers costs.

Temática
Data Preparation for Machine Learning
Pregunta 45

A retail company uses AWS Lambda to process images uploaded to an Amazon S3 bucket. The company wants to enhance the Lambda function to extract metadata from the images (such as resolution and file format), store the metadata in an Amazon DynamoDB table, and trigger a machine learning model hosted on SageMaker to classify the images. Which of the following options is the most proper and scalable for described workflow?

Use an S3 event trigger to invoke the Lambda function, extract metadata, store it in DynamoDB, and directly invoke the SageMaker endpoint for classification within the same Lambda function.

Explicación
Although feasible, combining the metadata extraction and model invocation in a single Lambda function is less modular and may lead to scalability or timeout issues.

Respuesta correcta
Use an S3 event trigger to invoke the Lambda function, extract metadata, and store it in DynamoDB. Then, trigger a separate Lambda function to invoke the SageMaker endpoint for classification.

Explicación
A two-step Lambda invocation (one for metadata extraction and one for image classification) ensures modularity and separates concerns. The first Lambda function can handle metadata extraction and DynamoDB storage, while the second Lambda function focuses on invoking the SageMaker model. This is a scalable and decoupled solution.

Set up an Amazon S3 batch operation to invoke the Lambda function, extract metadata, and invoke the SageMaker endpoint. Use a separate process to store the metadata in DynamoDB after classification.

Explicación
S3 batch operations are not typically used for real-time image processing and invoking SageMaker in this scenario.

Use Amazon EventBridge to monitor S3 uploads, triggering the Lambda function to extract metadata, store it in DynamoDB, and schedule a periodic invocation of SageMaker for classification.

Explicación
EventBridge is unnecessary for triggering a Lambda function based on S3 events. Direct S3 event triggers are more appropriate.

Temática
Data Preparation for Machine Learning
Pregunta 46

A data scientist is training a machine learning model using a large dataset with various features. During the initial evaluation, the model appears to be overfitting. Which combination of techniques can the data scientist apply to reduce overfitting and improve generalization? (Choose TWO)

Selección correcta
Use cross-validation by rotating through training and validation sets.

Explicación
Cross-validation helps in improving generalization by ensuring the model performs well across different subsets of data, reducing the likelihood of overfitting.

Increase the number of parameters in the model to capture more data patterns.

Explicación
Increasing parameters can make the model more complex, which can exacerbate overfitting.

Selección correcta
Apply regularization to penalize extreme model parameters.

Explicación
Regularization penalizes extreme parameters, leading to a simpler model that is less prone to overfitting.

Reduce the amount of training data to limit the complexity of the model.

Explicación
Reducing training data can increase overfitting as the model may not generalize well without sufficient data.

Avoid using dimensionality reduction techniques to retain all feature information.

Explicación
Dimensionality reduction helps to focus on essential features, reducing the risk of overfitting by ignoring irrelevant data patterns.

Temática
ML Model Development
Pregunta 47

A machine learning team is developing a model to predict customer churn. They need to address bias and ensure fairness in their model outputs. Which Amazon SageMaker feature can they use to analyze and mitigate bias during and after the training phase?

Respuesta correcta
SageMaker Clarify

Explicación
SageMaker Clarify is designed specifically for bias detection and mitigation, allowing for both pre-training and post-training bias assessments to ensure fairness and transparency in model predictions.

SageMaker Data Wrangler

Explicación
SageMaker Data Wrangler is primarily used for data preparation and transformation but does not focus on bias detection.

SageMaker Debugger

Explicación
SageMaker Debugger is used for monitoring and debugging model training issues but does not provide bias detection capabilities.

SageMaker Ground Truth

Explicación
SageMaker Ground Truth is used for creating and labeling datasets, but it does not have bias analysis features.

Temática
Data Preparation for Machine Learning
Pregunta 48

A machine learning engineer is using Amazon SageMaker Debugger to monitor a training job. The engineer notices that the model’s validation loss starts to increase while the training loss continues to decrease. What action should the engineer take to address this issue?

Increase the learning rate to improve the model's performance.

Explicación
Increasing the learning rate may lead to unstable training and potentially worsen model performance.

Respuesta correcta
Use early stopping to prevent further overfitting.

Explicación
Early stopping is a technique that stops training when the validation loss increases, which typically indicates overfitting. This can prevent the model from learning noise in the data and help maintain generalization.

Add more features to the model to increase its complexity.

Explicación
Adding more features increases model complexity, which may lead to further overfitting rather than mitigating it.

Reduce the amount of training data to prevent overfitting.

Explicación
Reducing training data is unlikely to help and may reduce the model's ability to generalize well.

Temática
ML Model Development
Pregunta 49

A data scientist is working to improve the generalization of their machine learning model and reduce overfitting. They have been advised to use cross-validation, add more training data, and apply regularization techniques. Which of the following best describes the purpose and impact of using regularization?

Respuesta correcta
Regularization simplifies the model by penalizing extreme parameter values, reducing overfitting risk.

Explicación
Regularization penalizes extreme model parameters, encouraging simpler models and reducing overfitting by preventing the model from fitting noise in the data.

Regularization increases model complexity to improve performance on diverse datasets.

Explicación
Regularization reduces, rather than increases, model complexity.

Regularization rotates through different subsets of data to enhance model generalization.

Explicación
This describes cross-validation, which uses different data subsets to ensure generalization.

Regularization augments data size by adding synthetic samples from minority classes.

Explicación
Increasing data size with synthetic samples is a method of data augmentation, not regularization.

Temática
ML Model Development
Pregunta 50

A machine learning specialist is using SageMaker Clarify to evaluate potential bias in a customer satisfaction prediction model. The data includes sensitive attributes like gender and age. Which of the following actions can SageMaker Clarify perform to detect and address bias in this context?

It will generate new features from the existing data to balance all classes.

Explicación
SageMaker Clarify does not create new features for class balancing; this would be a function of data preparation tools like SageMaker Data Wrangler.

Respuesta correcta
It will run an analysis on specified sensitive attributes and generate a report with bias metrics.

Explicación
SageMaker Clarify can analyze specified sensitive attributes, such as gender and age, and produce a bias report with metrics, helping detect potential bias.

It will optimize the model by re-training it on only the sensitive features to reduce bias.

Explicación
Clarify does not re-train models with only sensitive features. Instead, it provides bias reports and recommendations.

It will apply synthetic data generation to equalize sample sizes across all categories.

Explicación
While synthetic data can address imbalance, it is not a bias detection function of SageMaker Clarify.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 51

A team is training a neural network model in Amazon SageMaker. During training, they notice that the model's performance has plateaued, and suspect a vanishing gradient issue. How can they leverage SageMaker Debugger to diagnose and address this problem?

Respuesta correcta
Set up custom debug rules to capture disappearing gradients and adjust the learning rate as needed.

Explicación
SageMaker Debugger can capture disappearing gradients through debug rules, allowing users to diagnose gradient issues and adjust parameters like the learning rate.

Enable SageMaker Clarify to detect vanishing gradients during training.

Explicación
SageMaker Clarify is focused on bias detection and explainability, not gradient issues.

Use cross-validation to rotate training data, preventing gradient loss.

Explicación
Cross-validation is used for generalization, not to address vanishing gradients.

Increase the training epochs until SageMaker Debugger resolves the issue.

Explicación
Increasing epochs alone won’t resolve vanishing gradient problems; adjustments in model configuration or learning rate are typically needed.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 52

A mobile application requires occasional, unpredictable access to machine learning predictions. The app’s usage fluctuates significantly throughout the day, leading to periods of high traffic and very low usage at off-peak times. Which deployment strategy in Amazon SageMaker would optimize costs while still ensuring scalability during peak periods?

Batch Transform

Explicación
Batch Transform is for batch processing, not for dynamically fluctuating real-time requests.

Real-Time Inference Endpoint

Explicación
Real-Time Inference Endpoints require continuous infrastructure, which is not cost-effective for applications with sporadic usage patterns.

Respuesta correcta
Serverless Inference Endpoint

Explicación
The Serverless Inference Endpoint in Amazon SageMaker is ideal for unpredictable traffic patterns, as it automatically scales the compute resources up or down based on demand. It is cost-effective because it allows users to pay only for the compute resources used during peak periods, saving costs during low-usage times.

Asynchronous Inference Endpoint

Explicación
Asynchronous Inference is suitable for larger, queued workloads and does not provide real-time responses necessary for a mobile application.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 53

An organization is deploying a machine learning model to edge devices with limited computing resources. They want to optimize the model to reduce resource usage without compromising accuracy. Which AWS service should they use to achieve this optimization?

Amazon SageMaker Real-Time Inference Endpoint

Explicación
Real-Time Inference Endpoints are for continuous, real-time predictions rather than model optimization for resource-constrained environments.

Respuesta correcta
Amazon SageMaker Neo

Explicación
Amazon SageMaker Neo is designed to optimize machine learning models for performance and efficiency across different hardware environments, including edge devices. It compiles models specifically for the target hardware, improving execution speed and reducing resource consumption without sacrificing model accuracy.

AWS IoT Greengrass

Explicación
AWS IoT Greengrass enables edge device deployment and management but does not optimize the model itself for resource efficiency.

Amazon SageMaker Batch Transform

Explicación
Batch Transform is intended for batch processing on large datasets and does not offer optimization for edge deployment.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 54

A financial institution wants to deploy a machine learning model that provides fraud detection in real time. Traffic to the model is highly variable, with periods of high activity followed by periods of low or no activity. The institution needs a cost-effective and scalable solution that adjusts to these traffic patterns.

Which deployment option should they choose in Amazon SageMaker?

Real-Time Inference Endpoint

Explicación
Real-time inference requires persistent infrastructure, which may result in unnecessary costs during low-traffic periods.

Batch Transform

Explicación
Batch transform is designed for offline, large-batch processing and does not handle real-time requests.

Respuesta correcta
Serverless Inference

Explicación
Serverless inference is ideal for unpredictable traffic patterns because it auto-scales based on demand and charges only for usage, making it cost-effective during off-peak hours.

Asynchronous Inference

Explicación
Asynchronous inference is used for high-latency tasks or large payloads, not for variable traffic patterns with real-time needs.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 55

A retail company is using Amazon SageMaker for customer recommendations. The company’s model requires low-latency responses to provide real-time product recommendations based on user interactions. They also need to monitor the model to detect data drift and quality drift over time to maintain recommendation accuracy.

Which of the following steps should they take to ensure both low-latency inference and model monitoring? (Choose TWO.)

Selección correcta
Deploy the model using a real-time inference endpoint for low-latency responses.

Explicación
Real-time inference endpoints are designed to provide low-latency responses, which are essential for real-time recommendations.

Use SageMaker Serverless Inference to handle traffic spikes and save costs.

Explicación
Serverless Inference is ideal for unpredictable traffic but may introduce higher latency, which is not suitable for real-time requirements.

Selección correcta
Set up SageMaker Model Monitor to detect data and quality drift on the deployed endpoint.

Explicación
SageMaker Model Monitor can be configured to detect data drift and quality drift, ensuring that the model remains accurate over time.

Use Batch Transform to periodically update recommendations for all users.

Explicación
Batch Transform is for batch processing and not suitable for real-time recommendations.

Enable SageMaker Neo to enhance model performance for edge deployment.

Explicación
SageMaker Neo optimizes models for specific hardware and is useful for edge devices, which is not required in this scenario.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 56

A tech company wants to deploy a machine learning model for processing large video files. The model processes each video file individually and does not require real-time responses. The company wants to minimize costs by only paying for compute when processing files and scale resources based on demand.

Which SageMaker deployment option should they choose?

Real-Time Inference Endpoint

Explicación
Real-time inference is designed for low-latency, real-time tasks and would incur higher costs due to persistent infrastructure.

Batch Transform

Explicación
Batch Transform is suitable for batch processing but lacks support for on-demand scaling and asynchronous request handling.

Serverless Inference

Explicación
Serverless inference handles unpredictable traffic but may not handle large, long-running tasks efficiently.

Respuesta correcta
Asynchronous Inference

Explicación
Asynchronous inference is suited for long-running tasks, such as processing large files, and scales resources based on demand. It also allows auto-scaling to zero when there are no requests, making it cost-effective.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 57

An e-commerce company wants to integrate Amazon Bedrock to enhance its customer support system with a generative AI chatbot. The company expects usage to fluctuate significantly, with peak usage during holiday sales and lower usage during off-peak times. They also want to avoid managing infrastructure manually.
Which Amazon Bedrock feature will best support these requirements?

Foundation models with fine-tuning capabilities

Explicación
Fine-tuning allows customization but does not address the need for scalability with fluctuating traffic.

Provisioned throughput mode for predictable traffic

Explicación
Provisioned throughput mode is better suited for steady, predictable workloads, not fluctuating usage patterns.

Respuesta correcta
Serverless environment with automatic scaling

Explicación
Amazon Bedrock's serverless environment with automatic scaling is ideal for applications with fluctuating traffic patterns, as it allows the service to scale resources automatically based on demand. This feature is well-suited for workloads with unpredictable peaks, such as during holiday sales, without requiring manual infrastructure management.

Knowledge bases to enhance model responses

Explicación
Knowledge bases improve response relevance but do not assist with handling traffic fluctuations.

Temática
Deployment and Orchestration of ML Workflows
Pregunta 58

A financial institution wants to detect fraudulent activities in real time to protect against payment fraud and account takeovers. They plan to use Amazon Fraud Detector for this purpose and need to automatically assess each transaction based on its risk level.
Which step in Amazon Fraud Detector’s workflow directly identifies transactions that are likely to be fraudulent?

Feature engineering to prepare data features

Explicación
Feature engineering prepares data features for training but does not directly detect fraud.

Respuesta correcta
Real-time evaluation using fraud score

Explicación
Amazon Fraud Detector assigns a fraud score to each transaction in real-time, which reflects the likelihood of fraud based on the model's assessment. This score is used to determine whether a transaction is flagged as potentially fraudulent.

Model deployment to a production environment

Explicación
Deploying the model makes it available for real-time predictions but does not involve assessing specific transactions.

Data ingestion of historical transaction data

Explicación
Data ingestion involves loading historical data to train the model but is not part of real-time fraud detection.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 59

A media company wants to generate synthetic data to improve the training of its recommendation model and is considering Amazon Bedrock for this purpose. The company plans to use a foundation model for content generation and requires secure, scalable data handling. Which Amazon Bedrock feature ensures data privacy and compliance during content generation?

Integration with Amazon CloudWatch for monitoring

Explicación
CloudWatch integration assists with monitoring but does not ensure data privacy and regional compliance.

On-demand pricing based on token usage

Explicación
On-demand pricing addresses cost-efficiency but does not involve data handling or privacy measures.

Respuesta correcta
Region-specific data handling and encryption

Explicación
Amazon Bedrock ensures data privacy and compliance by keeping all data within the AWS region where the API is called, along with encryption both at rest and in transit. This feature helps organizations adhere to data privacy requirements during model training and content generation.

Provisioned throughput mode for steady workloads

Explicación
Provisioned throughput mode is for predictable workloads but does not address data security requirements.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 60

An e-commerce company is planning to integrate Amazon Bedrock for customer support and product recommendation. They want a solution that provides personalized product suggestions based on user interactions in real-time. Additionally, they need a secure environment where sensitive user data remains within a specific AWS region.

Which feature of Amazon Bedrock would best address their need for data security?

Knowledge bases

Explicación
Knowledge bases improve the quality of responses but do not specifically address security.

Serverless environment with automated scaling

Explicación
The serverless environment offers automated scaling but does not directly relate to data security.

AWS Identity and Access Management (IAM) integration

Explicación
While IAM can control access, it does not guarantee encryption of data in transit and at rest.

Respuesta correcta
Data encryption in transit and at rest

Explicación
Data encryption in transit and at rest ensures sensitive information is protected and contained within a specified AWS region, addressing the security needs of the company.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 61

An organization has multiple AWS Lambda functions that require specific permissions to access S3 and DynamoDB resources. The team wants to avoid managing permissions for each individual Lambda function and instead use a centralized approach.

Which AWS IAM feature should the team use to manage access permissions effectively for these Lambda functions?

IAM Users

Explicación
IAM Users are for individual users and require management of separate credentials, which is not suitable for Lambda functions.

IAM Groups

Explicación
IAM Groups are used to manage permissions for users, not AWS services.

Respuesta correcta
IAM Roles

Explicación
IAM Roles are ideal for assigning permissions to AWS services, such as Lambda functions, allowing them to access other AWS resources without individual user credentials. The team can define a role with necessary policies and assign it to multiple Lambda functions, streamlining permission management.

Root User Access

Explicación
The root user has full permissions, but using it for routine tasks is a security risk and is not a best practice.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 62

A security-sensitive application requires encryption for all data stored in Amazon S3. The organization also wants complete control over encryption keys, including the ability to audit their usage and set specific rotation policies.

Which type of AWS Key Management Service (KMS) key should they choose?

AWS Managed Key

Explicación
AWS Managed Keys offer auditing but do not allow full control over key policies or rotation settings.

AWS Owned Key

Explicación
AWS Owned Keys are fully managed by AWS, with no user access or control, making them unsuitable for security-sensitive needs.

Respuesta correcta
Customer Managed Key

Explicación
Customer Managed Keys provide full control, including the ability to set policies, enable auditing via CloudTrail, and manage key rotation. This meets the requirement for security-sensitive applications that need detailed key management.

Symmetric Key

Explicación
Symmetric keys can be part of Customer Managed or AWS Managed options but are not a standalone solution for control and auditing.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 63

A company is concerned about potential Distributed Denial of Service (DDoS) attacks on its application, which has been targeted in the past. They require advanced protection, including automatic mitigation for complex DDoS threats and access to AWS’s DDoS Response Team (DRT) for real-time support during incidents.

Which AWS service should they use to meet these requirements?

AWS Shield Standard

Explicación
AWS Shield Standard provides basic DDoS protection, automatically enabled and free, but lacks advanced features and DRT support.

Respuesta correcta
AWS Shield Advanced

Explicación
AWS Shield Advanced provides enhanced DDoS protection with access to the AWS DDoS Response Team (DRT), as well as additional features such as detailed attack diagnostics and mitigation of complex DDoS attacks, which are needed for high-risk applications.

AWS WAF

Explicación
AWS WAF helps in blocking specific web traffic but does not offer comprehensive DDoS protection or DRT support.

Amazon CloudWatch

Explicación
Amazon CloudWatch monitors application performance but does not provide DDoS protection or attack mitigation.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 64

An organization needs to ensure that its AWS resources comply with regulatory standards. They decide to implement AWS Config to monitor and manage the configuration of these resources. The organization also requires automated alerts for non-compliant resources and periodic compliance checks.

Which combination of AWS Config features should they enable to meet these requirements? (Choose TWO)

Configuration Items

Explicación
Configuration Items track changes in resources but do not enforce compliance or trigger evaluations.

Selección correcta
AWS Config Rules with periodic evaluation

Explicación
AWS Config Rules with periodic evaluation can be set to periodically check resources for compliance, allowing the organization to automate regular compliance checks.

Configuration Stream

Explicación
Configuration Stream provides a continuous record of configuration changes but does not evaluate or enforce compliance.

Selección correcta
Custom rules created with AWS Lambda

Explicación
Custom rules with AWS Lambda allow the organization to create specific compliance criteria to trigger alerts for non-compliant resources, enabling automated responses.

Real-time proactive evaluation mode

Explicación
Proactive evaluation mode evaluates resources pre-deployment but does not offer the continuous or periodic checks the organization needs.

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 65

A company wants to use AWS Config to maintain a detailed history of configuration changes across all of its resources for auditing purposes. They need to ensure the history of these changes is available long-term for regulatory requirements.

Which approach will allow them to retain and access this history over an extended period?

Enable configuration items and configure periodic evaluations.

Explicación
While configuration items capture changes, they are not designed for long-term storage.

Respuesta correcta
Use configuration snapshots and store them in Amazon S3.

Explicación
Configuration snapshots capture the state of AWS infrastructure at specific points and can be stored in Amazon S3 for long-term retention, meeting auditing and regulatory requirements.

Enable AWS Config Rules and use CloudTrail to track changes.

Explicación
AWS Config Rules check compliance but do not store a history of configuration changes for long-term audit purposes.

Use a configuration stream and set up an Amazon RDS database for storage.

Explicación
Configuration streams track changes but do not provide storage solutions, and an RDS database is not required for storing AWS Config data.

Temática
ML Solution Monitoring, Maintenance, and Security