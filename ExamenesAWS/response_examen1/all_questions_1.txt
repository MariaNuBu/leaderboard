Pregunta 1

An ML engineer is building a generative AI application on Amazon Bedrock using large language models (LLMs). The engineer needs to understand key generative AI concepts to optimize and customize the model's performance for the application. Consider these generative AI terms followed by the descriptions:

Generative AI terms:

Embedding
Token
Retrieval Augmented Generation (RAG)
Temperature
Prompt
Descriptions: A. A method to combine external knowledge retrieval with generative AI to produce more accurate and relevant responses. B. Numerical representation of text (or data) in a high-dimensional vector space. C. An input provided to a model to guide it to generate an appropriate response or output for the input. D. Units of text (like words, subwords, or characters) processed by an LLM. E. A parameter used to control the randomness of the model's output, influencing whether responses are more creative or deterministic.

Can you match the generative AI term to its correct description?

1-B, 2-D, 3-C

Respuesta correcta
1-B, 2-D, 3-A

1-D, 2-B, 3-A

4-C, 5-E, 3-A

Explicación general
Correct option:

1-B, 2-D, 3-A

RAG is a technique that integrates external data retrieval (e.g., from a knowledge base) with generative AI to provide more contextually accurate and relevant outputs. It is particularly useful when the model lacks specific information or domain knowledge.

An embedding is a numerical representation of text (or data) in a high-dimensional vector space. Embeddings allow models to measure the similarity between words, phrases, or sentences, which is essential for tasks like semantic search or clustering.

Tokens are units of text (like words, subwords, or characters) processed by an LLM.

Incorrect options:

4-C, 5-E, 3-A - Prompt is an input provided to a model to guide it to generate an appropriate response or output for the input. Temperature adjusts the randomness of LLM outputs. A lower temperature makes outputs more deterministic, while a higher temperature increases variability and creativity. So, this option is incorrect.

1-B, 2-D, 3-C - Prompt is an input provided to a model to guide it to generate an appropriate response or output for the input. So, this option is incorrect.

1-D, 2-B, 3-A - This option swaps the definition of token with that of embedding. So, this option is incorrect.

References:

https://docs.aws.amazon.com/bedrock/latest/userguide/key-definitions.html

https://aws.amazon.com/what-is/generative-ai/

Temática
ML Model Development
Pregunta 2

Which benefits might persuade a developer to choose a transparent and explainable machine learning model? (Select two)

They enhance security by concealing model logic

Selección correcta
They facilitate easier debugging and optimization

They require less computational power and storage

Selección correcta
They foster trust and confidence in model predictions

They simplify the integration process with other systems

Explicación general
Correct options:

They facilitate easier debugging and optimization

Transparent models allow developers to understand how inputs are transformed into outputs, making it easier to identify and correct errors or inefficiencies in the model. This capability is crucial for optimizing the model’s performance and ensuring it behaves as expected.

They foster trust and confidence in model predictions

When stakeholders can understand the decision-making process of a model, it builds trust in its predictions. Transparency is key in high-stakes scenarios, such as healthcare or finance, where understanding the rationale behind predictions is critical for acceptance and trust.

Incorrect options:

They require less computational power and storage - The computational and storage requirements of a model depend on its complexity and the amount of data it processes, not necessarily on its transparency. Both transparent and opaque models can vary widely in their resource needs.

They enhance security by concealing model logic - Opaque models, not transparent ones, are typically associated with enhanced security through obscurity. Transparent models, by definition, reveal their internal workings, which can be less secure if the logic itself needs to be protected.

They simplify the integration process with other systems - The ease of integrating a model with other systems is more related to the architecture and compatibility of the model with existing systems rather than its transparency. Transparent models do not inherently simplify integration processes.

References:

https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.html

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-explainability.html

Temática
ML Model Development
Pregunta 3

A global e-commerce company has developed a custom sentiment analysis model using Amazon Comprehend in Account A within the us-west-2 Region. The company now needs to copy the model to Account B, which handles customer service operations, so that the model can be used to analyze customer feedback. The solution must ensure secure and efficient transfer of the model with minimal development effort.

Which solution will meet these requirements?

Leverage AWS Glue Data Catalog to register the model metadata in Account A and enable cross-account sharing with Account B. Use the Comprehend console in Account B to access the model

Manually download the model artifacts from Amazon S3 in Account A and upload them to an S3 bucket in Account B. Use a custom script to recreate the model in Account B

Respuesta correcta
Use the Amazon Comprehend ImportModel API operation in Account B to securely import the sentiment analysis model from Account A by configuring a resource based IAM policy in account A

Use the Amazon Comprehend ImportModel API operation in Account B to securely import the sentiment analysis model from Account A by configuring a resource based IAM policy in account B

Explicación general
Correct option:

Use the Amazon Comprehend ImportModel API operation in Account B to securely import the sentiment analysis model from Account A by configuring a resource based IAM policy in account A

The Amazon Comprehend ImportModel API operation is designed for transferring custom models between accounts. The source model must be in the same AWS Region that you're using when you import. You can't import a model that's in a different Region.

Amazon Comprehend users can copy trained custom models between AWS accounts in a two-step process. First, a user in one AWS account (account A), shares a custom model that's in their account. Then, a user in another AWS account (account B) imports the model into their account. The account B user does not need to train the model, and does not need to copy (or access) the original training data or test data.

To share a custom model in account A, you need to attach a resource based IAM policy in account A. This policy authorizes an entity in account B, such as a user or role, to import the model version into Amazon Comprehend in their AWS account. The account B user must import the model into the same AWS Region as the original model.

 via - https://docs.aws.amazon.com/comprehend/latest/dg/custom-copy-sharing.html#custom-copy-sharing-example-policy

Incorrect options:

Manually download the model artifacts from Amazon S3 in Account A and upload them to an S3 bucket in Account B. Use a custom script to recreate the model in Account B - Amazon Comprehend custom models are managed services, and their artifacts are not directly accessible as files. You cannot export them to S3 for manual transfer. Instead, you must use the Amazon Comprehend APIs for sharing or copying models between accounts.

Leverage AWS Glue Data Catalog to register the model metadata in Account A and enable cross-account sharing with Account B. Use the Comprehend console in Account B to access the model - AWS Glue Data Catalog is used for metadata management in data lakes and ETL workflows, not for transferring Comprehend custom models.

Use the Amazon Comprehend ImportModel API operation in Account B to securely import the sentiment analysis model from Account A by configuring a resource based IAM policy in account B - You need to attach a resource based IAM policy in account A and NOT in account B.

References:

https://docs.aws.amazon.com/comprehend/latest/APIReference/API_ImportModel.html

https://docs.aws.amazon.com/comprehend/latest/dg/custom-copy-sharing.html#custom-copy-sharing-example-policy

https://docs.aws.amazon.com/comprehend/latest/dg/custom-copy.html

Temática
ML Model Development
Pregunta 4

A financial services company is building a customer churn prediction model on AWS. The dataset includes call logs, customer interaction history, and transactional data from an on-premises PostgreSQL database. The call logs and interaction history are stored in Amazon S3, while the PostgreSQL tables remain on-premises. The data science team needs to aggregate and preprocess data from these various sources to ensure it is ready for machine learning model training. They must also resolve challenges such as feature inconsistencies and ensure schema alignment across the data sources.

Which AWS service or feature can efficiently connect and aggregate the data from these sources?

Use Amazon EMR Spark jobs to preprocess and aggregate the data directly from S3 and on-premises sources for ML model training

Respuesta correcta
Use AWS Lake Formation to aggregate and centrally manage the data from S3 and on-premises PostgreSQL for seamless access and integration

Use Amazon SageMaker Data Wrangler to connect, aggregate, and preprocess the data

Use AWS Database Migration Service (DMS) to transfer and preprocess data for ML training

Explicación general
Correct option:

Use AWS Lake Formation to aggregate and centrally manage the data from S3 and on-premises PostgreSQL for seamless access and integration

AWS Lake Formation is specifically designed for aggregating and managing large datasets from various sources, including Amazon S3, databases, and other on-premises or cloud-based sources. It simplifies the process of:

Integrating data from various locations.

Cataloging the data in a centralized data lake.

Managing permissions and security for the aggregated dataset.

AWS Lake Formation supports connecting to on-premises PostgreSQL databases and Amazon S3, making it the best choice for aggregating transaction logs, customer profiles, and database tables. Additionally, the centralized data lake can be used for further analysis and ML training.

 via - https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html

Incorrect options:

Use Amazon SageMaker Data Wrangler to connect, aggregate, and preprocess the data - While SageMaker Data Wrangler is great for data preparation, it does not perform large-scale ETL operations or automate schema inference from on-premises databases. AWS Lake Formation is better suited for aggregating raw data from multiple sources.

Use AWS Database Migration Service (DMS) to transfer and preprocess data for ML training - AWS DMS is used to migrate data from on-premises databases to AWS services, but it does not include preprocessing or schema alignment functionalities needed for ML model training.

Use Amazon EMR Spark jobs to preprocess and aggregate the data directly from S3 and on-premises sources for ML model training - While Amazon EMR with Spark can preprocess and aggregate data, it requires significant manual configuration and management. Compared to AWS Lake Formation, it introduces operational overhead and complexity, which is not ideal for simpler workflows requiring schema inference and automated integration.

References:

https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html

https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 5

You are working as a data scientist at a financial services company tasked with developing a credit risk prediction model. After experimenting with several models, including logistic regression, decision trees, and support vector machines, you find that none of the models individually achieves the desired level of accuracy and robustness. Your goal is to improve overall model performance by combining these models in a way that leverages their strengths while minimizing their weaknesses.

Given the scenario, which of the following approaches is the MOST LIKELY to improve the model’s performance?

Implement boosting by training sequentially different types of models - logistic regression, decision trees, and support vector machines - where each new model corrects the errors of the previous ones

Use a simple voting ensemble, where the final prediction is based on the majority vote from the logistic regression, decision tree, and support vector machine models

Use bagging, where different types of models - logistic regression, decision trees, and support vector machines - are trained on different subsets of the data, and their predictions are averaged to produce the final result

Respuesta correcta
Apply stacking, where the predictions from logistic regression, decision trees, and support vector machines are used as inputs to a meta-model, such as a random forest, to make the final prediction

Explicación general
Correct option:

Apply stacking, where the predictions from logistic regression, decision trees, and support vector machines are used as inputs to a meta-model, such as a random forest, to make the final prediction

 via - https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/

In bagging, data scientists improve the accuracy of weak learners by training several of them at once on multiple datasets. In contrast, boosting trains weak learners one after another.

Stacking involves training a meta-model on the predictions of several base models. This approach can significantly improve performance because the meta-model learns to leverage the strengths of each base model while compensating for their weaknesses.

For the given use case, leveraging a meta-model like a random forest can help capture the relationships between the predictions of logistic regression, decision trees, and support vector machines.

Incorrect options:

Use a simple voting ensemble, where the final prediction is based on the majority vote from the logistic regression, decision tree, and support vector machine models - A voting ensemble is a straightforward way to combine models, and it can improve performance. However, it typically does not capture the complex interactions between models as effectively as stacking.

Implement boosting by training sequentially different types of models - logistic regression, decision trees, and support vector machines - where each new model corrects the errors of the previous ones - Boosting is a powerful technique for improving model performance by training models sequentially, where each model focuses on correcting the errors of the previous one. However, it typically involves the same base model, such as decision trees (e.g., XGBoost), rather than combining different types of models.

Use bagging, where different types of models - logistic regression, decision trees, and support vector machines - are trained on different subsets of the data, and their predictions are averaged to produce the final result - Bagging, like boosting, is effective for reducing variance and improving the stability of models, particularly for high-variance models like decision trees. However, it usually involves training multiple instances of the same model type (e.g., decision trees in random forests) rather than combining different types of models.

References:

https://aws.amazon.com/blogs/machine-learning/efficiently-train-tune-and-deploy-custom-ensembles-using-amazon-sagemaker/

https://aws.amazon.com/what-is/boosting/

Temática
ML Model Development
Pregunta 6

A research organization collects weather data from multiple sensor devices across various locations. The data is stored as CSV files in a central Amazon S3 bucket. The CSV files contain the same schema, with an observation date column to record when each reading was taken. The organization needs to perform ad-hoc queries on the data to filter and analyze based on the observation date.

The solution must enable efficient querying. Which solution will meet this requirement with the LEAST operational overhead?

Respuesta correcta
Use Amazon Athena and run a CTAS (CREATE TABLE AS SELECT) query with partitioning enabled on the observation date column to query and optimize the CSV files based on the observation date column

Use Amazon EMR with Apache Hive to preprocess the CSV files and filter the data using HiveQL

Stream the new CSV files into Amazon Redshift and query the data using SQL filters on the observation date column

Use AWS Glue to create an ETL job that partitions and transforms the CSV files for querying by the observation date and processes the output into a new S3 bucket

Explicación general
Correct option:

Use Amazon Athena and run a CTAS (CREATE TABLE AS SELECT) query with partitioning enabled on the observation date column to query and optimize the CSV files based on the observation date column

Amazon Athena is a serverless query service that allows you to run SQL queries directly on data stored in Amazon S3. Using the CTAS (CREATE TABLE AS SELECT) query:

You can filter the data efficiently based on the observation date column since the data is partitioned.

Athena can write the filtered results back to S3 in optimized formats like Parquet or ORC, which significantly improves query performance and reduces costs.

Athena requires no infrastructure management, making it the most efficient and low-operational solution for querying S3-based CSV data.

Incorrect options:

Use AWS Glue to create an ETL job that partitions and transforms the CSV files for querying by the observation date and processes the output into a new S3 bucket - AWS Glue is suitable for ETL tasks but requires more configuration and operational effort compared to Athena for simple ad-hoc querying. In addition, using a new S3 bucket for writing the output is inefficient.

Use Amazon EMR with Apache Hive to preprocess the CSV files and filter the data using HiveQL - EMR with Hive is capable of handling this task, but it introduces cluster management overhead and is not as cost-effective as Athena for serverless queries.

Stream the new CSV files into Amazon Redshift and query the data using SQL filters on the observation date column - Amazon Redshift is a managed data warehouse that requires data ingestion, schema design, and ongoing management, adding complexity for simple querying tasks.

References:

https://docs.aws.amazon.com/athena/latest/ug/ctas.html

https://docs.aws.amazon.com/athena/latest/ug/ctas-examples.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 7

You are a machine learning engineer at a healthcare company responsible for developing and deploying an end-to-end ML workflow for predicting patient readmission rates. The workflow involves data preprocessing, model training, hyperparameter tuning, and deployment. Additionally, the solution must support regular retraining of the model as new data becomes available, with minimal manual intervention. You need to select the right solution to orchestrate this workflow efficiently while ensuring scalability, reliability, and ease of management.

Given these requirements, which of the following options is the MOST SUITABLE for orchestrating your ML workflow?

Leverage Amazon EC2 instances to manually execute each step of the ML workflow, use Amazon RDS for storing intermediate results, and deploy the model using Amazon SageMaker endpoints

Use AWS Glue for data preprocessing, Amazon SageMaker for model training and tuning, and manually deploy the model to an Amazon EC2 instance for inference

Respuesta correcta
Implement the entire ML workflow using Amazon SageMaker Pipelines, which provides integrated orchestration for data processing, model training, tuning, and deployment

Use AWS Step Functions to define and orchestrate each step of the ML workflow, integrate with SageMaker for model training and deployment, and leverage AWS Lambda for data preprocessing tasks

Explicación general
Correct option:

Implement the entire ML workflow using Amazon SageMaker Pipelines, which provides integrated orchestration for data processing, model training, tuning, and deployment

Amazon SageMaker Pipelines is a purpose-built workflow orchestration service to automate machine learning (ML) development. SageMaker Pipelines is specifically designed to orchestrate end-to-end ML workflows, integrating data processing, model training, hyperparameter tuning, and deployment in a seamless manner. It provides built-in versioning, lineage tracking, and support for continuous integration and delivery (CI/CD), making it the best choice for this use case.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html

Incorrect options:

Use AWS Step Functions to define and orchestrate each step of the ML workflow, integrate with SageMaker for model training and deployment, and leverage AWS Lambda for data preprocessing tasks - AWS Step Functions is a powerful service for orchestrating workflows, and it can integrate with SageMaker and Lambda. However, using Step Functions for the entire ML workflow adds complexity since it requires coordinating multiple services, whereas SageMaker Pipelines provides a more seamless, integrated solution for ML-specific workflows.

Leverage Amazon EC2 instances to manually execute each step of the ML workflow, use Amazon RDS for storing intermediate results, and deploy the model using Amazon SageMaker endpoints - Manually managing each step of the ML workflow using EC2 instances and RDS is labor-intensive, prone to errors, and not scalable. It also lacks the automation and orchestration capabilities needed for a robust ML workflow.

Use AWS Glue for data preprocessing, Amazon SageMaker for model training and tuning, and manually deploy the model to an Amazon EC2 instance for inference - While using AWS Glue for data preprocessing and SageMaker for training is possible, manually deploying the model on EC2 lacks the orchestration and management features provided by SageMaker Pipelines. This approach also misses out on the integrated tracking, automation, and scalability features offered by SageMaker Pipelines.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 8

A healthcare company is deploying an ML model to predict patient readmission rates using Amazon SageMaker. The company’s ML engineer is setting up a CI/CD pipeline using AWS CodePipeline to automate the model retraining and deployment process. The pipeline must automatically trigger when new training data is uploaded to an Amazon S3 bucket. The goal is to retrain the model and deploy it for real-time inference. Given this context, consider the following steps:

The pipeline deploys the model version in SageMaker Model Monitor for real-time inferences.

A new data upload triggers the pipeline via an Amazon S3 event notification.

The pipeline deploys the retrained model to a SageMaker endpoint for real-time predictions.

Amazon SageMaker retrains the model using updated data stored in the S3 bucket.

An S3 Lifecycle rule attempts to start the pipeline when fresh data arrives.

Which three steps should be selected and ordered correctly to configure the pipeline?

5,4,1

2,4,1

5,4,3

Respuesta correcta
2,4,3

Explicación general
Correct option:

2,4,3

A new data upload triggers the pipeline via an Amazon S3 event notification.
Amazon S3 event notifications can be configured to automatically trigger the CI/CD pipeline in AWS CodePipeline when new training data is uploaded. This ensures that the pipeline starts automatically without manual intervention.

Amazon SageMaker retrains the model using updated data stored in the S3 bucket.
After the pipeline is triggered, Amazon SageMaker retrains the model using the updated training data in the S3 bucket. This step is essential for ensuring the model reflects the most recent data.

The pipeline deploys the retrained model to a SageMaker endpoint for real-time predictions.
Once the model is retrained, the CI/CD pipeline automatically deploys the updated model to a SageMaker endpoint for real-time inference.

Incorrect options:

2,4,1

5,4,1

SageMaker Model Monitor is designed to detect drift and monitor model quality after deployment. You cannot deploy a model to SageMaker Model Monitor. Rather, deploying a model to a SageMaker endpoint ensures it serves real-time predictions, aligning with the question's deployment requirement. So, both these options are incorrect.

5,4,3 - S3 Lifecycle rules are used to transition or expire objects based on predefined policies. They cannot invoke pipelines or trigger actions when data is uploaded. So, this option is incorrect.

Reference:

https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 9

A company stores its training datasets on Amazon S3 in the form of tabular data running into millions of rows. The company needs to prepare this data for Machine Learning jobs. The data preparation involves data selection, cleansing, exploration, and visualization using a single visual interface.

Which Amazon SageMaker service is the best fit for this requirement?

Amazon SageMaker Feature Store

Amazon SageMaker Clarify

Respuesta correcta
Amazon SageMaker Data Wrangler

SageMaker Model Dashboard

Explicación general
Correct option:

Amazon SageMaker Data Wrangler

Amazon SageMaker Data Wrangler reduces the time it takes to aggregate and prepare tabular and image data for ML from weeks to minutes. With SageMaker Data Wrangler, you can simplify the process of data preparation and feature engineering, and complete each step of the data preparation workflow (including data selection, cleansing, exploration, visualization, and processing at scale) from a single visual interface. You can use SQL to select the data that you want from various data sources and import it quickly. Next, you can use the data quality and insights report to automatically verify data quality and detect anomalies, such as duplicate rows and target leakage. SageMaker Data Wrangler contains over 300 built-in data transformations, so you can quickly transform data without writing code.

With the SageMaker Data Wrangler data selection tool, you can quickly access and select your tabular and image data from various popular sources - such as Amazon Simple Storage Service (Amazon S3), Amazon Athena, Amazon Redshift, AWS Lake Formation, Snowflake, and Databricks - and over 50 other third-party sources - such as Salesforce, SAP, Facebook Ads, and Google Analytics. You can also write queries for data sources using SQL and import data directly into SageMaker from various file formats, such as CSV, Parquet, JSON, and database tables.

How Data Wrangler works:  via - https://aws.amazon.com/sagemaker/data-wrangler/

Incorrect options:

SageMaker Model Dashboard - Amazon SageMaker Model Dashboard is a centralized portal, accessible from the SageMaker console, where you can view, search, and explore all of the models in your account. You can track which models are deployed for inference and if they are used in batch transform jobs or hosted on endpoints.

Amazon SageMaker Clarify - SageMaker Clarify helps identify potential bias during data preparation without writing code. You specify input features, such as gender or age, and SageMaker Clarify runs an analysis job to detect potential bias in those features.

Amazon SageMaker Feature Store - Amazon SageMaker Feature Store is a fully managed, purpose-built repository to store, share, and manage features for machine learning (ML) models. Features are inputs to ML models used during training and inference.

Reference:

https://aws.amazon.com/sagemaker/data-wrangler/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 10

You are working on a machine learning project for a financial services company, developing a model to predict credit risk. After deploying the initial version of the model using Amazon SageMaker, you find that its performance, measured by the AUC (Area Under the Curve), is not meeting the company’s accuracy requirements. Your team has gathered more data and believes that the model can be further optimized.

You are considering various methods to improve the model’s performance, including feature engineering, hyperparameter tuning, and trying different algorithms. However, given the limited time and computational resources, you need to prioritize the most impactful strategies.

Which of the following approaches are the MOST LIKELY to lead to a significant improvement in model performance? (Select two)

Selección correcta
Focus on feature engineering by creating domain-specific features and use SageMaker Clarify to evaluate feature importance

Switch to a more complex algorithm, such as deep learning, and use transfer learning to leverage pre-trained models

Increase the size of the training dataset by incorporating synthetic data and then retrain the existing model

Perform hyperparameter tuning using Bayesian optimization and increase the number of trials to explore a broader search space

Selección correcta
Use Amazon SageMaker Debugger to debug and improve model performance by addressing underlying problems such as overfitting, saturated activation functions, and vanishing gradients

Explicación general
Correct options:

Focus on feature engineering by creating domain-specific features and use SageMaker Clarify to evaluate feature importance

Feature engineering is one of the most effective ways to boost model performance, particularly in domain-specific applications like credit risk modeling. By creating more informative features, you can provide the model with better signals for prediction. SageMaker Clarify can be used to evaluate feature importance, helping you identify the most impactful features and further refine the model.

 via - https://aws.amazon.com/sagemaker/clarify/

Use Amazon SageMaker Debugger to debug and improve model performance by addressing underlying problems such as overfitting, saturated activation functions, and vanishing gradients

A machine learning (ML) training job can have problems such as overfitting, saturated activation functions, and vanishing gradients, which can compromise model performance.

SageMaker Debugger provides tools to debug training jobs and resolve such problems to improve the performance of your model. Debugger also offers tools to send alerts when training anomalies are found, take actions against the problems, and identify the root cause of them by visualizing collected metrics and tensors.

SageMaker Debugger:

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html

Incorrect options:

Increase the size of the training dataset by incorporating synthetic data and then retrain the existing model - Increasing the size of the dataset with synthetic data can improve model performance, but it also introduces the risk of adding noise or bias if the synthetic data is not carefully generated. This approach may not guarantee a significant performance boost unless the original dataset was severely lacking in size.

Switch to a more complex algorithm, such as deep learning, and use transfer learning to leverage pre-trained models - Switching to a more complex algorithm or using transfer learning could improve performance, but it also increases the risk of overfitting, especially if the new algorithm is not well suited to the data. Additionally, deep learning models require more data and tuning, which may not be feasible given the time and resource constraints.

Perform hyperparameter tuning using Bayesian optimization and increase the number of trials to explore a broader search space - Hyperparameter tuning, especially using Bayesian optimization, can help optimize the model’s performance, but the gains might be marginal if the underlying features are not informative. It’s a valuable approach, but may not be the most impactful first step.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html

https://aws.amazon.com/sagemaker/clarify/

Temática
ML Model Development
Pregunta 11

You are tasked with building a predictive model for customer lifetime value (CLV) using Amazon SageMaker. Given the complexity of the model, it’s crucial to optimize hyperparameters to achieve the best possible performance. You decide to use SageMaker’s automatic model tuning (hyperparameter optimization) with Random Search strategy to fine-tune the model. You have a large dataset, and the tuning job involves several hyperparameters, including the learning rate, batch size, and dropout rate.

During the tuning process, you observe that some of the trials are not converging effectively, and the results are not as expected. You suspect that the hyperparameter ranges or the strategy you are using may need adjustment.

Which of the following approaches is MOST LIKELY to improve the effectiveness of the hyperparameter tuning process?

Use the Grid Search strategy with a wide range for all hyperparameters and increase the number of total trials

Increase the number of hyperparameters being tuned and widen the range for all hyperparameters

Respuesta correcta
Switch from the Random Search strategy to the Bayesian Optimization strategy and narrow the range of critical hyperparameters

Decrease the number of total trials but increase the number of parallel jobs to speed up the tuning process

Explicación general
Correct option:

Switch from the Random Search strategy to the Bayesian Optimization strategy and narrow the range of critical hyperparameters

When you’re training machine learning models, each dataset and model needs a different set of hyperparameters, which are a kind of variable. The only way to determine these is through multiple experiments, where you pick a set of hyperparameters and run them through your model. This is called hyperparameter tuning. In essence, you're training your model sequentially with different sets of hyperparameters. This process can be manual, or you can pick one of several automated hyperparameter tuning methods.

Bayesian Optimization is a technique based on Bayes’ theorem, which describes the probability of an event occurring related to current knowledge. When this is applied to hyperparameter optimization, the algorithm builds a probabilistic model from a set of hyperparameters that optimizes a specific metric. It uses regression analysis to iteratively choose the best set of hyperparameters.

Random Search selects groups of hyperparameters randomly on each iteration. It works well when a relatively small number of the hyperparameters primarily determine the model outcome.

Bayesian Optimization is more efficient than Random Search for hyperparameter tuning, especially when dealing with complex models and large hyperparameter spaces. It learns from previous trials to predict the best set of hyperparameters, thus focusing the search more effectively. Narrowing the range of critical hyperparameters can further improve the chances of finding the optimal values, leading to better model convergence and performance.

How hyperparameter tuning with Amazon SageMaker works:

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html

Incorrect options:

Increase the number of hyperparameters being tuned and widen the range for all hyperparameters - Increasing the number of hyperparameters and widening the range without any strategic approach can lead to a more extensive search space, which could cause the tuning process to become inefficient and less likely to converge on optimal values.

Decrease the number of total trials but increase the number of parallel jobs to speed up the tuning process - Reducing the total number of trials might speed up the tuning process, but it also reduces the chances of finding the best hyperparameters, especially if the model is complex. Increasing parallel jobs can improve throughput but doesn't necessarily enhance the quality of the search.

Use the Grid Search strategy with a wide range for all hyperparameters and increase the number of total trials - Grid Search works well, but it’s relatively tedious and computationally intensive, especially with large numbers of hyperparameters. It is less efficient than Bayesian Optimization for complex models. A wide range of hyperparameters without focus would result in more trials, but it is not guaranteed to find the best values, especially with a larger search space.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html

https://aws.amazon.com/what-is/hyperparameter-tuning/

https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html

Temática
ML Model Development
Pregunta 12

A retail company is developing a customer churn prediction model on AWS to identify customers likely to cancel their subscriptions. The training dataset includes customer purchase history, support interaction logs, and subscription data. The purchase history and support logs are stored in Amazon S3, while the subscription data resides in an on-premises PostgreSQL database.

The dataset has two major challenges:

A class imbalance where very few customers are labeled as "churned", impacting the model’s learning.

There are strong feature interdependencies among categorical features (e.g., "membership tier") and numerical features (e.g., "purchase frequency").

The ML engineer needs to select an Amazon SageMaker built-in algorithm to train the model and address the challenges with the least operational effort. Which algorithm should the ML engineer use to meet these requirements?

Amazon SageMaker K-Means

Respuesta correcta
Amazon SageMaker Linear Learner

Amazon SageMaker Random Cut Forest (RCF)

Amazon SageMaker Neural Topic Model (NTM)

Explicación general
Correct option:

Amazon SageMaker Linear Learner

Amazon SageMaker Linear Learner is ideal for supervised learning tasks like binary classification (e.g., churn prediction). It is specifically designed to handle class imbalance by adjusting class weights. Linear Learner ensures that the minority class (churned customers) is adequately represented during training. It minimizes operational effort as Linear Learner is straightforward to use, optimized for AWS, and requires less hyperparameter tuning compared to other complex algorithms.

Incorrect options:

Amazon SageMaker K-Means - K-Means is an unsupervised clustering algorithm that identifies groups but does not solve supervised problems like predicting churn.

Amazon SageMaker Random Cut Forest (RCF) - RCF is an unsupervised anomaly detection algorithm, not suitable for supervised churn classification tasks.

Amazon SageMaker Neural Topic Model (NTM) - Amazon SageMaker AI NTM is an unsupervised learning algorithm that is used to organize a corpus of documents into topics that contain word groupings based on their statistical distribution. It is not suitable for supervised churn classification tasks.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html

https://community.aws/content/2eux4F6yvezbPPZFWnQbdHF9HwC/linear-learner-in-sagemaker-hyperparameter-tuning?lang=en

Temática
ML Model Development
Pregunta 13

A data scientist is working for a real estate analytics company to build an ML model that predicts the rental prices of commercial office spaces. The dataset has several features but the data scientist is particularly interested in the following features - Building Type and Year Constructed (building_type_year), City Name (city) and Building Size in square meters (building_size). The data has inconsistencies and requires preprocessing before model training. The data scientist plans to use the following feature engineering techniques to transform the data:

Feature splitting

Binning

One-hot encoding

Standardization

How would you match the given features with the most relevant feature engineering technique?

building_type_year - feature splitting, city - One-hot encoding, building_size - binning

building_type_year - feature splitting, city - binning, building_size - standardization

Respuesta correcta
building_type_year - feature splitting, city - One-hot encoding, building_size - standardization

building_type_year - standardization, city - binning, building_size - One-hot encoding

Explicación general
Correct option:

building_type_year - feature splitting, city - One-hot encoding, building_size - standardization

Feature splitting breaks a compound feature (e.g., "Office_2010") into separate features such as "Type" and "Year." This technique improves model interpretability and performance by treating each sub-feature independently. So, this is the correct technique for the feature building_type_year.

One-hot encoding converts categorical data (e.g., 'city') into binary numerical columns. It allows ML models to process categorical variables effectively. So, this is the correct technique for the feature city.

 via - https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.ONE_HOT_ENCODING.html

Standardizing numerical features scales them to have a mean of 0 and a standard deviation of 1. This technique ensures that numerical features with different units or scales contribute equally to the model. So, this is the correct technique for the feature building_size.

Incorrect options:

building_type_year - feature splitting, city - One-hot encoding, building_size - binning - Binning involves grouping continuous numerical values into bins or ranges. While it is useful for segmenting data, it does not apply to the building_size feature for the given use case.

building_type_year - feature splitting, city - binning, building_size - standardization - Binning involves grouping continuous numerical values into bins or ranges. While it is useful for segmenting data, it does not apply to the city feature for the given use case.

building_type_year - standardization, city - binning, building_size - One-hot encoding - Binning involves grouping continuous numerical values into bins or ranges. While it is useful for segmenting data, it does not apply to the city feature for the given use case.

References:

https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/data-preprocessing.html

https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.ONE_HOT_ENCODING.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 14

A healthcare company has developed multiple machine learning (ML) models for predicting patient outcomes. These models are stored in Amazon Elastic Container Registry (Amazon ECR) repositories across different AWS accounts where they were trained. The company needs to create a centralized catalog of these models to streamline version management, deployment tracking, and governance compliance. The catalog must provide cross-account access, allowing each AWS account to interact with the central repository securely. The solution should also enable the company to organize models logically and manage the lifecycle of different model versions efficiently.

Which solution will meet these requirements?

Respuesta correcta
Use SageMaker Model Registry to create model groups for organizing models and managing versions. Configure cross-account access by attaching a resource-based policy to the SageMaker Model Registry that grants permissions to other AWS accounts

Set up an Amazon Redshift cluster to store metadata from ECR repositories across accounts and use SQL queries to track and catalog models centrally. Configure cross-account access to Redshift via federated roles

Use Amazon ECR to replicate model images across accounts and configure cross-account access using IAM roles. Track metadata and deployment status using custom tagging

Create a central Amazon S3 bucket to store exported model artifacts from each account, enable cross-account access via bucket policies, and manually maintain a versioning structure for compliance

Explicación general
Correct option:

Use SageMaker Model Registry to create model groups for organizing models and managing versions. Configure cross-account access by attaching a resource-based policy to the SageMaker Model Registry that grants permissions to other AWS accounts

SageMaker Model Registry enables centralized management of ML models, including organizing models into model groups, tracking versions, and enforcing governance policies. By using a resource-based policy, the company can grant cross-account access. A resource-based policy is attached directly to the SageMaker Model Registry resource to define access permissions for specific AWS accounts or organizational units (OUs). This approach ensures secure cross-account access without additional setup of resource shares. It also provides granular permission control for actions like registering, accessing, and deploying models.

You should note that Amazon SageMaker Model Registry also integrates with AWS Resource Access Manager (AWS RAM), making it easier to securely share and discover machine learning (ML) models across your AWS accounts.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-version.html#model-registry-version-xaccount

Incorrect options:

Use Amazon ECR to replicate model images across accounts and configure cross-account access using IAM roles. Track metadata and deployment status using custom tagging - While ECR supports cross-account replication, it lacks tools for managing model lifecycle features such as versioning, deployment tracking, or governance. Manual tagging adds operational overhead and is error-prone.

Create a central Amazon S3 bucket to store exported model artifacts from each account, enable cross-account access via bucket policies, and manually maintain a versioning structure for compliance - Amazon S3 can store models but does not provide tools for tracking versions, monitoring deployment status, or enforcing governance. Manually managing compliance using bucket policies and versioning is inefficient.

Set up an Amazon Redshift cluster to store metadata from ECR repositories across accounts and use SQL queries to track and catalog models centrally. Configure cross-account access to Redshift via federated roles - Redshift is a data warehouse designed for analytical queries, not for managing ML models or their lifecycle. It lacks support for logical organization (model groups), version tracking, and governance.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-version.html#model-registry-version-xaccount

https://aws.amazon.com/about-aws/whats-new/2024/06/amazon-sagemaker-model-registry-cross-account-ml-model-sharing/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 15

You are a data scientist at a financial technology company developing a fraud detection system. The system needs to identify fraudulent transactions in real-time based on patterns in transaction data, including amounts, locations, times, and account histories. The dataset is large and highly imbalanced, with only a small percentage of transactions labeled as fraudulent. Your team has access to Amazon SageMaker and is considering various built-in algorithms to build the model.

Given the need for both high accuracy and the ability to handle imbalanced data, which SageMaker built-in algorithm is the MOST SUITABLE for this use case?

Select the Random Cut Forest (RCF) algorithm for its ability to detect anomalies in transaction data

Use the Linear Learner algorithm with weighted classification to address the class imbalance

Implement the K-Nearest Neighbors (k-NN) algorithm to classify transactions based on similarity to known fraudulent cases

Respuesta correcta
Apply the XGBoost algorithm with a custom objective function to optimize for precision and recall

Explicación general
Correct option:

Apply the XGBoost algorithm with a custom objective function to optimize for precision and recall

The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:

Its robust handling of a variety of data types, relationships, distributions.

The variety of hyperparameters that you can fine-tune.

XGBoost is a powerful gradient boosting algorithm that excels in structured data problems, such as fraud detection. It allows for custom objective functions, making it highly suitable for optimizing precision and recall, which are critical in imbalanced datasets. Additionally, XGBoost has built-in techniques for handling class imbalance, such as scale_pos_weight.

Incorrect options:

Use the Linear Learner algorithm with weighted classification to address the class imbalance - The Linear Learner algorithm can handle classification tasks, and weighting classes can help with imbalance. However, it may not be as effective in capturing complex patterns in the data as more sophisticated algorithms like XGBoost.

Select the Random Cut Forest (RCF) algorithm for its ability to detect anomalies in transaction data - Random Cut Forest (RCF) is designed for anomaly detection, which can be relevant for fraud detection. However, RCF is unsupervised and may not leverage the labeled data effectively, leading to suboptimal results in a supervised classification task like this.

Implement the K-Nearest Neighbors (k-NN) algorithm to classify transactions based on similarity to known fraudulent cases - K-Nearest Neighbors (k-NN) can classify based on similarity, but it does not scale well with large datasets and may struggle with the high-dimensional, imbalanced nature of the data in this context.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html

https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html

https://aws.amazon.com/blogs/gametech/fraud-detection-for-games-using-machine-learning/

https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Build_a_fraud_detection_system_with_Amazon_SageMaker_AIM359-R1.pdf

Temática
ML Model Development
Pregunta 16

You are a machine learning engineer at a fintech company tasked with developing and deploying an end-to-end machine learning workflow for fraud detection. The workflow involves multiple steps, including data extraction, preprocessing, feature engineering, model training, hyperparameter tuning, and deployment. The company requires the solution to be scalable, support complex dependencies between tasks, and provide robust monitoring and versioning capabilities. Additionally, the workflow needs to integrate seamlessly with existing AWS services.

Which deployment orchestrator is the MOST SUITABLE for managing and automating your ML workflow?

Use Apache Airflow to define and manage the workflow with custom DAGs (Directed Acyclic Graphs), integrating with AWS services through operators and hooks

Use AWS Lambda functions to manually trigger each step of the ML workflow, enabling flexible execution without needing a predefined orchestration tool

Use AWS Step Functions to build a serverless workflow that integrates with SageMaker for model training and deployment, ensuring scalability and fault tolerance

Respuesta correcta
Use Amazon SageMaker Pipelines to orchestrate the entire ML workflow, leveraging its built-in integration with SageMaker features like training, tuning, and deployment

Explicación general
Correct option:

Use Amazon SageMaker Pipelines to orchestrate the entire ML workflow, leveraging its built-in integration with SageMaker features like training, tuning, and deployment

Amazon SageMaker Pipelines is a purpose-built workflow orchestration service to automate machine learning (ML) development.

SageMaker Pipelines is specifically designed for orchestrating ML workflows. It provides native integration with SageMaker features like model training, tuning, and deployment. It also supports versioning, lineage tracking, and automatic execution of workflows, making it the ideal choice for managing end-to-end ML workflows in AWS.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html

Incorrect options:

Use Apache Airflow to define and manage the workflow with custom DAGs (Directed Acyclic Graphs), integrating with AWS services through operators and hooks - Apache Airflow is a powerful orchestration tool that allows you to define complex workflows using custom DAGs. However, it requires significant setup and maintenance, and while it can integrate with AWS services, it does not provide the seamless, built-in integration with SageMaker that SageMaker Pipelines offers.

Amazon Managed Workflows for Apache Airflow (Amazon MWAA):

 via - https://aws.amazon.com/managed-workflows-for-apache-airflow/

Use AWS Step Functions to build a serverless workflow that integrates with SageMaker for model training and deployment, ensuring scalability and fault tolerance - AWS Step Functions is a serverless orchestration service that can integrate with SageMaker and other AWS services. However, it is more general-purpose and lacks some of the ML-specific features, such as model lineage tracking and hyperparameter tuning, that are built into SageMaker Pipelines.

Use AWS Lambda functions to manually trigger each step of the ML workflow, enabling flexible execution without needing a predefined orchestration tool - AWS Lambda is useful for triggering specific tasks, but manually managing each step of a complex ML workflow without a comprehensive orchestration tool is not scalable or maintainable. It does not provide the task dependency management, monitoring, and versioning required for an end-to-end ML workflow.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html

https://aws.amazon.com/managed-workflows-for-apache-airflow/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 17

You are a data scientist working on a deep learning model to classify medical images for disease detection. The model initially shows high accuracy on the training data but performs poorly on the validation set, indicating signs of overfitting. The dataset is limited in size, and the model is complex, with many parameters. To improve generalization and reduce overfitting, you need to implement appropriate techniques while balancing model complexity and performance.

Given these challenges, which combination of techniques is the MOST LIKELY to help prevent overfitting and improve the model’s performance on unseen data?

Apply early stopping to halt training when the validation loss stops improving, and use dropout as a regularization technique to prevent the model from becoming too reliant on specific neurons

Prune the model by removing less important layers and nodes, and use L2 regularization to reduce the magnitude of the model’s weights, preventing overfitting

Use ensembling by combining multiple versions of the same model trained with different random seeds, and apply data augmentation to artificially increase the size of the dataset

Respuesta correcta
Combine data augmentation to increase the diversity of the training data with early stopping to prevent overfitting, and use ensembling to average predictions from multiple models

Explicación general
Correct option:

Combine data augmentation to increase the diversity of the training data with early stopping to prevent overfitting, and use ensembling to average predictions from multiple models

 via - https://aws.amazon.com/what-is/overfitting/

This option combines data augmentation to artificially expand the training dataset, which is crucial when data is limited, with early stopping to prevent the model from overtraining. Additionally, ensembling helps improve generalization by averaging predictions from multiple models, reducing the likelihood that overfitting in any single model will dominate the final prediction. This combination addresses both data limitations and model overfitting effectively.

Incorrect options:

Apply early stopping to halt training when the validation loss stops improving, and use dropout as a regularization technique to prevent the model from becoming too reliant on specific neurons - Dropout is a form of regularization used in neural networks that reduces overfitting by trimming codependent neurons. Early stopping and dropout are effective techniques for preventing overfitting, particularly in deep learning models. However, while they can help, they may not be sufficient alone, especially when dealing with limited data. Combining these techniques with others, such as data augmentation or ensembling, would provide a more robust solution.

Use ensembling by combining multiple versions of the same model trained with different random seeds, and apply data augmentation to artificially increase the size of the dataset - Ensembling and data augmentation are powerful techniques, but ensembling by combining multiple versions of the same model trained with different random seeds might not provide significant diversity in predictions. A combination of diverse models or more comprehensive techniques might be more effective.

Prune the model by removing less important layers and nodes, and use L2 regularization to reduce the magnitude of the model’s weights, preventing overfitting - Regularization helps prevent linear models from overfitting training data examples by penalizing extreme weight values. L1 regularization reduces the number of features used in the model by pushing the weight of features that would otherwise have very small weights to zero. L1 regularization produces sparse models and reduces the amount of noise in the model. L2 regularization results in smaller overall weight values, which stabilizes the weights when there is high correlation between the features. Pruning and L2 regularization are useful for reducing model complexity and preventing overfitting. However, pruning can sometimes lead to underfitting if not done carefully, and using these techniques alone might not fully address the overfitting issue, especially with limited data.

References:

https://aws.amazon.com/what-is/overfitting/

https://docs.aws.amazon.com/sagemaker/latest/dg/object2vec-hyperparameters.html

https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters.html

Temática
ML Model Development
Pregunta 18

A pharmaceutical company is using Amazon SageMaker to develop machine learning models for drug discovery. The data scientists need a solution that provides granular control over their ML pipelines to manage the steps involved in preclinical testing simulations. They also require the ability to visualize workflows for experiments as a directed acyclic graph (DAG) to better understand dependencies. In addition, the solution must allow them to maintain a history of experiment trials for reproducibility and optimization, as well as tools to implement model governance for regulatory audits and compliance verifications.

Which solution will meet these requirements?

Use AWS CodePipeline for orchestration, visualize workflows as a series of stages, and implement SageMaker Experiments to maintain a running history of model discovery experiments

Respuesta correcta
Use SageMaker Pipelines for orchestrating ML workflows, visualize them as a DAG, and leverage SageMaker ML Lineage Tracking for auditing and compliance

Use SageMaker Experiments for tracking and visualizing all workflows and rely on AWS CodePipeline for orchestration and governance

Use SageMaker Pipelines for orchestration, visualize workflows as a DAG, and leverage SageMaker Experiments for model governance and compliance

Explicación general
Correct option:

Use SageMaker Pipelines for orchestrating ML workflows, visualize them as a DAG, and leverage SageMaker ML Lineage Tracking for auditing and compliance

SageMaker Pipelines is specifically designed for ML workflow orchestration. It provides:

Fine-grained control - Data scientists can define step-by-step ML workflows, including data preparation, model training, and deployment.

Visualization as a DAG - Workflows are represented as a directed acyclic graph (DAG), making it easy to visualize dependencies.

Seamless integration with SageMaker ML Lineage Tracking - Automatically tracks lineage information, including input datasets, model artifacts, and inference endpoints, ensuring compliance and auditability.

SageMaker ML Lineage Tracking provides tools to establish model governance by maintaining a detailed record of all components involved in the ML lifecycle.

Amazon SageMaker ML Lineage Tracking:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html

Incorrect options:

Use AWS CodePipeline for orchestration, visualize workflows as a series of stages, and implement SageMaker Experiments to maintain a running history of model discovery experiments - AWS CodePipeline is a general-purpose CI/CD tool for automating workflows, but it does not provide DAG visualization or fine-grained control for ML-specific workflows like SageMaker Pipelines. Additionally, SageMaker Experiments does not track lineage for auditing and compliance.

Use SageMaker Experiments for tracking and visualizing all workflows and rely on AWS CodePipeline for orchestration and governance - SageMaker Experiments tracks and organizes experiment trials but does not provide workflow orchestration or DAG visualization. It also lacks tools for comprehensive compliance and auditing.

Use SageMaker Pipelines for orchestration, visualize workflows as a DAG, and leverage SageMaker Experiments for model governance and compliance - SageMaker Experiments is useful for managing experiments but does not provide lineage tracking, which is critical for ensuring governance for regulatory audits and compliance verifications.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html

https://aws.amazon.com/sagemaker-ai/experiments/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 19

A retail company has deployed a machine learning (ML) model using Amazon SageMaker to forecast product demand. The model is exposed via a SageMaker endpoint that processes requests from multiple applications. The company needs to record and monitor all API call events made to the endpoint and receive a notification whenever the number of requests exceeds a specific threshold during peak traffic hours.

Which solution will meet these requirements?

Use SageMaker Model Monitor to capture and analyze endpoint traffic and configure a rule to notify when API calls exceed the specified threshold

Respuesta correcta
Use Amazon CloudWatch to monitor the API call metrics for the SageMaker endpoint and create an alarm to send notifications through Amazon SNS when the call count breaches the threshold

Enable AWS CloudTrail to log all SageMaker API call events and use CloudTrail Insights to send notifications when the API call volume exceeds a threshold

Use Amazon EventBridge to capture SageMaker API call events and configure a rule to send a notification when the event count breaches the threshold

Explicación general
Correct option:

Use Amazon CloudWatch to monitor the API call metrics for the SageMaker endpoint and create an alarm to send notifications through Amazon SNS when the call count breaches the threshold

Amazon CloudWatch is the most suitable solution for this scenario because it provides:

Metrics for API call counts - CloudWatch automatically collects invocation metrics for SageMaker endpoints, including Invocations, InvocationErrors, and Latency.

Alarms - Alarms can be created to monitor thresholds for metrics, such as the number of API calls.

Notifications - When a threshold is breached, the alarm can send notifications through Amazon Simple Notification Service (SNS).

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html

Incorrect options:

Enable AWS CloudTrail to log all SageMaker API call events and use CloudTrail Insights to send notifications when the API call volume exceeds a threshold - AWS CloudTrail logs API calls for auditing purposes and can detect unusual activity using CloudTrail Insights. However, it is not designed for threshold-based alarms.

Use SageMaker Model Monitor to capture and analyze endpoint traffic and configure a rule to notify when API calls exceed the specified threshold - SageMaker Model Monitor is used to track data quality, bias, and drift for endpoint traffic. It does not natively monitor API call metrics or provide mechanisms for triggering notifications based on call counts.

Use Amazon EventBridge to capture SageMaker API call events and configure a rule to send a notification when the event count breaches the threshold - EventBridge is suitable for capturing and routing specific event patterns, but it does not aggregate or monitor API metrics over time. It lacks the capability to set thresholds or alarms for continuous monitoring.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 20

You are a machine learning engineer at a financial services company tasked with building a real-time fraud detection system. The model needs to be highly accurate to minimize false positives and false negatives. However, the company has a limited budget for cloud resources, and the model needs to be retrained frequently to adapt to new fraud patterns. You must carefully balance model performance, training time, and cost to meet these requirements.

Which of the following strategies is the MOST LIKELY to achieve an optimal balance between model performance, training time, and cost?

Use a deep neural network with multiple layers and complex architecture to maximize performance, even if it requires significant computational resources and longer training times

Deploy a simpler model like logistic regression to reduce training time and cost, while accepting a slight reduction in model accuracy

Choose a support vector machine (SVM) with a nonlinear kernel to enhance accuracy, regardless of the increased training time and cost associated with large datasets

Respuesta correcta
Implement a tree-based model like XGBoost with early stopping and hyperparameter tuning, balancing accuracy with reduced training time and computational cost

Explicación general
Correct option:

Implement a tree-based model like XGBoost with early stopping and hyperparameter tuning, balancing accuracy with reduced training time and computational cost

XGBoost is known for its ability to deliver high performance with relatively efficient training times, especially with techniques like early stopping and hyperparameter tuning. This approach balances the need for accuracy with reduced computational cost and training time, making it an ideal choice for this scenario.

Incorrect options:

Use a deep neural network with multiple layers and complex architecture to maximize performance, even if it requires significant computational resources and longer training times - A deep neural network may provide high accuracy but typically requires significant computational resources and longer training times, leading to higher costs. This approach may not be feasible within a limited budget, especially with the need for frequent retraining.

Deploy a simpler model like logistic regression to reduce training time and cost, while accepting a slight reduction in model accuracy - Logistic regression is simple and cost-effective but may not achieve the level of accuracy required for a critical application like fraud detection. This tradeoff might be too significant if accuracy is compromised.

Choose a support vector machine (SVM) with a nonlinear kernel to enhance accuracy, regardless of the increased training time and cost associated with large datasets - SVMs with nonlinear kernels can be very accurate but are computationally intensive, particularly with large datasets. The increased training time and cost might outweigh the benefits, especially when there are more cost-effective alternatives like XGBoost.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html

Temática
ML Model Development
Pregunta 21

A healthcare organization manages a large dataset containing patient records stored in Amazon S3. The dataset includes information such as patient names, addresses, and phone numbers, but it contains duplicate records due to inconsistencies in data entry. Some duplicates are exact matches, while others have slight variations (e.g., spelling errors in names or incomplete addresses). The organization needs to identify and group duplicate records efficiently while ensuring minimal code development to streamline the deduplication process.

Which solution on AWS will detect duplicates in the dataset with the LEAST operational overhead?

Use SageMaker Data Wrangler to detect and group duplicate records in the dataset by leveraging its data preparation and transformation features

Use AWS Glue ETL to create a custom job for processing and filtering duplicate records.

Respuesta correcta
Use AWS Glue FindMatches to automatically detect and group duplicate records in the dataset

Use Amazon EMR with Apache Spark to write a custom deduplication script using SparkSQL

Explicación general
Correct option:

Use AWS Glue FindMatches to automatically detect and group duplicate records in the dataset

AWS Glue FindMatches is a built-in feature designed to detect duplicate records in datasets, even when the records are not exact matches. It uses machine learning to find similarities across key attributes, such as customer names, addresses, and emails.

Key Benefits of AWS Glue FindMatches:

Minimal coding required - The ML-based approach simplifies the deduplication process.

Flexible matching logic - Automatically identifies fuzzy matches and near-duplicates.

Scalable and serverless - Works seamlessly with large datasets in Amazon S3.

Incorrect options:

Use Amazon EMR with Apache Spark to write a custom deduplication script using SparkSQL - While SparkSQL is powerful, writing custom scripts requires significant coding and operational overhead compared to the serverless and no-code FindMatches feature.

Use AWS Glue ETL to create a custom job for processing and filtering duplicate records. - AWS Glue ETL jobs require manual configuration and coding to deduplicate data, which adds complexity compared to FindMatches’ built-in ML-based approach.

Use SageMaker Data Wrangler to detect and group duplicate records in the dataset by leveraging its data preparation and transformation features - While Amazon SageMaker Data Wrangler is a powerful tool for data preparation and transformation, it does not have built-in capabilities for detecting or grouping duplicate records, especially those requiring fuzzy matching. Data Wrangler is better suited for cleaning, visualizing, and transforming datasets before training machine learning models, but it lacks the machine learning-powered deduplication features provided by AWS Glue FindMatches.

References:

https://community.aws/content/2c0dOcTyJPnK1NR7H05Oii4HblJ/aws-glue

Temática
Data Preparation for Machine Learning (ML)
Pregunta 22

A marketing analytics company is building a customer segmentation model to identify groups of users based on their purchasing behavior and engagement. The dataset includes user demographic data, transaction history, and website interaction logs. The demographic data and transaction history are stored in Amazon S3, while website interaction logs are stored in an on-premises PostgreSQL database. The dataset includes a mix of categorical features (e.g., "region," "customer tier") and numerical features (e.g., "purchase amount," "session duration"). To improve the model's performance, the ML engineer must transform and preprocess the data. The solution must minimize operational overhead while ensuring the dataset is ready for model training.

Which solution will meet these requirements with the LEAST operational overhead?

Use Amazon SageMaker Data Wrangler to preprocess the data, including transforming numerical data into categorical data using one-hot encoding and standardizing numerical features

Respuesta correcta
Use Amazon SageMaker Data Wrangler to preprocess the data, including transforming categorical data into numerical data using one-hot encoding and standardizing numerical features

Use AWS Glue ETL jobs to convert categorical data into numerical format and apply custom transformations to the dataset

Use Amazon Athena queries to manually transform categorical data into numerical representations and store the output back into S3

Explicación general
Correct option:

Use Amazon SageMaker Data Wrangler to preprocess the data, including transforming categorical data into numerical data using one-hot encoding and standardizing numerical features

Amazon SageMaker Data Wrangler simplifies the process of data preprocessing and transformation for machine learning. It provides a visual, no-code interface to:

Transform categorical data: Convert it into numerical representations using techniques like one-hot encoding or ordinal encoding. Process numerical data: Apply transformations like standardization or normalization to ensure numerical features contribute effectively to the model. Handle class imbalances: Data Wrangler allows applying oversampling or undersampling techniques. Data Wrangler integrates seamlessly with Amazon S3, SageMaker pipelines, and other AWS services, reducing the need for manual coding or operational overhead.

Key Benefits:

Minimal operational effort with a visual, interactive interface.

Built-in transformations for categorical and numerical data.

Easily exports transformed datasets to SageMaker for training.

SageMaker Data Wrangler provides a data quality and insights report that automatically verifies data quality (such as missing values, duplicate rows, and data types) and helps detect anomalies (such as outliers, class imbalance, and data leakage) in your data. SageMaker Data Wrangler offers over 300 prebuilt PySpark transformations and a natural language interface to prepare tabular, timeseries, text and image data without coding. Common use cases such vectorize text, featurize datetime, encoding, balancing data, or image augmentation are covered.

 via - https://aws.amazon.com/sagemaker-ai/data-wrangler/

 via - https://aws.amazon.com/sagemaker-ai/data-wrangler/

Incorrect options:

Use AWS Glue ETL jobs to convert categorical data into numerical format and apply custom transformations to the dataset - AWS Glue ETL jobs require writing custom scripts for data transformations, increasing operational overhead. Glue is better suited for large-scale ETL rather than quick, visual feature transformations.

Use Amazon Athena queries to manually transform categorical data into numerical representations and store the output back into S3 - Athena is designed for querying data using SQL but does not support efficient preprocessing or feature transformations like standardization and encoding.

Use Amazon SageMaker Data Wrangler to preprocess the data, including transforming numerical data into categorical data using one-hot encoding and standardizing numerical features - You cannot transform numerical data into categorical data using one-hot encoding. So this option is incorrect.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html

https://aws.amazon.com/sagemaker-ai/data-wrangler/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 23

You are an ML engineer at a data analytics company tasked with training a deep learning model on a large, computationally intensive dataset. The training job can tolerate interruptions and is expected to run for several hours or even days, depending on the available compute resources. The company has a limited budget for cloud infrastructure, so you need to minimize costs as much as possible.

Which strategy is the MOST EFFECTIVE for your ML training job while minimizing cost and ensuring the job completes successfully?

Respuesta correcta
Use Amazon SageMaker Managed Spot Training to dynamically allocate Spot Instances for the training job, automatically retrying any interrupted instances via checkpoints

Deploy the training job on a fixed number of On-Demand EC2 instances to ensure stability, and manually add Spot Instances as needed to speed up the job during off-peak hours

Start the training job using only Spot Instances to minimize cost, and switch to On-Demand instances manually if any Spot Instances are interrupted during training

Use Amazon EC2 Auto Scaling to automatically add Spot Instances to the training job based on demand, and configure the job to continue processing even if some Spot Instances are interrupted

Explicación general
Correct option:

Use Amazon SageMaker Managed Spot Training to dynamically allocate Spot Instances for the training job, automatically retrying any interrupted instances via checkpoints

Managed Spot Training uses Amazon EC2 Spot instance to run training jobs instead of on-demand instances. You can specify which training jobs use spot instances and a stopping condition that specifies how long SageMaker waits for a job to run using Amazon EC2 Spot instances. Spot instances can be interrupted, causing jobs to take longer to start or finish. You can configure your managed spot training job to use checkpoints. SageMaker copies checkpoint data from a local path to Amazon S3. When the job is restarted, SageMaker copies the data from Amazon S3 back into the local path. The training job can then resume from the last checkpoint instead of restarting.

 via - https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/

Incorrect options:

Use Amazon EC2 Auto Scaling to automatically add Spot Instances to the training job based on demand, and configure the job to continue processing even if some Spot Instances are interrupted - Amazon EC2 Auto Scaling can add Spot Instances based on demand, but it does not provide the same level of automation and resilience as SageMaker Managed Spot Training, especially for ML-specific workloads where Spot interruptions need to be handled gracefully.

Deploy the training job on a fixed number of On-Demand EC2 instances to ensure stability, and manually add Spot Instances as needed to speed up the job during off-peak hours - Using a fixed number of On-Demand EC2 instances provides stability, but manually adding Spot Instances introduces complexity and may not fully optimize costs. Automating this process with SageMaker is more efficient.

Start the training job using only Spot Instances to minimize cost, and switch to On-Demand instances manually if any Spot Instances are interrupted during training - Starting with only Spot Instances minimizes costs, but manually switching to On-Demand instances increases the risk of delays and interruptions if Spot capacity becomes unavailable. SageMaker Managed Spot Training offers a more reliable and automated solution.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html

https://aws.amazon.com/blogs/aws/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 24

You are a Machine Learning Engineer working for a large retail company that has developed multiple machine learning models to improve various aspects of their business, including personalized recommendations, generative AI, and fraud detection. The models have different deployment requirements:

The recommendations models need to handle real-time inference with low latency.

The generative AI model requires high scalability to manage fluctuating loads.

The fraud detection model is a large model and needs to be integrated into serverless applications to minimize infrastructure management.

Which of the following deployment targets should you choose for the different machine learning models, given their specific requirements? (Select two)

Use AWS Lambda to deploy the fraud detection model, which requires rapid scaling and integration into an existing serverless architecture, minimizing infrastructure management

Choose Amazon Elastic Container Service (Amazon ECS) for the recommendation model, as it provides container orchestration for large-scale, batch processing workloads with tight integration into other AWS services

Deploy all models using Amazon SageMaker endpoints for consistency and ease of management, regardless of their individual requirements for scalability, latency, or integration

Selección correcta
Deploy the real-time recommendation model using Amazon SageMaker endpoints to ensure low-latency, high-availability, and managed infrastructure for real-time inference

Selección correcta
Deploy the generative AI model using Amazon Elastic Kubernetes Service (Amazon EKS) to leverage containerized microservices for high scalability and control over the deployment environment

Explicación general
Correct options:

Deploy the real-time recommendation model using Amazon SageMaker endpoints to ensure low-latency, high-availability, and managed infrastructure for real-time inference

Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.

This makes it an ideal choice for the recommendation model, which must provide fast responses to user interactions with minimal downtime.

Deploy the generative AI model using Amazon Elastic Kubernetes Service (Amazon EKS) to leverage containerized microservices for high scalability and control over the deployment environment

Amazon EKS is designed for containerized applications that need high scalability and flexibility. It is suitable for the generative AI model, which may require complex orchestration and scaling in response to varying demand, while giving you full control over the deployment environment.

 via - https://aws.amazon.com/blogs/containers/deploy-generative-ai-models-on-amazon-eks/

Incorrect options:

Use AWS Lambda to deploy the fraud detection model, which requires rapid scaling and integration into an existing serverless architecture, minimizing infrastructure management - While AWS Lambda is excellent for serverless applications, it may not be the best choice for a fraud detection model if it requires continuous, low-latency processing or needs to handle very high throughput. Lambda is better suited for lightweight, event-driven tasks rather than long-running, complex inference jobs.

Choose Amazon Elastic Container Service (Amazon ECS) for the recommendation model, as it provides container orchestration for large-scale, batch processing workloads with tight integration into other AWS services - Amazon ECS is a good choice for containerized workloads but is generally more appropriate for batch processing or large-scale, stateless applications. It might not provide the low-latency and real-time capabilities needed for the recommendation model.

Deploy all models using Amazon SageMaker endpoints for consistency and ease of management, regardless of their individual requirements for scalability, latency, or integration - Deploying all models using Amazon SageMaker endpoints without considering their specific requirements for latency, scalability, and integration would be suboptimal. While SageMaker endpoints are highly versatile, they may not be the best fit for every use case, especially for models requiring serverless architecture or advanced container orchestration.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html

https://aws.amazon.com/blogs/containers/deploy-generative-ai-models-on-amazon-eks/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 25

A manufacturing company is building an anomaly detection system to identify defective products in its production lines. The datasets include sensor logs from IoT devices stored in Amazon S3 and a list of production metadata from an on-premises SQL database.

The company must:

Aggregate and preprocess the data from multiple sources.

Implement a solution to detect anomalies automatically in the sensor data.

Visualize the results for analysis by the operations team.

Which solution will meet these requirements most efficiently?

Respuesta correcta
Use Amazon SageMaker Data Wrangler to aggregate, clean, and prepare data for anomaly detection while generating visual insights

Use Amazon Athena to query the sensor data and identify anomalies through SQL queries. Use Amazon QuickSight to visualize the results

Use Amazon Kinesis Data Streams to process and analyze the sensor data in real time for anomaly detection. Use Amazon QuickSight to visualize the results

Use Amazon EMR with Spark MLlib to run anomaly detection algorithms and visualize the results using custom dashboards

Explicación general
Correct option:

Use Amazon SageMaker Data Wrangler to aggregate, clean, and prepare data for anomaly detection while generating visual insights

Amazon SageMaker Data Wrangler is a managed service designed to simplify data aggregation, cleaning, and feature engineering for machine learning workflows. It connects seamlessly to data sources like Amazon S3 and SQL databases, allowing engineers to preprocess large datasets and detect anomalies efficiently. It also provides built-in visualizations for analyzing trends, correlations, and outliers.

Key Benefits:

Integrates with Amazon S3 and on-premises databases for data aggregation.

Provides built-in anomaly detection tools and visual insights for streamlined analysis.

Simplifies feature engineering and model input preparation.

SageMaker Data Wrangler:

 via - https://aws.amazon.com/sagemaker-ai/data-wrangler/

Incorrect options:

Use Amazon Kinesis Data Streams to process and analyze the sensor data in real time for anomaly detection. Use Amazon QuickSight to visualize the results - Amazon Kinesis Data Streams is designed for real-time data ingestion and streaming. While it can process large volumes of sensor data in real time, it does not include built-in anomaly detection capabilities or support data aggregation and preprocessing for machine learning workflows. Additional services like Amazon SageMaker or custom code would be required to analyze anomalies, thereby increasing complexity for the solution.

Use Amazon Athena to query the sensor data and identify anomalies through SQL queries. Use Amazon QuickSight to visualize the results - Athena is a query service that allows running SQL queries directly on Amazon S3 data. While useful for ad hoc analysis, it does not support automatic anomaly detection or data preprocessing, making it less suitable for this use case.

Use Amazon EMR with Spark MLlib to run anomaly detection algorithms and visualize the results using custom dashboards - While EMR with Spark MLlib is powerful for anomaly detection, it requires significant manual configuration, coding, and operational overhead. Custom dashboards for visualization further add complexity compared to SageMaker Data Wrangler.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html

https://aws.amazon.com/sagemaker-ai/data-wrangler/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 26

You are working as a machine learning engineer for a startup that provides image recognition services. The service is currently in its beta phase, and the company expects varying levels of traffic, with some days having very few requests and other days experiencing sudden spikes. The company wants to minimize costs during low-traffic periods while still being able to handle large, infrequent spikes of requests efficiently. Given these requirements, you are considering using Amazon SageMaker for your deployment.

Which of the following statements is the BEST recommendation for the given scenario?

Respuesta correcta
Use Amazon SageMaker Serverless Inference that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently

Use Batch transform to run inference with Amazon SageMaker that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently

Use Amazon SageMaker Asynchronous Inference that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently

Use Amazon SageMaker Real-time Inference that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently

Explicación general
Correct option:

Use Amazon SageMaker Serverless Inference that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html

Serverless Inference is designed to automatically scale the compute resources based on incoming requests, making it highly efficient for handling varying levels of traffic. It is cost-effective because you only pay for the compute time used when requests are being processed. This makes it an excellent choice for scenarios where traffic is unpredictable, with periods of low or no traffic. It is ideal for workloads that have idle periods between traffic spikes and can tolerate cold starts.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html

Incorrect options:

Use Amazon SageMaker Asynchronous Inference that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently - Asynchronous Inference is ideal for handling large and long-running inference requests that do not require an immediate response. However, it may not be as cost-effective for handling fluctuating traffic where immediate scaling and low-latency are priorities.

Use Amazon SageMaker Real-time Inference that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently - Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements.

Use Batch transform to run inference with Amazon SageMaker that minimizes costs during low-traffic periods while managing large infrequent spikes of requests efficiently - To get predictions for an entire dataset, you can use Batch transform with Amazon SageMaker.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html

https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 27

A fintech company is developing an AI-driven fraud detection system using Amazon SageMaker. The system must provide end-to-end ML capabilities, including data preprocessing, model training, versioned model storage, deployment, and monitoring. Customer transaction data is stored securely in Amazon S3.

The company requires the following:

Secure access to training data for different ML workflows to ensure data isolation.

A centralized model registry to manage model versions and deployments with minimal operational overhead.

What is the most appropriate approach to meet these requirements with the LEAST operational overhead?

Use unique tags for each model version and use SageMaker Model Registry to manage model deployments

Respuesta correcta
Use SageMaker Model Registry to manage model versions and associate models with model groups

Use Amazon S3 versioning to track model artifacts and manage deployments manually

Use Amazon Elastic Container Registry (Amazon ECR) to store model artifacts and versions

Explicación general
Correct option:

Use SageMaker Model Registry to manage model versions and associate models with model groups

Amazon SageMaker Model Registry is purpose-built for storing, versioning, and managing ML models as part of the SageMaker ecosystem. It allows users to organize models into model groups, which act as containers for different versions of a model. This feature integrates seamlessly with SageMaker workflows like training, deployment, and monitoring, reducing operational overhead while maintaining robust version control.

By using SageMaker Model Registry:

Different versions of a model can be tracked and managed efficiently.

Secure and isolated use of training data can be implemented by leveraging IAM roles and SageMaker training jobs.

Deployment pipelines can be automated using SageMaker endpoints, further reducing manual intervention.

Model Registry Models, Model Versions, and Model Groups:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html

Incorrect options:

Use Amazon Elastic Container Registry (Amazon ECR) to store model artifacts and versions - Amazon ECR is designed for storing and managing container images, not directly for ML model versioning. While SageMaker models might use Docker containers stored in ECR for deployment, ECR does not provide capabilities like tracking model metadata, associating models with model groups, or automating versioning. This would add operational overhead.

Use unique tags for each model version and use SageMaker Model Registry to manage model deployments - While unique tags can help organize models, SageMaker Model Registry already provides built-in capabilities for versioning and organizing models through model groups. Adding tags would add unnecessary complexity and does not add significant benefits over using model groups.

Use Amazon S3 versioning to track model artifacts and manage deployments manually - Amazon S3 versioning can track different versions of objects (artifacts), but it does not offer a centralized model registry or automated ML lifecycle management. Managing deployments manually increases overhead, making this approach inefficient.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html

https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-deploy.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 28

A healthcare company is building a predictive model to identify high-risk patients for hospital readmission. The dataset includes patient records such as demographic information, past diagnoses, and admission history. The data is stored in Amazon S3 and a relational database hosted on an on-premises PostgreSQL server. The dataset has a class imbalance issue where very few patients are flagged as high-risk, which affects the performance of the model. Additionally, the dataset contains both categorical features (e.g., "diagnosis type") and numerical features (e.g., "days in hospital"). The ML engineer must preprocess the data to resolve the class imbalance and ensure the dataset is ready for training, using a solution that requires minimal operational effort.

Which solution will meet these requirements?

Use AWS Glue ETL jobs to write custom scripts for handling class imbalance by oversampling the minority class

Leverage AWS Glue DataBrew’s native features to clean and transform the dataset, including attempting to balance class distribution by duplicating records from the minority class

Respuesta correcta
Use Amazon SageMaker Data Wrangler's 'balance data' operation to oversample the minority class to resolve the class imbalance

Use SageMaker Feature Store to automatically balance the class distribution in the dataset

Explicación general
Correct option:

Use Amazon SageMaker Data Wrangler's 'balance data' operation to oversample the minority class to resolve the class imbalance

Amazon SageMaker Data Wrangler provides an intuitive, visual interface for performing data preparation tasks. Key benefits include:

No-code approach with minimal operational overhead.

Supports popular techniques for balancing data (oversampling/undersampling).

Integrates seamlessly with Amazon S3 and SageMaker workflows.

The "balance data" operation in SageMaker Data Wrangler allows users to easily address class imbalance issues using techniques like -

Oversampling: Duplicating data from the minority class.

Undersampling: Reducing the majority class data to match the minority class.

This built-in operation eliminates the need for custom scripts or complex workflows, ensuring the task is completed with minimal operational effort. The balanced dataset can then be directly exported to SageMaker for model training.

Balance your data for machine learning with Amazon SageMaker Data Wrangler:  via - https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/

Incorrect options:

Leverage AWS Glue DataBrew’s native features to clean and transform the dataset, including attempting to balance class distribution by duplicating records from the minority class - AWS Glue DataBrew provides built-in capabilities for data cleaning and transformations, such as filtering rows, handling missing data, and applying basic aggregations. However, DataBrew does not have native features for oversampling or undersampling to balance class distributions. Addressing class imbalance would require custom steps or manual workarounds, which increase operational overhead compared to SageMaker Data Wrangler's built-in "balance data" operation.

Use SageMaker Feature Store to automatically balance the class distribution in the dataset - SageMaker Feature Store is used for storing and serving preprocessed features, but it does not provide functionality to balance data.

Use AWS Glue ETL jobs to write custom scripts for handling class imbalance by oversampling the minority class - AWS Glue ETL jobs require custom Python or PySpark scripts for balancing data, which increases operational overhead compared to SageMaker Data Wrangler's built-in operations.

References:

https://aws.amazon.com/sagemaker-ai/data-wrangler/

https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/

https://aws.amazon.com/blogs/big-data/7-most-common-data-preparation-transformations-in-aws-glue-databrew/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 29

A financial analytics company runs a data aggregation job every Saturday night to process transactional data from the past week. The job is scheduled to run for approximately 2 hours and can tolerate interruptions without impacting the results. The company plans to run this job consistently every weekend for the next 6 months.

Which EC2 instance purchasing option will meet these requirements MOST cost-effectively?

Use EC2 On-Demand Instances to run the job each weekend for 2 hours, paying only for the compute time consumed

Use EC2 Reserved Instances with a 6-month commitment to ensure lower costs and guaranteed availability

Respuesta correcta
Use EC2 Spot Instances to run the aggregation job, as they provide substantial cost savings and can handle interruptions

Use EC2 Dedicated Hosts to run the job for full control over the infrastructure and long-term cost efficiency

Explicación general
Correct option:

Use EC2 Spot Instances to run the aggregation job, as they provide substantial cost savings and can handle interruptions

EC2 Spot Instances are the best fit for this scenario because they allow the company to:

Leverage unused EC2 capacity at significantly reduced prices, offering up to 90% cost savings compared to On-Demand pricing.

Run the job in off-peak hours (Saturday night), reducing the likelihood of interruptions.

Benefit from the fact that the job tolerates interruptions, aligning perfectly with Spot Instances' use case.

This makes Spot Instances the most cost-effective solution for a short-duration, intermittent, and interruption-tolerant workload.

Incorrect options:

Use EC2 On-Demand Instances to run the job each weekend for 2 hours, paying only for the compute time consumed - On-Demand Instances are more expensive compared to Spot Instances. While they provide guaranteed availability, the workload can handle interruptions, making Spot Instances a better choice.

Use EC2 Reserved Instances with a 6-month commitment to ensure lower costs and guaranteed availability - Reserved Instances are suitable for long-term, predictable workloads that require guaranteed availability. The available term for Reserved Instances is 1 or 3 years. For a weekly job that is slated to run only for 6 months, this approach would result in wasted costs for unused hours.

Use EC2 Dedicated Hosts to run the job for full control over the infrastructure and long-term cost efficiency - Dedicated Hosts are expensive and are typically used for workloads requiring dedicated physical servers due to compliance or licensing requirements. They are not cost-effective for short-duration batch jobs.

Reference:

https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 30

A healthcare company is training a neural network to classify medical images as healthy or abnormal. The ML engineer observes that the model's performance on the validation dataset improves significantly during the early epochs but begins to degrade after a certain number of epochs.

Which solutions will mitigate this problem? (Select two)

Reduce the size of the training dataset to simplify the learning process and avoid overfitting

Selección correcta
Add dropout layers to the neural network architecture to prevent overfitting

Increase the number of epochs to ensure the model learns the training data thoroughly before stopping

Selección correcta
Use early stopping to halt training once the validation loss stops improving

Increase the number of layers as well as the number of neurons to prevent overfitting

Explicación general
Correct option:

Use early stopping to halt training once the validation loss stops improving

Early stopping monitors the performance of the model on the validation dataset and halts training when validation performance stops improving. This prevents overfitting by stopping training at the optimal number of epochs.

Add dropout layers to the neural network architecture to prevent overfitting

Dropout layers randomly set a fraction of neurons to zero during training, reducing the model's reliance on specific neurons and encouraging it to learn more generalized patterns. This is a proven method to mitigate overfitting.

 via - https://aws.amazon.com/what-is/overfitting/

Incorrect options:

Increase the number of epochs to ensure the model learns the training data thoroughly before stopping - Increasing the number of epochs will exacerbate overfitting as the model will continue to optimize for the training data, further degrading performance on the validation dataset.

Reduce the size of the training dataset to simplify the learning process and avoid overfitting - Reducing the size of the training dataset would likely decrease the model's ability to generalize, as it would have less data to learn from. This would worsen overfitting rather than mitigate it.

Increase the number of layers as well as the number of neurons to prevent overfitting - Increasing neurons and layers can lead to overfitting if the model capacity exceeds the complexity of the task. So, this option is ruled out.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-early-stopping.html

https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-supports-early-stopping-of-training-jobs/

https://aws.amazon.com/what-is/overfitting/

Temática
ML Model Development
Pregunta 31

Which of the following strategies best aligns with the defense-in-depth security approach for generative AI applications on AWS?

Respuesta correcta
Applying multiple layers of security measures including input validation, access controls, and continuous monitoring to address vulnerabilities

Implementing a single-layer firewall to block unauthorized access to the AI models

Relying solely on data encryption to protect the AI training data

Using a single authentication mechanism for all users and services accessing the AI models

Explicación general
Correct option:

Applying multiple layers of security measures including input validation, access controls, and continuous monitoring to address vulnerabilities

Architecting a defense-in-depth security approach involves implementing multiple layers of security to protect generative AI applications. This includes input validation to prevent malicious data inputs, strict access controls to limit who can interact with the AI models, and continuous monitoring to detect and respond to security incidents. These measures can help address common vulnerabilities and meet the best practices for securing generative AI applications on AWS.

Incorrect options:

Implementing a single-layer firewall to block unauthorized access to the AI models - While a firewall is an important security measure, relying on a single layer of defense is insufficient for comprehensive security. Defense-in-depth requires multiple, overlapping layers of protection.

Relying solely on data encryption to protect the AI training data - Data encryption is crucial for protecting data at rest and in transit, but it does not address other vulnerabilities such as input validation or unauthorized access. A holistic security strategy is needed.

Using a single authentication mechanism for all users and services accessing the AI models - Employing a single authentication mechanism is a weak security practice. Multiple authentication and authorization mechanisms should be used to ensure robust access control.

Reference:

https://aws.amazon.com/blogs/machine-learning/architect-defense-in-depth-security-for-generative-ai-applications-using-the-owasp-top-10-for-llms/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 32

You are a data scientist at a healthcare company developing a machine learning model to analyze medical imaging data, such as X-rays and MRIs, for disease detection. The dataset consists of 10 million high-resolution images stored in Amazon S3, amounting to several terabytes of data. The training process requires processing these images efficiently to avoid delays due to I/O bottlenecks, and you must ensure that the chosen data access method aligns with the large dataset size and the high throughput requirements of the model.

Given the size and nature of the dataset, which SageMaker input mode and AWS Cloud Storage configuration is the MOST SUITABLE for this use case?

Respuesta correcta
Select the Pipe input mode to stream the data directly from Amazon S3 to the training instances, allowing the model to start processing data immediately without requiring local storage for the entire dataset

Use the File input mode with EFS (Amazon Elastic File System) to mount the dataset across multiple instances, ensuring data is shared and accessible during distributed training

Use the File input mode to download the entire dataset from Amazon S3 to the training instances' local storage before starting the training process, ensuring that all data is available locally during training

Implement the FastFile input mode with FSx for Lustre, to enable on-demand streaming of data chunks from Amazon S3 with low latency and high throughput

Explicación general
Correct option:

Select the Pipe input mode to stream the data directly from Amazon S3 to the training instances, allowing the model to start processing data immediately without requiring local storage for the entire dataset

In pipe mode, data is pre-fetched from Amazon S3 at high concurrency and throughput, and streamed into a named pipe, which also known as a First-In-First-Out (FIFO) pipe for its behavior. Each pipe may only be read by a single process.

Pipe input mode is designed for large datasets, allowing data to be streamed directly from Amazon S3 into the training instances. This minimizes disk usage and allows training to begin immediately as the data streams in, making it ideal for your scenario where high throughput and efficiency are critical.





via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html

Incorrect options:

Use the File input mode to download the entire dataset from Amazon S3 to the training instances' local storage before starting the training process, ensuring that all data is available locally during training - The File input mode downloads the entire dataset to the training instance before starting the training job. For a dataset as large as yours, this would lead to significant delays and require large amounts of local storage, which is not optimal for efficiency or cost.

Implement the FastFile input mode with FSx for Lustre, to enable on-demand streaming of data chunks from Amazon S3 with low latency and high throughput - FastFile mode is useful for scenarios where you need rapid access to data with low latency, but it is best suited for workloads with many small files. You should note that FastFile mode can be used only while accessing data from Amazon S3 and not with Amazon FSx for Lustre. So, this option acts as a distractor.

Use the File input mode with EFS (Amazon Elastic File System) to mount the dataset across multiple instances, ensuring data is shared and accessible during distributed training - Using Amazon EFS for the given use case requires transferring the medical imaging data from Amazon S3 into Amazon EFS, which leads to unnecessary data transfer as well as data storage costs. So, this option is ruled out.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html

https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 33

A financial services company is developing an AI-based credit risk assessment system using Amazon SageMaker. The system needs to support end-to-end ML workflows, including experimentation, model training, version management, deployment, and monitoring. To comply with internal governance policies, the company requires a manual approval-based workflow to ensure that only approved models can be deployed to production endpoints. All training data should be securely stored in Amazon S3, and the models should be managed through a centralized system.

Which solution will best meet these requirements?

Use Amazon SageMaker Model Monitor to validate and approve models before deployment

Respuesta correcta
Use SageMaker Pipelines with conditional steps to implement manual approval workflows for model deployment

Use Amazon SageMaker Lineage Tracking to validate and approve models before deployment

Use AWS CodePipeline to manage deployments and set manual approval actions for endpoint updates

Explicación general
Correct option:

Use SageMaker Pipelines with conditional steps to implement manual approval workflows for model deployment

Amazon SageMaker Pipelines is the recommended solution for implementing manual approval-based workflows for model deployment. SageMaker Pipelines allows you to design automated ML workflows, including steps for training, registering models in SageMaker Model Registry, and deploying models to endpoints. You can use conditional steps in SageMaker Pipelines to introduce a manual approval step before proceeding to production deployments. This ensures only models explicitly approved by a human reviewer are deployed, which aligns perfectly with the requirement for governance and control.

Key Benefits:

Supports manual approval workflows within automated pipelines.

Integrates seamlessly with SageMaker Model Registry to manage approved models.

Reduces operational overhead by automating model deployment with built-in approval checks.

Incorrect options:

Use Amazon SageMaker Model Monitor to validate and approve models before deployment - SageMaker Model Monitor is designed to detect drift and monitor model quality after deployment. It does not provide manual approval workflows or enforce governance before deployment.

Use AWS CodePipeline to manage deployments and set manual approval actions for endpoint updates - While AWS CodePipeline can include manual approval actions, it is primarily a CI/CD tool. It does not integrate natively with SageMaker Model Registry or support the orchestration of ML workflows like SageMaker Pipelines does. This makes it less suitable for the ML-specific use case described.

Use Amazon SageMaker Lineage Tracking to validate and approve models before deployment - SageMaker Lineage Tracking is used to track the lineage of artifacts (e.g., datasets, models, and experiments) within an ML workflow. It provides visibility into the relationships between components, such as which dataset and training job produced a specific model version. However, it does not support manual approval workflows or enforce governance for deploying models to production. While lineage tracking is valuable for auditing and reproducibility, it does not include built-in mechanisms for validating or approving models for deployment.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-approve.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 34

You are a data scientist at an insurance company developing a machine learning model to predict the likelihood of claims being fraudulent. The company has a strong commitment to fairness and wants to ensure that the model does not disproportionately affect any specific demographic group. You decide to use Amazon SageMaker Clarify to assess potential bias in your model. In particular, you are interested in understanding how the model’s predictions differ across demographic groups when conditioned on relevant factors like income level, which could influence the likelihood of fraudulent claims.

Given this scenario, which of the following BEST describes how Conditional Demographic Disparity (CDD) can be used to assess and mitigate bias in your model?

Respuesta correcta
CDD evaluates the disparity in positive prediction rates across demographic groups, conditioned on a specific feature like income, to detect bias that may not be apparent when only considering overall outcomes

CDD measures the difference in average predicted outcomes between demographic groups, helping to identify overall bias without considering other factors

CDD assesses the proportion of correctly predicted outcomes for each demographic group, helping to ensure that the model is equally accurate across groups

CDD focuses on the relationship between feature importance and demographic groups, highlighting whether certain features disproportionately influence predictions for specific groups

Explicación general
Correct option:

CDD evaluates the disparity in positive prediction rates across demographic groups, conditioned on a specific feature like income, to detect bias that may not be apparent when only considering overall outcomes

Conditional Demographic Disparity (CDD) measures the difference in positive prediction rates between demographic groups, while conditioning on relevant features like income. This allows you to identify subtle biases that might be masked when looking only at overall predictions, ensuring that the model's decisions are fair across different groups given their specific circumstances.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-metric-cddl.html

Incorrect options:

CDD measures the difference in average predicted outcomes between demographic groups, helping to identify overall bias without considering other factors - This describes a general measure of demographic disparity but does not account for conditioning on other relevant features. CDD specifically conditions on features like income to assess disparity more accurately.

CDD assesses the proportion of correctly predicted outcomes for each demographic group, helping to ensure that the model is equally accurate across groups - This describes accuracy metrics rather than Conditional Demographic Disparity. CDD is focused on measuring differences in prediction rates, not the correctness of predictions.

CDD focuses on the relationship between feature importance and demographic groups, highlighting whether certain features disproportionately influence predictions for specific groups - This option describes feature importance analysis rather than CDD. While understanding the influence of features on predictions is important, CDD specifically examines disparities conditioned on certain features.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-metric-cddl.html

https://aws.amazon.com/blogs/machine-learning/learn-how-amazon-sagemaker-clarify-helps-detect-bias/

Temática
ML Model Development
Pregunta 35

You are an ML engineer at a retail company that uses a SageMaker model to generate product recommendations for customers in real-time. During peak shopping periods, the traffic to the recommendation engine increases dramatically. The company needs to ensure that the model endpoint can handle these spikes in demand without compromising on response time or customer experience. At the same time, you want to optimize costs by scaling down resources during periods of low demand. You are evaluating different scaling policies to manage this dynamic workload effectively.

Which scaling policy is the MOST SUITABLE for this scenario, and why?

Respuesta correcta
Use a target tracking scaling policy that automatically adjusts the number of instances based on a predefined target metric, such as CPU utilization or invocations per instance, to maintain a steady level of performance during traffic spikes

Use a step scaling policy that adjusts the number of instances based on the size of the traffic spike, adding a set number of instances for each level of increased demand

Use scheduled scaling to preemptively add or remove instances based on anticipated traffic patterns, such as known peak times during Black Friday, to ensure sufficient capacity is available when needed

Use a manual scaling policy where you adjust the number of instances based on real-time monitoring of traffic, allowing you to fine-tune resource allocation as needed during high-demand periods

Explicación general
Correct option:

Use a target tracking scaling policy that automatically adjusts the number of instances based on a predefined target metric, such as CPU utilization or invocations per instance, to maintain a steady level of performance during traffic spikes

A target tracking scaling policy is ideal for handling dynamic and unpredictable traffic spikes, as it continuously adjusts the number of instances to maintain a predefined metric (e.g., CPU utilization, invocations per instance). This ensures that performance remains consistent even during high-demand periods like Black Friday, while also scaling down during quieter times to save costs.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

Incorrect options:

Use a step scaling policy that adjusts the number of instances based on the size of the traffic spike, adding a set number of instances for each level of increased demand - Step scaling is useful for handling sudden, sharp increases in demand, but it may not respond as smoothly to varying levels of traffic. It’s less flexible than target tracking and might lead to over- or under-provisioning if traffic patterns are unpredictable.

Use scheduled scaling to preemptively add or remove instances based on anticipated traffic patterns, such as known peak times during Black Friday, to ensure sufficient capacity is available when needed - Scheduled scaling works well when traffic patterns are predictable, such as daily or weekly cycles. However, it’s less effective for managing unexpected traffic spikes, as it relies on predefined schedules rather than real-time data.

Use a manual scaling policy where you adjust the number of instances based on real-time monitoring of traffic, allowing you to fine-tune resource allocation as needed during high-demand periods - Manual scaling provides the most control but requires constant monitoring and intervention, which is impractical during high-traffic events like Black Friday. It also risks delays in scaling, which could lead to performance issues during traffic surges.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 36

You are an ML engineer at a startup that is developing a recommendation engine for an e-commerce platform. The workload involves training models on large datasets and deploying them to serve real-time recommendations to customers. The training jobs are sporadic but require significant computational power, while the inference workloads must handle varying traffic throughout the day. The company is cost-conscious and aims to balance cost efficiency with the need for scalability and performance.

Given these requirements, which approach to resource allocation is the MOST SUITABLE for training and inference, and why?

Use provisioned resources with reserved instances for both training and inference to lock in lower costs and guarantee resource availability, ensuring predictability in budgeting

Respuesta correcta
Use on-demand instances for training, allowing the flexibility to scale resources as needed, and use provisioned resources with auto-scaling for inference to handle varying traffic while controlling costs

Use on-demand instances for both training and inference to ensure that the company only pays for the compute resources it uses when it needs them, avoiding any upfront commitments

Use provisioned resources with spot instances for both training and inference to take advantage of the lowest possible costs, accepting the potential for interruptions during workload execution

Explicación general
Correct option:

Use on-demand instances for training, allowing the flexibility to scale resources as needed, and use provisioned resources with auto-scaling for inference to handle varying traffic while controlling costs

Using on-demand instances for training offers flexibility, allowing you to allocate resources only when needed, which is ideal for sporadic training jobs. For inference, provisioned resources with auto-scaling ensure that the system can handle varying traffic while controlling costs, as it can scale down during periods of low demand.

 via - https://aws.amazon.com/ec2/pricing/

Incorrect options:

Use on-demand instances for both training and inference to ensure that the company only pays for the compute resources it uses when it needs them, avoiding any upfront commitments - On-demand instances are flexible and ensure that you only pay for what you use, but they can be more expensive over time compared to provisioned resources, especially if workloads are consistent and predictable. This approach may be suboptimal for cost-sensitive long-term use.

Use provisioned resources with reserved instances for both training and inference to lock in lower costs and guarantee resource availability, ensuring predictability in budgeting - Provisioned resources with reserved instances provide cost savings and guaranteed availability but lack the flexibility needed for sporadic training jobs. For inference workloads with fluctuating demand, this approach might not handle traffic spikes efficiently without additional auto-scaling mechanisms.

Use provisioned resources with spot instances for both training and inference to take advantage of the lowest possible costs, accepting the potential for interruptions during workload execution - Spot instances provide significant cost savings but come with the risk of interruptions, which can be problematic for both training and real-time inference workloads. This option is generally better suited for non-critical batch jobs where interruptions can be tolerated.

References:

https://aws.amazon.com/ec2/pricing/

https://aws.amazon.com/ec2/pricing/reserved-instances/

https://aws.amazon.com/ec2/spot/

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 37

A retail company is building a web-based AI application using Amazon SageMaker to predict customer purchase behavior. The system must support full ML lifecycle features such as experimentation, training, centralized model registry, deployment, and monitoring. The training data is securely stored in Amazon S3, and the models need to be deployed to real-time endpoints to serve predictions. The company is now planning to run an on-demand workflow to monitor for bias drift in the deployed models to ensure fairness and accuracy in predictions.

What do you recommend?

Use AWS Glue Data Quality to validate and monitor data quality for detecting bias drift in real-time endpoints

Use the sagemaker-model-monitor-analyzer built-in SageMaker image to automatically analyze and resolve bias drift in deployed models

Use Amazon SageMaker Lineage Tracking to trace and analyze bias drift in model predictions

Respuesta correcta
Use Amazon SageMaker Clarify to analyze captured inference data from real-time endpoints for bias detection on demand

Explicación general
Correct option:

Use Amazon SageMaker Clarify to analyze captured inference data from real-time endpoints for bias detection on demand

Amazon SageMaker Clarify can analyze bias in the input and output data captured from SageMaker real-time endpoints. By enabling data capture in SageMaker endpoints and integrating Clarify, you can run on-demand bias analysis workflows to identify and measure bias drift.

This solution leverages Clarify’s post-deployment bias detection capabilities, allowing the company to analyze inference data over time and ensure the model maintains fairness in production.

Key Steps:

Enable data capture for real-time endpoints.

Use SageMaker Clarify to analyze the captured data for bias.

Generate detailed reports to detect and quantify bias drift.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-bias-drift.html

Incorrect options:

Use Amazon SageMaker Lineage Tracking to trace and analyze bias drift in model predictions - SageMaker Lineage Tracking is designed to track the lineage of ML artifacts, such as datasets, models, and experiments. While useful for auditability and reproducibility, it does not perform bias drift analysis or real-time model monitoring.

Use AWS Glue Data Quality to validate and monitor data quality for detecting bias drift in real-time endpoints - AWS Glue Data Quality is designed to evaluate and enforce data quality rules on datasets processed through AWS Glue ETL jobs. While it ensures input data meets quality thresholds during extraction, transformation, and loading, it does not provide bias drift monitoring for deployed machine learning models at real-time endpoints.

Use the sagemaker-model-monitor-analyzer built-in SageMaker image to automatically analyze and resolve bias drift in deployed models - The sagemaker-model-monitor-analyzer built-in image is used as part of Amazon SageMaker Model Monitor to run custom monitoring jobs, such as analyzing data captured from endpoints for drift, bias, or feature issues. However, this built-in image is only used for analysis and reporting purposes. It does not automatically resolve or fix bias drift.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-monitor-bias-drift.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 38

You are a data scientist at a retail company responsible for deploying a machine learning model that predicts customer purchase behavior. The model needs to serve real-time predictions with low latency to support the company’s recommendation engine on its e-commerce platform. The deployment solution must also be scalable to handle varying traffic loads during peak shopping periods, such as Black Friday and holiday sales. Additionally, you need to monitor the model's performance and automatically roll out updates when a new version of the model is available.

Given these requirements, which AWS deployment service and configuration is the MOST SUITABLE for deploying the machine learning model?

Respuesta correcta
Deploy the model using Amazon SageMaker real-time hosting services with an auto-scaling endpoint, enabling you to automatically adjust the number of instances based on traffic demand

Use AWS Lambda to deploy the model as a serverless function, automatically scaling based on the number of requests, and store the model artifacts in Amazon S3

Deploy the model on Amazon EC2 instances with a load balancer to distribute traffic, manually scaling the instances based on expected traffic during peak periods

Deploy the model on Amazon SageMaker with batch transform jobs, running the jobs periodically to generate predictions and storing the results in Amazon S3 for the recommendation engine

Explicación general
Correct option:

Deploy the model using Amazon SageMaker real-time hosting services with an auto-scaling endpoint, enabling you to automatically adjust the number of instances based on traffic demand

Amazon SageMaker hosting services provide a managed environment for deploying models in real-time, with support for auto-scaling based on traffic. This ensures low latency and scalability during peak periods, making it ideal for an e-commerce platform with fluctuating traffic. SageMaker also offers monitoring and versioning capabilities, allowing you to manage model updates efficiently.

Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html

Incorrect options:

Deploy the model on Amazon EC2 instances with a load balancer to distribute traffic, manually scaling the instances based on expected traffic during peak periods - Deploying on Amazon EC2 with a load balancer can work, but it requires manual scaling, which may not be responsive enough to sudden traffic spikes. It also lacks the integrated monitoring and management features provided by SageMaker.

Use AWS Lambda to deploy the model as a serverless function, automatically scaling based on the number of requests, and store the model artifacts in Amazon S3 - AWS Lambda is suitable for serverless deployments with lightweight models and use cases where request volume is unpredictable. However, it may not be ideal for high-throughput, low-latency applications like real-time recommendation engines, especially if the model is large or requires significant compute resources.

Deploy the model on Amazon SageMaker with batch transform jobs, running the jobs periodically to generate predictions and storing the results in Amazon S3 for the recommendation engine - Batch transform jobs in SageMaker are designed for batch predictions rather than real-time inference. While this approach works for generating predictions in bulk, it does not meet the requirement for real-time, low-latency predictions needed by the recommendation engine.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html

https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 39

A retail analytics company stores historical data in .csv files in Amazon S3. The data is partially populated, lacks column labels, and contains missing values. The company needs to prepare and structure this data so it can be used effectively for training ML models. Given this context, consider the following five steps:

Use AWS Glue crawlers to infer the schemas and available columns.

Use Amazon EMR with Apache Spark for data cleaning and feature engineering.

Store the resulting data back in Amazon S3.

Use Amazon Redshift Spectrum to infer the schemas and available columns.

Use AWS Glue DataBrew for data cleaning and feature engineering.

What is the correct order in which three of these steps should be selected to achieve this task efficiently?

5,1,3

Respuesta correcta
1,5,3

4,5,3

1,2,3

Explicación general
Correct option:

1,5,3

Use AWS Glue crawlers to infer the schemas and available columns.
AWS Glue crawlers can analyze the .csv files in Amazon S3 and automatically infer the schema and structure of the data. This creates a table definition in the AWS Glue Data Catalog, enabling the data to be organized and understood.

Using crawlers to populate the Data Catalog:  via - https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html

Use AWS Glue DataBrew for data cleaning and feature engineering.
AWS Glue DataBrew is a visual data preparation tool that allows the ML engineer to clean and preprocess the .csv data. Tasks such as filling in missing fields, transforming formats, and normalizing data can be performed easily with Glue DataBrew.

 via - https://docs.aws.amazon.com/databrew/latest/dg/what-is.html

Store the resulting data back in Amazon S3.
After the data is cleaned and processed, it is stored back into Amazon S3, where it can be accessed for ML model training or further analysis.

Incorrect options:

5,1,3 - You cannot proceed with data cleaning and feature engineering before inferring the schema of the underlying data, therefore this option is incorrect.

1,2,3 - While EMR with Apache Spark is capable of data cleaning and feature engineering, it is a more complex and costly solution compared to AWS Glue DataBrew, which is a managed service specifically designed for such tasks with lower operational overhead.

4,5,3 - Amazon Redshift Spectrum allows querying data in Amazon S3 using Redshift’s SQL engine. However, it does not infer schemas automatically; this is a capability of AWS Glue Crawlers. Spectrum is typically used for ad hoc querying rather than schema inference.

References:

https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html

https://docs.aws.amazon.com/databrew/latest/dg/what-is.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 40

You are an ML Engineer working for a healthcare company that uses a machine learning model to recommend personalized treatment plans to patients. The model is deployed on Amazon SageMaker and is critical to the company's operations, as any incorrect predictions could have significant consequences. A new version of the model has been developed, and you need to deploy it in production. However, you want to ensure that the deployment process is robust, allowing you to quickly roll back to the previous version if any issues arise. Additionally, you need to maintain version control for future updates and manage traffic between different model versions.

Which of the following strategies should you implement to ensure a smooth and reliable deployment of the new model version using Amazon SageMaker, considering best practices for versioning and rollback strategies? (Select two)

Create a backup of the current model, deploy the new version, and if any issues arise, manually roll back by redeploying the previous model version

Deploy the new model version immediately and redirect 100% of traffic to it, assuming it has been thoroughly tested and will not require a rollback

Selección correcta
Use Amazon SageMaker’s built-in versioning to manage different versions of the model, and deploy the new version in a canary release by redirecting a small percentage of traffic to it initially

Deploy the new model version alongside the current one, and use Amazon SageMaker’s multi-model endpoint to serve both models simultaneously, splitting traffic between them

Selección correcta
Utilize Amazon SageMaker’s blue/green deployment strategy to shift traffic gradually from the old model to the new one, ensuring that you can monitor performance and quickly revert if needed

Explicación general
Correct options:

Use Amazon SageMaker’s built-in versioning to manage different versions of the model, and deploy the new version in a canary release by redirecting a small percentage of traffic to it initially

Amazon SageMaker supports model versioning, which is crucial for tracking different iterations of your model. A canary release allows you to deploy the new model version to a small portion of users, minimizing risk by limiting exposure in case of issues. If the new version performs well, you can gradually increase traffic to it.

Utilize Amazon SageMaker’s blue/green deployment strategy to shift traffic gradually from the old model to the new one, ensuring that you can monitor performance and quickly revert if needed

A blue/green deployment strategy is a best practice in model deployment. It allows you to deploy the new model version in parallel with the existing one, gradually shifting traffic to the new version while monitoring its performance. If issues are detected, you can quickly roll back to the previous version without disrupting service.

In a blue/green deployment, SageMaker provisions a new fleet with the updates (the green fleet). Then, SageMaker shifts traffic from the old fleet (the blue fleet) to the green fleet. Once the green fleet operates smoothly for a set evaluation period (called the baking period), SageMaker terminates the blue fleet. You can specify Amazon CloudWatch alarms that SageMaker uses to monitor the green fleet. If an issue with the updated code trips any of the alarms, SageMaker initiates an auto-rollback to the blue fleet in order to maintain availability thereby minimizing risk.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html

Incorrect options:

Deploy the new model version immediately and redirect 100% of traffic to it, assuming it has been thoroughly tested and will not require a rollback - Redirecting 100% of traffic to the new model version immediately is risky, especially in a critical application like healthcare. Without a rollback plan, any issues with the new version could lead to significant consequences.

Create a backup of the current model, deploy the new version, and if any issues arise, manually roll back by redeploying the previous model version - While creating a backup and manually rolling back is a possible strategy, it is not as efficient or reliable as using a built-in rollback feature like blue/green deployment or canary releases. Manual rollbacks can lead to delays and increased downtime.

Deploy the new model version alongside the current one, and use Amazon SageMaker’s multi-model endpoint to serve both models simultaneously, splitting traffic between them - While using a multi-model endpoint could serve both models simultaneously, it is not the best approach for managing risk during deployment. This strategy is more suited for scenarios where you need to serve multiple models for different purposes rather than managing a controlled rollout.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-blue-green.html

https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails-rolling.html

https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 41

A healthcare company is building an AI application to predict patient readmission rates using Amazon SageMaker. The application must support end-to-end machine learning workflows, including data preprocessing, model training, version management, and deployment. The training data, stored securely in Amazon S3, must be used in isolated and secure environments to comply with regulatory requirements. As part of model experimentation, the data science team is running multiple training jobs back-to-back to test different hyperparameter configurations.

To improve the team’s productivity, the company needs to reduce the startup time for each consecutive training job. What is the most efficient solution to achieve this goal?

Respuesta correcta
Enable SageMaker Warm Pools to reuse training instances between jobs

Use Amazon EC2 On-Demand Instances with pre-configured AMIs for SageMaker

Use Amazon SageMaker Managed Spot Training for faster resource allocation

Use SageMaker Training Compiler to minimize data transfer latency

Explicación general
Correct option:

Enable SageMaker Warm Pools to reuse training instances between jobs

Amazon SageMaker Warm Pools allow reuse of ML compute infrastructure between consecutive training jobs. This significantly reduces startup times because instances remain warm and do not require new provisioning or configuration. Warm Pools work seamlessly with SageMaker training jobs, helping minimize infrastructure startup overhead while ensuring the infrastructure is reused securely and efficiently. This is ideal for use cases where consecutive training jobs are frequent, as in experimentation workflows.

Key Benefits:

Reduces time spent on infrastructure provisioning.

Optimizes compute resource utilization for iterative training.

Supports secure training job execution as it integrates with SageMaker's role-based permissions.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html

Incorrect options:

Use Amazon SageMaker Managed Spot Training for faster resource allocation - Managed Spot Training in Amazon SageMaker is a cost-saving feature that uses spare EC2 capacity to run training jobs at a reduced cost. However, spot instances are not guaranteed to always be available and may lead to longer infrastructure startup times due to capacity provisioning delays. It does not minimize startup times and is not ideal for consecutive training jobs requiring quick infrastructure readiness.

Use Amazon EC2 On-Demand Instances with pre-configured AMIs for SageMaker - While pre-configured AMIs can reduce configuration time, EC2 On-Demand Instances still require provisioning at the start of each job. SageMaker Warm Pools are specifically designed to minimize startup times for training jobs.

Use SageMaker Training Compiler to minimize data transfer latency - The SageMaker Training Compiler is used to optimize deep learning training workloads by accelerating model training and reducing costs. While it improves training performance, it does not address infrastructure startup times or reduce the time required to initialize the training environment.

References:

https://aws.amazon.com/blogs/machine-learning/best-practices-for-amazon-sagemaker-training-managed-warm-pools/

https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 42

A financial institution has deployed a machine learning model using Amazon SageMaker to predict whether credit card transactions are fraudulent. To ensure model performance remains consistent, the company configured Amazon SageMaker Model Monitor to track deviations in the model accuracy over time. The model's baseline accuracy was recorded during its initial deployment. However, after several months of operation, the model’s accuracy drops significantly despite no changes being made to the model.

What could be the reason for the reduced model accuracy?

The model’s hyperparameters have automatically adjusted during inference, causing a decline in predictive accuracy

The deployed model’s architecture degraded over time, reducing its predictive performance

Amazon SageMaker Model Monitor miscalculated the accuracy metric due to a configuration error

Respuesta correcta
Concept drift occurred in the underlying customer data that was used for predictions, changing the relationship between input features and the target variable over time

Explicación general
Correct option:

Concept drift occurred in the underlying customer data that was used for predictions, changing the relationship between input features and the target variable over time

Concept drift happens when the relationship between input features (e.g., customer transaction patterns, demographics, or usage behaviors) and the target variable (e.g., fraud detection or churn likelihood) changes over time. For instance:

Customer behavior might evolve, resulting in new patterns that were not present in the original training dataset.

Fraud tactics may have changed, altering the model’s assumptions and causing it to make inaccurate predictions.

While the model may still receive input data that appears valid, the patterns it learned during training no longer match real-world behaviors. This leads to a significant drop in predictive performance, such as accuracy or F1 score.

Automate model retraining with Amazon SageMaker Pipelines when drift is detected:  via - https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/

Incorrect options:

The model’s hyperparameters have automatically adjusted during inference, causing a decline in predictive accuracy - Hyperparameters do not automatically adjust in production. SageMaker deployments use a static model until explicitly retrained or updated.

Amazon SageMaker Model Monitor miscalculated the accuracy metric due to a configuration error - SageMaker Model Monitor accurately captures metrics unless the input configuration is incorrect, which is unlikely if it previously worked as expected.

The deployed model’s architecture degraded over time, reducing its predictive performance - Model architectures do not degrade over time. Reduced accuracy typically results from data drift or concept drift, not the model itself.

Reference:

https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 43

You are a machine learning engineer at an e-commerce company that uses a recommendation model to suggest products to customers. The model was trained on data from the past year, but after being in production for several months, you notice that the model's recommendations are becoming less relevant. You suspect that either data drift or model drift could be causing the decline in performance. To investigate and resolve the issue, you need to understand the difference between these two types of drift and how to monitor them using Amazon SageMaker.

Which of the following statements BEST describes the difference between data drift and model drift, and how you would address them using Amazon SageMaker?

Data drift is a sudden change in the model’s accuracy, while model drift is a gradual degradation in model performance. You should use SageMaker Feature Store to manage both types of drift by standardizing input data

Respuesta correcta
Data drift occurs when the distribution of the input data changes over time, while model drift happens when the model’s underlying assumptions or parameters become outdated. To address data drift, you should use SageMaker Model Monitor to track changes in input data distribution. For model drift, you should periodically retrain the model using the latest data

Data drift refers to changes in the model’s accuracy due to shifts in the data, while model drift refers to changes in the underlying data features over time. To address both, you should use SageMaker Clarify to detect bias and retrain the model monthly

Data drift occurs when the model’s predictions start to deviate from the expected outcomes, while model drift occurs when the model's accuracy declines due to changes in the input data. SageMaker Pipelines should be used to automate retraining for both types of drift

Explicación general
Correct option:

Data drift occurs when the distribution of the input data changes over time, while model drift happens when the model’s underlying assumptions or parameters become outdated. To address data drift, you should use SageMaker Model Monitor to track changes in input data distribution. For model drift, you should periodically retrain the model using the latest data

This option correctly defines data drift as changes in the distribution of the input data over time, which can lead to the model receiving data that is different from what it was trained on.

Model drift, on the other hand, occurs when the model’s performance degrades because its assumptions or parameters no longer align with the real-world data. SageMaker Model Monitor can be used to detect data drift by tracking changes in data distribution, while model drift is addressed by retraining the model with updated data.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

Incorrect options:

Data drift refers to changes in the model’s accuracy due to shifts in the data, while model drift refers to changes in the underlying data features over time. To address both, you should use SageMaker Clarify to detect bias and retrain the model monthly - This option incorrectly describes data drift as related to model accuracy and model drift as related to changes in data features. SageMaker Clarify is used for bias detection, not specifically for drift detection.

Data drift occurs when the model’s predictions start to deviate from the expected outcomes, while model drift occurs when the model's accuracy declines due to changes in the input data. SageMaker Pipelines should be used to automate retraining for both types of drift - This option incorrectly defines data drift and model drift. Data drift is about changes in data distribution, not prediction deviations. SageMaker Pipelines is for automating ML workflows, not directly for drift detection.

Data drift is a sudden change in the model’s accuracy, while model drift is a gradual degradation in model performance. You should use SageMaker Feature Store to manage both types of drift by standardizing input data - This option misinterprets the nature of data and model drift. SageMaker Feature Store is used for managing and serving features, not for directly addressing drift.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 44

You are a Data Scientist working for an e-commerce company that is developing a machine learning model to predict whether a customer will make a purchase based on their browsing behavior. You need to evaluate the model's performance using different evaluation metrics to understand how well the model is predicting the positive class (i.e., customers who will make a purchase). The dataset is imbalanced, with a small percentage of customers making a purchase. Given this context, you must decide on the most appropriate evaluation techniques to assess your model's effectiveness and identify potential areas for improvement.

Which of the following evaluation techniques and metrics should you prioritize when assessing the performance of your model, considering the dataset's imbalance and the need for a comprehensive understanding of both false positives and false negatives? (Select two)

Selección correcta
Use precision and recall to focus on the model's ability to correctly identify positive cases while minimizing false positives and false negatives

Utilize the AUC-ROC curve to evaluate the model’s ability to distinguish between classes across various thresholds, particularly in the presence of class imbalance

Selección correcta
Evaluate the model using the confusion matrix, which provides insights into true positives, false positives, true negatives, and false negatives, allowing you to calculate additional metrics such as precision, recall, and F1 score

Use accuracy as the primary metric, as it measures the percentage of correct predictions out of all predictions made by the model

Prioritize Root mean squared error (RMSE) as the key metric, as it measures the average magnitude of the errors between predicted and actual values

Explicación general
Correct options:

Evaluate the model using the confusion matrix, which provides insights into true positives, false positives, true negatives, and false negatives, allowing you to calculate additional metrics such as precision, recall, and F1 score

The confusion matrix illustrates in a table the number or percentage of correct and incorrect predictions for each class by comparing an observation's predicted class and its true class. The confusion matrix is crucial for understanding the detailed performance of your model, especially in an imbalanced dataset. It allows you to calculate additional metrics such as precision, recall, and F1 score, which are essential for understanding how well your model handles false positives and false negatives.

Use precision and recall to focus on the model's ability to correctly identify positive cases while minimizing false positives and false negatives

Precision and recall are particularly important in an imbalanced dataset. Precision measures the proportion of true positive predictions among all positive predictions, while recall measures the proportion of actual positives that are correctly identified. Focusing on these metrics helps in assessing how well the model avoids false positives and false negatives, which is critical in your scenario.

Key metrics to measure machine learning model performance:





via - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Incorrect options:

Use accuracy as the primary metric, as it measures the percentage of correct predictions out of all predictions made by the model - While accuracy is a common metric, it is not suitable for imbalanced datasets because it can be misleading. A model predicting the majority class most of the time can achieve high accuracy without effectively capturing the minority class (e.g., customers who make a purchase).

Prioritize Root mean squared error (RMSE) as the key metric, as it measures the average magnitude of the errors between predicted and actual values - RMSE is a regression metric, not suitable for classification problems. In this scenario, you are dealing with a classification task, so metrics like precision, recall, and F1 score are more appropriate.

Utilize the AUC-ROC curve to evaluate the model’s ability to distinguish between classes across various thresholds, particularly in the presence of class imbalance - The AUC-ROC curve is a useful tool, especially in imbalanced datasets. However, understanding the confusion matrix and calculating precision and recall provide more direct insights into the types of errors the model is making, which is crucial for improving the model’s performance in your specific context.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

https://docs.aws.amazon.com/machine-learning/latest/dg/binary-classification.html

Temática
ML Model Development
Pregunta 45

You are a data scientist at a healthcare startup tasked with developing a machine learning model to predict the likelihood of patients developing a specific chronic disease within the next five years. The dataset available includes patient demographics, medical history, lab results, and lifestyle factors, but it is relatively small, with only 1,000 records. Additionally, the dataset has missing values in some critical features, and the class distribution is highly imbalanced, with only 5% of patients labeled as having developed the disease.

Given the data limitations and the complexity of the problem, which of the following approaches is the MOST LIKELY to determine the feasibility of an ML solution and guide your next steps?

Respuesta correcta
Conduct exploratory data analysis (EDA) to understand the data distribution, address missing values, and assess the class imbalance before determining if an ML solution is feasible

Increase the dataset size by generating synthetic data and then train a simple logistic regression model to avoid overfitting

Proceed with training a deep neural network (DNN) model using the available data, as DNNs can handle small datasets by learning complex patterns

Immediately apply an oversampling technique to balance the dataset, then train an XGBoost model to maximize performance on the minority class

Explicación general
Correct option:

Conduct exploratory data analysis (EDA) to understand the data distribution, address missing values, and assess the class imbalance before determining if an ML solution is feasible

Conducting exploratory data analysis (EDA) is the most appropriate first step. EDA allows you to understand the data distribution, identify and address missing values, and assess the extent of the class imbalance. This process helps determine whether the available data is sufficient to build a reliable model and what preprocessing steps might be necessary.

 via - https://aws.amazon.com/blogs/machine-learning/exploratory-data-analysis-feature-engineering-and-operationalizing-your-data-flow-into-your-ml-pipeline-with-amazon-sagemaker-data-wrangler/

Incorrect options:

Proceed with training a deep neural network (DNN) model using the available data, as DNNs can handle small datasets by learning complex patterns - Training a deep neural network on a small dataset is not advisable, as DNNs typically require large amounts of data to perform well and avoid overfitting. Additionally, jumping directly to model training without assessing the data first may lead to poor results.

Increase the dataset size by generating synthetic data and then train a simple logistic regression model to avoid overfitting - While generating synthetic data can help increase the dataset size, it may introduce biases if not done carefully. Additionally, without first understanding the data through EDA, you risk applying the wrong strategy or misinterpreting the results.

Immediately apply an oversampling technique to balance the dataset, then train an XGBoost model to maximize performance on the minority class - Although oversampling can address class imbalance, it’s important to first understand the underlying data issues through EDA. Oversampling should not be the immediate next step without understanding the data quality, feature importance, and potential need for feature engineering.

References:

https://aws.amazon.com/blogs/machine-learning/exploratory-data-analysis-feature-engineering-and-operationalizing-your-data-flow-into-your-ml-pipeline-with-amazon-sagemaker-data-wrangler/

https://aws.amazon.com/blogs/machine-learning/use-amazon-sagemaker-canvas-for-exploratory-data-analysis/

Temática
ML Model Development
Pregunta 46

You are an ML Engineer working for a logistics company that uses multiple machine learning models to optimize delivery routes in real-time. Each model needs to process data quickly to provide up-to-the-minute route adjustments, but the company also has strict cost constraints. You need to deploy the models in an environment where performance, cost, and latency are carefully balanced. There may be slight variations in the access frequency of the models. Any excessive costs could impact the project’s profitability.

Which of the following strategies should you consider to balance the tradeoffs between performance, cost, and latency when deploying your model in Amazon SageMaker? (Select two)

Deploy the model on a high-performance GPU instance to minimize latency, regardless of the higher cost, ensuring real-time route adjustments

Selección correcta
Use Amazon SageMaker’s multi-model endpoint to deploy multiple models on a single instance, reducing costs by sharing resources

Selección correcta
Implement auto-scaling on a fleet of medium-sized instances, allowing the system to adjust resources based on real-time demand, balancing cost and performance dynamically

Choose a lower-cost CPU instance, accepting longer inference times, as the savings on compute costs are more important than minimizing latency

Leverage Amazon SageMaker Neo to compile the model for optimized deployment on edge devices, reducing latency and cost but with limited scalability for large datasets

Explicación general
Correct options:

Use Amazon SageMaker’s multi-model endpoint to deploy multiple models on a single instance, reducing costs by sharing resources

Amazon SageMaker’s multi-model endpoint allows you to deploy multiple models on a single instance. This can significantly reduce costs by sharing resources among models, but it may introduce slight increases in latency due to the need to load the correct model into memory. This tradeoff can be acceptable if cost savings are a priority and latency requirements are not ultra-strict.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

Implement auto-scaling on a fleet of medium-sized instances, allowing the system to adjust resources based on real-time demand, balancing cost and performance dynamically

Auto-scaling allows you to dynamically adjust the number of instances based on demand, which helps balance performance and cost. During peak times, more instances can be provisioned to maintain low latency, while during off-peak times, fewer instances are used, reducing costs. This strategy offers a flexible way to manage the tradeoffs between performance, cost, and latency.

Incorrect options:

Deploy the model on a high-performance GPU instance to minimize latency, regardless of the higher cost, ensuring real-time route adjustments - While deploying on a high-performance GPU instance would minimize latency, it may not be cost-effective, especially if the model does not require the full computational power of a GPU. The high cost might outweigh the benefits of lower latency.

Choose a lower-cost CPU instance, accepting longer inference times, as the savings on compute costs are more important than minimizing latency - Choosing a lower-cost CPU instance could lead to unacceptable delays in route adjustments, which could impact delivery times. In this scenario, optimizing latency is critical, and sacrificing performance for cost could be detrimental to the business.

Leverage Amazon SageMaker Neo to compile the model for optimized deployment on edge devices, reducing latency and cost but with limited scalability for large datasets - While Amazon SageMaker Neo can optimize models for deployment on edge devices, it is not the best fit for this scenario. Neo is more suitable for low-latency, cost-effective deployments on devices with limited resources. In this scenario, the need for scalable, cloud-based infrastructure is more important.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 47

A retail company trained an ML model on Amazon SageMaker to detect damaged products from warehouse surveillance images. The training dataset was created using SageMaker Data Wrangler, which included images of damaged and undamaged products. During training and validation, the model achieved high accuracy; however, in production, the model’s performance is degraded due to differences in lighting conditions and image quality across various warehouse locations. The ML engineer needs to improve the model's accuracy to handle variations in image quality with the LEAST amount of time and effort.

What do you recommend?

Respuesta correcta
Use SageMaker Data Wrangler’s corrupt image transform to preprocess the training data by simulating variations in image quality for robust model training

Deploy a preprocessing pipeline in Amazon SageMaker to filter out low-quality images before sending them to the model for inference

Use SageMaker Data Wrangler’s outlier detection transform to remove outlier images from the dataset and improve the model’s performance

Fine-tune the model by manually collecting additional data from various cameras and retrain it with the new dataset

Explicación general
Correct option:

Use SageMaker Data Wrangler’s corrupt image transform to preprocess the training data by simulating variations in image quality for robust model training

SageMaker Data Wrangler’s corrupt image transform is specifically designed to simulate real-world image imperfections, such as noise, blurriness, or resolution changes, during the preprocessing stage. By applying this transformation to the training dataset:

The model learns to handle variations in image quality, making it more robust in production.

The solution requires minimal time because it builds on the existing training pipeline and does not require extensive data collection or custom scripts.

The approach enhances model generalization without the need for a complete retraining process from scratch.

Key Benefits of Corrupt Image Transform:

Simulates real-world image imperfections, improving robustness.

Requires no manual data collection or custom preprocessing scripts.

Integrates seamlessly with the existing SageMaker Data Wrangler workflow.

Prepare image data with Amazon SageMaker Data Wrangler:  via - https://aws.amazon.com/blogs/machine-learning/prepare-image-data-with-amazon-sagemaker-data-wrangler/

Incorrect options:

Deploy a preprocessing pipeline in Amazon SageMaker to filter out low-quality images before sending them to the model for inference - Filtering low-quality images could result in the loss of important information, which may decrease model accuracy rather than improve it.

Fine-tune the model by manually collecting additional data from various cameras and retrain it with the new dataset - Collecting additional data and retraining is time-consuming and requires significant manual effort. It is not the fastest solution for improving the model.

Use SageMaker Data Wrangler’s outlier detection transform to remove outlier images from the dataset and improve the model’s performance - While the outlier detection transform in SageMaker Data Wrangler is useful for identifying and removing anomalous data points in numerical datasets, it is not designed for handling image quality variations. Removing outliers based on predefined metrics would not address the issue of poor model performance due to lighting and quality inconsistencies in production images. This approach could even lead to a loss of valuable data.

Reference:

https://aws.amazon.com/blogs/machine-learning/prepare-image-data-with-amazon-sagemaker-data-wrangler/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 48

A logistics company is building a delivery time prediction model on AWS to estimate the number of hours it will take for packages to reach their destination. The dataset includes information such as distance traveled, traffic conditions, and package weight. The model outputs a continuous numerical value representing the estimated delivery time in hours. The ML engineer needs to evaluate the model’s performance to determine how accurately it predicts delivery times.

Which metric should the ML engineer use to evaluate the model's performance?

Precision

Respuesta correcta
Mean Absolute Error (MAE)

Accuracy

Area Under the ROC Curve (AUC-ROC)

Explicación general
Correct option:

Mean Absolute Error (MAE)

Mean Absolute Error (MAE) is a suitable metric for regression problems where the goal is to predict continuous numerical values. MAE calculates the average absolute difference between the predicted and actual values, providing an interpretable measure of model error. Unlike MSE, MAE does not square the errors, so it is less sensitive to outliers and provides a straightforward interpretation in the same units as the target variable.

Metrics used to measure machine learning model performance:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Incorrect options:

Precision - Precision measures how well an algorithm predicts the true positives (TP) out of all of the positives that it identifies. It is defined as follows: Precision = TP/(TP+FP), with values ranging from zero to one, and is used in binary classification.

Accuracy - Accuracy is the ratio of the number of correctly classified items to the total number of (correctly and incorrectly) classified items. It is used for both binary and multiclass classification.

Area Under the ROC Curve (AUC-ROC) - AUC-ROC is used to evaluate the performance of binary classification models. It measures how well the model distinguishes between classes, which is irrelevant for regression.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Temática
ML Model Development
Pregunta 49

You are a lead machine learning engineer at a growing tech startup that is developing a recommendation system for a mobile app. The recommendation engine must be able to scale quickly as the user base grows, remain cost-effective to align with the startup’s budget constraints, and be easy to maintain by a small team of engineers. The company has decided to use AWS for the ML infrastructure. Your goal is to design an infrastructure that meets these needs, ensuring that it can handle rapid scaling, remains within budget, and is simple to update and monitor.

Which combination of practices and AWS services is MOST LIKELY to result in a maintainable, scalable, and cost-effective ML infrastructure?

Train models using Amazon EMR for cost efficiency, deploy the models using AWS Lambda for serverless inference, and manually monitor the system using CloudWatch to reduce operational overhead

Implement Amazon SageMaker for model training, deploy the models using Amazon EC2 with manual scaling to handle inference, and use AWS CloudFormation for managing infrastructure as code to ensure repeatability

Use Amazon SageMaker for training, deploy models on Amazon ECS for flexible scaling, and implement infrastructure monitoring with a combination of CloudWatch and AWS Systems Manager to ensure maintainability

Respuesta correcta
Use Amazon SageMaker for both training and deployment, leverage auto-scaling endpoints for real-time inference, and apply SageMaker Pipelines for orchestrating end-to-end ML workflows, ensuring scalability and automation

Explicación general
Correct option:

Use Amazon SageMaker for both training and deployment, leverage auto-scaling endpoints for real-time inference, and apply SageMaker Pipelines for orchestrating end-to-end ML workflows, ensuring scalability and automation

Amazon SageMaker provides a managed service for both training and deployment, which simplifies the infrastructure and reduces operational overhead. Auto-scaling endpoints in SageMaker ensure the system can handle increasing demand without manual intervention. SageMaker Pipelines automates the entire ML workflow, enabling continuous integration and delivery (CI/CD) practices, making the infrastructure scalable, maintainable, and cost-effective.

Incorrect options:

Implement Amazon SageMaker for model training, deploy the models using Amazon EC2 with manual scaling to handle inference, and use AWS CloudFormation for managing infrastructure as code to ensure repeatability - Using Amazon SageMaker for training and Amazon EC2 for inference with manual scaling can work, but it requires more effort to manage scaling, and manually managing infrastructure is less maintainable. Auto-scaling and automation would be more effective for a growing startup.

Train models using Amazon EMR for cost efficiency, deploy the models using AWS Lambda for serverless inference, and manually monitor the system using CloudWatch to reduce operational overhead - While Amazon EMR is cost-effective for big data processing, it’s not optimized for ML model training in the same way that SageMaker is. AWS Lambda is useful for serverless inference but may not scale effectively for high-volume, real-time recommendations. Manual monitoring adds operational overhead.

Use Amazon SageMaker for training, deploy models on Amazon ECS for flexible scaling, and implement infrastructure monitoring with a combination of CloudWatch and AWS Systems Manager to ensure maintainability - Amazon ECS offers flexible scaling, but SageMaker’s auto-scaling capabilities and built-in integration with ML workflows make it more suitable for this use case. Additionally, SageMaker Pipelines offers better orchestration for ML tasks compared to a manually managed solution.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 50

You are a data scientist at a pharmaceutical company that builds predictive models to analyze clinical trial data. Due to regulatory requirements, the company must maintain strict version control of all models used in decision-making processes. This includes tracking which data, hyperparameters, and code were used to train each model, as well as ensuring that models can be easily reproduced and audited in the future. You decide to implement a system to manage model versions and track their lifecycle effectively.

Which of the following strategies is the MOST LIKELY to ensure model versioning, repeatability, and auditability?

Create a version control system in Git for the model’s training code and configuration files, while storing the trained models in a separate S3 bucket for easy retrieval

Use SageMaker Model Monitor to track the performance of models in production, ensuring that any changes in model behavior are documented for future audits

Respuesta correcta
Leverage the SageMaker Model Registry to register, track, and manage different versions of models, capturing all relevant metadata, including data sources, hyperparameters, and training code

Use Amazon S3 to store each version of the model manually, tagging the stored files with metadata about the training data, hyperparameters, and code used for training

Explicación general
Correct option:

Leverage the SageMaker Model Registry to register, track, and manage different versions of models, capturing all relevant metadata, including data sources, hyperparameters, and training code

The SageMaker Model Registry is specifically designed for managing model versions in a systematic and organized manner. It allows you to register different versions of a model, track metadata such as data sources, hyperparameters, and training code, and ensure that each version is easily reproducible. This approach is ideal for regulatory environments where audit trails and model governance are critical.

With the Amazon SageMaker Model Registry you can do the following:

Catalog models for production.

Manage model versions.

Associate metadata, such as training metrics, with a model.

View information from Amazon SageMaker Model Cards in your registered models.

Manage the approval status of a model.

Deploy models to production.

Automate model deployment with CI/CD.

Share models with other users.

Incorrect options:

Use Amazon S3 to store each version of the model manually, tagging the stored files with metadata about the training data, hyperparameters, and code used for training - While using Amazon S3 to store model versions with metadata is possible, it requires a lot of manual effort and lacks the automated tracking and management capabilities needed for comprehensive version control, repeatability, and auditability.

Create a version control system in Git for the model’s training code and configuration files, while storing the trained models in a separate S3 bucket for easy retrieval - Using Git for version control of the training code and configurations is a good practice, but it does not address the need to manage the actual trained models and their associated metadata systematically. The SageMaker Model Registry offers a more comprehensive solution that integrates both code and model versioning.

Use SageMaker Model Monitor to track the performance of models in production, ensuring that any changes in model behavior are documented for future audits - SageMaker Model Monitor is useful for monitoring model performance in production, but it does not handle version control or track the metadata necessary for repeatability and audits. It is complementary to, but not a substitute for, the SageMaker Model Registry.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

Temática
ML Model Development
Pregunta 51

A company’s data science team uses Amazon SageMaker notebook instances to develop machine learning models. The team frequently collaborates on projects that require access to shared datasets and specific Amazon S3 buckets. Currently, permissions for accessing S3 buckets are managed by creating individual IAM roles for each SageMaker notebook instance. This decentralized approach has led to inconsistent permissions, duplication of effort, and difficulty in managing access across team members. The company wants to centralize permissions management to ensure all SageMaker notebook instances used by the team can access the required S3 buckets consistently and efficiently.

Which solution will meet this requirement?

Attach the necessary permissions directly to each data scientist's IAM user to enable granular control over access to the notebook instances

Respuesta correcta
Attach a single IAM role with S3 permissions to all SageMaker notebook instances used by the data science team

Use inline policies directly on each notebook instance to define local permissions for the data scientists

Create an IAM group for the data science team, associate the required S3 access policies to the group, and attach the IAM group directly to the SageMaker notebook instances to grant permissions to all data scientists

Explicación general
Correct option:

Attach a single IAM role with S3 permissions to all SageMaker notebook instances used by the data science team

The correct way to centralize access to Amazon S3 buckets for SageMaker notebook instances is to:

Create a single IAM role with the required S3 access permissions.

Attach this role to all SageMaker notebook instances used by the data science team.

IAM roles are specifically designed to grant AWS service resources (like SageMaker notebook instances) secure, temporary access to other AWS services. This ensures:

Consistent permissions across all team members.

Simplified management of access policies without duplicating roles.

Scalability as new SageMaker notebook instances are created.

Incorrect options:

Create an IAM group for the data science team, associate the required S3 access policies to the group, and attach the IAM group directly to the SageMaker notebook instances to grant permissions to all data scientists - IAM groups cannot be directly attached to Amazon SageMaker notebook instances or any AWS resources. IAM groups are used to organize IAM users and simplify permission management by attaching policies to the group, which are then inherited by its members. To grant permissions to resources like SageMaker notebook instances, you must use IAM roles or directly attach policies to individual IAM users. This option incorrectly implies that an IAM group can be attached to a notebook instance, which is not supported in AWS.

Attach the necessary permissions directly to each data scientist's IAM user to enable granular control over access to the notebook instances - Assigning policies directly to IAM users creates decentralized management, which becomes difficult to maintain as the team grows.

Use inline policies directly on each notebook instance to define local permissions for the data scientists - This option acts as a distractor. An inline policy is a policy created for a single IAM identity (a user, user group, or role). It cannot be directly attached to a resource, such as the SageMaker notebook instance.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 52

Which AWS service is used to store, share and manage inputs to Machine Learning models used during training and inference?

Amazon SageMaker Ground Truth

Amazon SageMaker Data Wrangler

Amazon SageMaker Clarify

Respuesta correcta
Amazon SageMaker Feature Store

Explicación general
Correct option:

Amazon SageMaker Feature Store

Amazon SageMaker Feature Store is a fully managed, purpose-built repository to store, share, and manage features for machine learning (ML) models. Features are inputs to ML models used during training and inference. For example, in an application that recommends a music playlist, features could include song ratings, listening duration, and listener demographics.

You can ingest data into SageMaker Feature Store from a variety of sources, such as application and service logs, clickstreams, sensors, and tabular data from Amazon Simple Storage Service (Amazon S3), Amazon Redshift, AWS Lake Formation, Snowflake, and Databricks Delta Lake.

How Feature Store works:  via - https://aws.amazon.com/sagemaker/feature-store/

Incorrect options:

Amazon SageMaker Clarify - SageMaker Clarify helps identify potential bias during data preparation without writing code. You specify input features, such as gender or age, and SageMaker Clarify runs an analysis job to detect potential bias in those features.

Amazon SageMaker Data Wrangler - Amazon SageMaker Data Wrangler reduces the time it takes to aggregate and prepare tabular and image data for ML from weeks to minutes. With SageMaker Data Wrangler, you can simplify the process of data preparation and feature engineering, and complete each step of the data preparation workflow (including data selection, cleansing, exploration, visualization, and processing at scale) from a single visual interface.

Amazon SageMaker Ground Truth - Amazon SageMaker Ground Truth offers the most comprehensive set of human-in-the-loop capabilities, allowing you to harness the power of human feedback across the ML lifecycle to improve the accuracy and relevancy of models. You can complete a variety of human-in-the-loop tasks with SageMaker Ground Truth, from data generation and annotation to model review, customization, and evaluation, either through a self-service or an AWS-managed offering.

References:

https://aws.amazon.com/sagemaker/feature-store/

https://aws.amazon.com/sagemaker/groundtruth

https://aws.amazon.com/sagemaker/clarify/

https://aws.amazon.com/sagemaker/data-wrangler/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 53

An e-commerce company’s ML engineer is tasked with building a machine learning model to predict customer purchase behavior. To ensure consistency and reusability of features, the engineer decides to use Amazon SageMaker Feature Store to create, manage, and store features for training and inference. Given this context, consider the following steps:

Prepare training dataset by accessing the feature data from the store

Load the feature data into the store.

Set up a feature group to organize and store features.

What should be the correct order of steps to create and use the features in Amazon SageMaker Feature Store?

Respuesta correcta
3,2,1

3,1,2

2,3,1

1,2,3

Explicación general
Correct option:

3,2,1

Set up a feature group to organize and store features.
A feature group is a logical grouping of features, which is the foundation of the SageMaker Feature Store. It defines the schema of the data, such as feature names, types, and metadata. Creating a feature group is the first step to structure and organize features for

Load the feature data into the store.
After the feature group is created, the engineer must ingest records into the feature group. This involves writing data (features and their values) into the Feature Store. Data can be ingested in either the online store for low-latency inference or the offline store for model training.

Prepare training dataset by accessing the feature data from the store
Once the records are ingested, the engineer can query the offline store to retrieve historical feature data for building datasets. These datasets can then be used to train the ML model.

Incorrect options:

1,2,3

2,3,1

3,1,2

These three options contradict the explanation provided above, so these options are incorrect.

References:

https://aws.amazon.com/sagemaker-ai/feature-store/

https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_FeatureGroup.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 54

You are a data scientist working on a binary classification model to predict whether customers will default on their loans. The dataset is highly imbalanced, with only 10% of the customers having defaulted in the past. After training the model, you need to evaluate its performance to ensure it effectively distinguishes between defaulters and non-defaulters. Given the class imbalance, accuracy alone is not sufficient to assess the model’s performance. Instead, you decide to use the Receiver Operating Characteristic (ROC) curve and the Area Under the ROC Curve (AUC) to evaluate the model.

Which of the following interpretations of the ROC and AUC metrics is MOST ACCURATE for assessing the model’s performance?

Respuesta correcta
An AUC close to 1.0 indicates that the model has excellent discriminatory power, effectively distinguishing between defaulters and non-defaulters

A ROC curve that is close to the diagonal line (AUC ~ 0.5) indicates that the model performs well across all thresholds

A ROC curve that is closer to the top-left corner of the plot (AUC ~ 1) shows that the model is overfitting, and its predictions are too optimistic

An AUC close to 0 indicates that the model is highly accurate, correctly classifying almost all instances of defaulters and non-defaulters

Explicación general
Correct option:

An AUC close to 1.0 indicates that the model has excellent discriminatory power, effectively distinguishing between defaulters and non-defaulters

Area Under the (Receiver Operating Characteristic) Curve (AUC) represents an industry-standard accuracy metric for binary classification models. AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples. Because it is independent of the score cut-off, you can get a sense of the prediction accuracy of your model from the AUC metric without picking a threshold.

The AUC metric returns a decimal value from 0 to 1. AUC values near 1 indicate an ML model that is highly accurate. Values near 0.5 indicate an ML model that is no better than guessing at random. Values near 0 are unusual to see, and typically indicate a problem with the data. Essentially, an AUC near 0 says that the ML model has learned the correct patterns, but is using them to make predictions that are flipped from reality ('0's are predicted as '1's and vice versa). The ROC curve is the plot of the true positive rate (TPR) against the false positive rate (FPR) at each threshold setting.

 via - https://aws.amazon.com/blogs/machine-learning/is-your-model-good-a-deep-dive-into-amazon-sagemaker-canvas-advanced-metrics/

An AUC close to 1.0 signifies that the model has excellent discriminatory power, meaning it can effectively distinguish between the positive class (defaulters) and the negative class (non-defaulters) across all thresholds. This is desirable in a classification task, especially in scenarios with class imbalance.

 via - https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html

Incorrect options:

A ROC curve that is close to the diagonal line (AUC ~ 0.5) indicates that the model performs well across all thresholds - A ROC curve close to the diagonal line (AUC ~ 0.5) indicates that the model has no discriminatory power and is performing no better than random guessing. This suggests poor model performance, not that the model performs well across all thresholds.

A ROC curve that is closer to the top-left corner of the plot (AUC ~ 1) shows that the model is overfitting, and its predictions are too optimistic - A ROC curve closer to the top-left corner of the plot (AUC closer to 1.0) indicates strong model performance, not overfitting. Overfitting is typically identified by other indicators, such as a large gap between training and validation performance, not by the shape of the ROC curve alone.

An AUC close to 0 indicates that the model is highly accurate, correctly classifying almost all instances of defaulters and non-defaulters - An AUC close to 0 is problematic, as it indicates that the model is consistently making incorrect predictions (i.e., it classifies negatives as positives and vice versa). A high AUC (close to 1) is what signifies strong model performance.

References:

https://docs.aws.amazon.com/machine-learning/latest/dg/binary-model-insights.html

https://aws.amazon.com/blogs/machine-learning/creating-high-quality-machine-learning-models-for-financial-services-using-amazon-sagemaker-autopilot/

https://aws.amazon.com/blogs/machine-learning/is-your-model-good-a-deep-dive-into-amazon-sagemaker-canvas-advanced-metrics/

Temática
ML Model Development
Pregunta 55

You are a data scientist at a marketing agency tasked with creating a sentiment analysis model to analyze customer reviews for a new product. The company wants to quickly deploy a solution with minimal training time and development effort. You decide to leverage a pre-trained natural language processing (NLP) model and fine-tune it using a custom dataset of labeled customer reviews. Your team has access to both Amazon Bedrock and SageMaker JumpStart.

Which approach is the MOST APPROPRIATE for fine-tuning the pre-trained model with your custom dataset?

Use SageMaker JumpStart to create a custom container for your pre-trained model and manually implement fine-tuning with TensorFlow

Use Amazon Bedrock to train a model from scratch using your custom dataset, as Bedrock is optimized for training large models efficiently

Respuesta correcta
Use SageMaker JumpStart to deploy a pre-trained NLP model and use the built-in fine-tuning functionality with your custom dataset to create a customized sentiment analysis model

Use Amazon Bedrock to select a base foundation model from a third-party provider, then fine-tune the base model directly in the Bedrock interface using your custom dataset

Explicación general
Correct option:

Use SageMaker JumpStart to deploy a pre-trained NLP model and use the built-in fine-tuning functionality with your custom dataset to create a customized sentiment analysis model

Amazon Bedrock is the easiest way to build and scale generative AI applications with foundation models. Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select FMs quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation. SageMaker JumpStart provides managed infrastructure and tools to accelerate scalable, reliable, and secure model building, training, and deployment of ML models.

Fine-tuning trains a pretrained model on a new dataset without training from scratch. This process, also known as transfer learning, can produce accurate models with smaller datasets and less training time.

SageMaker JumpStart is specifically designed for scenarios like this, where you can quickly deploy a pre-trained model and fine-tune it using your custom dataset. This approach allows you to leverage existing NLP models, reducing both development time and computational resources needed for training from scratch.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-fine-tune.html

Incorrect options:

Use Amazon Bedrock to select a base foundation model from a third-party provider, then fine-tune the base model directly in the Bedrock interface using your custom dataset - Amazon Bedrock provides access to foundation models from third-party providers, allowing for easy deployment and integration into applications. However, Bedrock does not support fine-tuning the base model within its interface. You need to create your own private copy of the base Foundation Model and then fine-tune this copy with your custom dataset.

Use Amazon Bedrock to train a model from scratch using your custom dataset, as Bedrock is optimized for training large models efficiently - Amazon Bedrock is not intended for training models from scratch, especially not for scenarios where fine-tuning a pre-trained model would be more efficient. Bedrock is optimized for deploying and scaling foundation models, not for raw model training.

Use SageMaker JumpStart to create a custom container for your pre-trained model and manually implement fine-tuning with TensorFlow - While it’s possible to create a custom container and manually fine-tune a model, SageMaker JumpStart already offers an integrated solution for fine-tuning pre-trained models without the need for custom containers or manual implementation. This makes it a more efficient and straightforward option for the task at hand.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-fine-tune.html

Temática
ML Model Development
Pregunta 56

You are a data scientist at a financial institution tasked with building a model to detect fraudulent transactions. The dataset is highly imbalanced, with only a small percentage of transactions being fraudulent. After experimenting with several models, you decide to implement a boosting technique to improve the model’s accuracy, particularly on the minority class. You are considering different types of boosting, including Adaptive Boosting (AdaBoost), Gradient Boosting, and Extreme Gradient Boosting (XGBoost).

Given the problem context and the need to effectively handle class imbalance, which boosting technique is MOST SUITABLE for this scenario?

Implement Gradient Boosting to sequentially train weak learners, using the gradient of the loss function to improve performance on the minority class

Use Adaptive Boosting (AdaBoost) to focus on correcting the errors of weak classifiers, giving more weight to incorrectly classified instances during each iteration

Use Gradient Boosting and manually adjust the learning rate and class weights to improve performance on the minority class, avoiding the complexities of XGBoost

Respuesta correcta
Apply Extreme Gradient Boosting (XGBoost) for its ability to handle imbalanced datasets effectively through regularization, weighted classes, and optimized computational efficiency

Explicación general
Correct option:

Apply Extreme Gradient Boosting (XGBoost) for its ability to handle imbalanced datasets effectively through regularization, weighted classes, and optimized computational efficiency

The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:

Its robust handling of a variety of data types, relationships, distributions.

The variety of hyperparameters that you can fine-tune.

XGBoost is an extension of Gradient Boosting that includes additional features such as regularization, handling of missing values, and support for weighted classes, making it particularly well-suited for imbalanced datasets like fraud detection. It also offers significant computational efficiency, which is beneficial when working with large datasets.

 via - https://aws.amazon.com/what-is/boosting/

Incorrect options:

Use Adaptive Boosting (AdaBoost) to focus on correcting the errors of weak classifiers, giving more weight to incorrectly classified instances during each iteration - AdaBoost works by focusing on correcting the errors of weak classifiers, assigning more weight to misclassified instances in each iteration. However, it may struggle with noisy data and extreme class imbalance, as it can overemphasize hard-to-classify instances.

Implement Gradient Boosting to sequentially train weak learners, using the gradient of the loss function to improve performance on the minority class - Gradient Boosting is a powerful technique that uses the gradient of the loss function to improve the model iteratively. While it can be adapted to handle class imbalance, it does not inherently provide the same level of flexibility and computational optimization as XGBoost for this specific problem.

Use Gradient Boosting and manually adjust the learning rate and class weights to improve performance on the minority class, avoiding the complexities of XGBoost - While manually adjusting the learning rate and class weights in Gradient Boosting can help, XGBoost already provides built-in mechanisms to handle these challenges more effectively, including advanced regularization techniques and hyperparameter optimization.

References:

https://aws.amazon.com/what-is/boosting/

https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html

https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html

https://aws.amazon.com/blogs/gametech/fraud-detection-for-games-using-machine-learning/

https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Build_a_fraud_detection_system_with_Amazon_SageMaker_AIM359-R1.pdf

Temática
ML Model Development
Pregunta 57

You are a data scientist at a credit risk management company building a machine learning model to predict loan defaults. To ensure transparency and regulatory compliance, you need to explain how the model makes its predictions, particularly for high-stakes decisions such as loan approvals or rejections. The company wants a detailed understanding of the influence of individual features on the model’s predictions for specific customers, as well as an overall view of how features impact the model's predictions across the entire dataset.

Which of the following explanations BEST describes the differences between Shapley values and Partial Dependence Plots (PDP) in the context of model explainability, and how you might use them for this purpose?

Shapley values and PDP are both global explainability methods that show the average effect of features on model predictions. Use either method to understand overall feature importance, but Shapley values are computationally less expensive than PDP

Shapley values provide a visual interpretation of feature importance using plots, while PDP provides numeric values indicating the marginal contribution of features to the model's predictions. Use Shapley values for visual analysis and PDP for quantitative analysis

Respuesta correcta
Shapley values provide a local explanation by quantifying the contribution of each feature to the prediction for a specific instance, while PDP provides a global explanation by showing the marginal effect of a feature on the model’s predictions across the dataset. Use Shapley values to explain individual predictions and PDP to understand the model's behavior at a dataset level

Shapley values provide a global view of the model’s behavior by measuring the average effect of each feature across all instances, while PDP offers a local view by showing the effect of a single feature on the model’s prediction for a specific instance. Use Shapley values to understand overall feature importance and PDP to interpret individual predictions

Explicación general
Correct option:

Shapley values provide a local explanation by quantifying the contribution of each feature to the prediction for a specific instance, while PDP provides a global explanation by showing the marginal effect of a feature on the model’s predictions across the dataset. Use Shapley values to explain individual predictions and PDP to understand the model's behavior at a dataset level

This option correctly captures the differences between Shapley values and PDP in the context of model explainability:

Shapley values are a local interpretability method that explains individual predictions by assigning each feature a contribution score based on its marginal effect on the prediction. This method is useful for understanding the impact of each feature on a specific instance's prediction.

Partial Dependence Plots (PDP), on the other hand, provide a global view of the model’s behavior by illustrating how the predicted outcome changes as a single feature is varied across its range, holding all other features constant. PDPs help understand the overall relationship between a feature and the model output across the entire dataset.

Thus, Shapley values are suited for explaining individual decisions, while PDP is used to understand broader trends in model behavior.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-explainability.html

Incorrect options:

Shapley values provide a global view of the model’s behavior by measuring the average effect of each feature across all instances, while PDP offers a local view by showing the effect of a single feature on the model’s prediction for a specific instance. Use Shapley values to understand overall feature importance and PDP to interpret individual predictions - This option is incorrect because it reverses the concepts of local and global explainability methods. PDP is a global method, while Shapley values provide local explanations.

Shapley values and PDP are both global explainability methods that show the average effect of features on model predictions. Use either method to understand overall feature importance, but Shapley values are computationally less expensive than PDP - Both methods are not purely global explainability tools. Shapley values are computationally more intensive than PDP due to the requirement to compute contributions for each feature across multiple permutations

Shapley values provide a visual interpretation of feature importance using plots, while PDP provides numeric values indicating the marginal contribution of features to the model's predictions. Use Shapley values for visual analysis and PDP for quantitative analysis - This option is incorrect because Shapley values are not just visual; they provide quantitative contributions of features to predictions, while PDP provides visual insight into the marginal effects, not numeric values alone.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-explainability.html

Temática
ML Model Development
Pregunta 58

A company specializes in providing personalized product recommendations for e-commerce platforms. You’ve been tasked with developing a solution that can quickly generate high-quality product descriptions, tailor marketing copy based on customer preferences, and analyze customer reviews to identify trends in sentiment. Given the scale of data and the need for flexibility in choosing foundational models, you decide to use an AI service that can integrate seamlessly with your existing AWS infrastructure while also offering managed foundational models from third-party providers.

Which AWS service would best meet your requirements?

Amazon Rekognition

Amazon Personalize

Respuesta correcta
Amazon Bedrock

Amazon SageMaker

Explicación general
Correct option:

Amazon Bedrock

Amazon Bedrock is the correct choice for the given use case. It is designed to help businesses build and scale generative AI applications quickly and efficiently. Bedrock offers access to a range of pre-trained foundational models from Amazon and third-party providers like AI21 Labs, Anthropic, and Stability AI. This makes it ideal for tasks such as generating product descriptions, creating marketing copy, and performing sentiment analysis on customer reviews. Bedrock allows users to easily integrate these AI capabilities into their applications without managing the underlying infrastructure, making it a perfect fit for your business needs.

 via - https://aws.amazon.com/bedrock/faqs/

Incorrect options:

Amazon Rekognition - Amazon Rekognition is primarily used for image and video analysis, such as detecting objects, text, and activities. It is not designed for generating text or analyzing sentiment based on large datasets, so it would not meet the requirements in this scenario.

Amazon SageMaker - While Amazon SageMaker is a powerful service for building, training, and deploying machine learning models, it requires more manual setup and expertise compared to Amazon Bedrock. SageMaker would be a more appropriate choice if you needed custom models rather than leveraging pre-trained foundational models with generative AI capabilities.

Amazon Personalize - Amazon Personalize is a fully managed machine learning service that uses your data to generate item recommendations for your users. It can also generate user segments based on the users' affinity for certain items or item metadata. It also lacks the flexibility provided by Bedrock in choosing from various foundational models.

 via - https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html

References:

https://aws.amazon.com/bedrock/

https://aws.amazon.com/bedrock/faqs/

https://docs.aws.amazon.com/personalize/latest/dg/what-is-personalize.html

Temática
ML Model Development
Pregunta 59

You are a machine learning engineer working for a telecommunications company that needs to develop a predictive maintenance model. The goal is to predict when network equipment is likely to fail based on historical sensor data. The data includes features such as temperature, pressure, usage, and error rates recorded over time. The company wants to avoid unplanned downtime and optimize maintenance schedules by predicting failures just in time.

Given the nature of the data and the business objective, which Amazon SageMaker built-in algorithm is the MOST SUITABLE for this use case?

DeepAR Algorithm to forecast future equipment failures based on historical data

Linear Learner Algorithm to classify equipment status as 'healthy' or 'at risk' based on sensor readings

Respuesta correcta
Random Cut Forest (RCF) Algorithm to detect anomalies in sensor data that may indicate impending failures

Time Series K-Means Algorithm to cluster similar patterns in the sensor data and predict failures

Explicación general
Correct option:

Random Cut Forest (RCF) Algorithm to detect anomalies in sensor data that may indicate impending failures

Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. They are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the "regular" data. Including these anomalies in a data set can drastically increase the complexity of a machine learning task since the "regular" data can often be described with a simple model.

Random Cut Forest (RCF) is specifically designed for detecting anomalies in data. This algorithm excels at identifying unexpected patterns in sensor data that could indicate the early stages of equipment failure. It’s particularly well-suited for scenarios where you need to react to unusual behaviors in near-real-time.

Mapping use cases to built-in algorithms:

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html

Incorrect options:

DeepAR Algorithm to forecast future equipment failures based on historical data - DeepAR is designed for forecasting future time series data, which could be useful for predicting future equipment behavior. However, it is not primarily used for anomaly detection, which is critical for identifying unusual patterns that precede failures.

Linear Learner Algorithm to classify equipment status as 'healthy' or 'at risk' based on sensor readings - Linear Learner could be used for classification tasks, but predicting maintenance needs often involves detecting subtle anomalies rather than simple classification. Additionally, a binary classification model might not capture the complex patterns associated with potential failures.

Time Series K-Means Algorithm to cluster similar patterns in the sensor data and predict failures - Time Series K-Means can cluster similar time series patterns, but clustering alone does not provide the precision needed for real-time anomaly detection, which is crucial for predictive maintenance.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html

https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html

Temática
ML Model Development
Pregunta 60

You are a machine learning engineer at a biotech company developing a custom deep learning model for analyzing genomic data. The model relies on a specific version of TensorFlow with custom Python libraries and dependencies that are not available in the standard SageMaker environments. To ensure compatibility and flexibility, you decide to use the "Bring Your Own Container" (BYOC) approach with Amazon SageMaker for both training and inference.

Given this scenario, which steps are MOST IMPORTANT for successfully deploying your custom container with SageMaker, ensuring that it meets the company’s requirements?

Deploy the model locally using Docker, then use the AWS Management Console to manually copy the environment and model files to a SageMaker instance for training

Package the model as a SageMaker-compatible file, upload it to Amazon S3, and use a pre-built SageMaker container for training, ensuring that the training job uses the custom environment

Build a Docker container with the required TensorFlow version and dependencies, push the container image to Docker Hub, and reference the image in SageMaker when creating the training job

Respuesta correcta
Create a Docker container with the required environment, push the container image to Amazon ECR (Elastic Container Registry), and use SageMaker’s Script Mode to execute the training script within the container

Explicación general
Correct option:

Create a Docker container with the required environment, push the container image to Amazon ECR (Elastic Container Registry), and use SageMaker’s Script Mode to execute the training script within the container

Script mode enables you to write custom training and inference code while still utilizing common ML framework containers maintained by AWS.

SageMaker supports most of the popular ML frameworks through pre-built containers, and has taken the extra step to optimize them to work especially well on AWS compute and network infrastructure in order to achieve near-linear scaling efficiency. These pre-built containers also provide some additional Python packages, such as Pandas and NumPy, so you can write your own code for training an algorithm. These frameworks also allow you to install any Python package hosted on PyPi by including a requirements.txt file with your training code or to include your own code directories.

This is the correct approach for using the BYOC strategy with SageMaker. You build a Docker container that includes the required TensorFlow version and custom dependencies, then push the image to Amazon ECR. SageMaker can reference this image to create training jobs and deploy endpoints. By using Script Mode, you can execute your custom training script within the container, ensuring compatibility with your specific environment.

 via - https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/

Incorrect options:

Build a Docker container with the required TensorFlow version and dependencies, push the container image to Docker Hub, and reference the image in SageMaker when creating the training job - While Docker Hub can be used to host container images, Amazon SageMaker is optimized to work with images stored in Amazon ECR, providing better security, performance, and integration with AWS services. Additionally, using Docker Hub for production ML workloads may pose security and compliance risks.

Package the model as a SageMaker-compatible file, upload it to Amazon S3, and use a pre-built SageMaker container for training, ensuring that the training job uses the custom environment - This option describes a standard SageMaker workflow using pre-built containers, which does not provide the customization required by the BYOC approach. SageMaker pre-built containers may not support the specific custom libraries and dependencies your model requires.

Deploy the model locally using Docker, then use the AWS Management Console to manually copy the environment and model files to a SageMaker instance for training - Manually deploying the model and environment locally and then copying files to SageMaker instances is not scalable or maintainable. SageMaker BYOC allows for a more robust, automated, and integrated solution.

References:

https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/

https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 61

An ML engineer is training a time series forecasting model using a recurrent neural network (RNN) to predict electricity demand for a utility company. The model is trained using stochastic gradient descent (SGD) as the optimizer. During training, the engineer notices the following:

The training loss and validation loss remain high.

The loss values oscillate, decreasing for a few epochs and then increasing again before repeating the cycle.

The ML engineer needs to resolve this issue to stabilize the training process and improve model performance. What should the ML engineer do to improve the training process?

Increase the number of training epochs to give the model more time to learn the patterns in the data

Respuesta correcta
Reduce the learning rate to allow the gradient updates to converge more smoothly and prevent oscillations in the loss values

Apply dropout regularization to the RNN layers to improve generalization and reduce oscillations in the loss

Increase the learning rate to allow the gradient updates to converge more smoothly and prevent oscillations in the loss values

Explicación general
Correct option:

Reduce the learning rate to allow the gradient updates to converge more smoothly and prevent oscillations in the loss values

The oscillating pattern of the loss values during training and validation suggests that the learning rate is too high. When the learning rate is large:

The gradient updates overshoot the optimal solution, causing loss values to oscillate instead of converging.

Training cannot settle into a local minimum, resulting in poor performance on the test set.

By reducing the learning rate, the gradient updates become smaller, allowing the model to converge more smoothly and stabilize the training process. This will help the loss values decrease steadily over time.

Incorrect options:

Apply dropout regularization to the RNN layers to improve generalization and reduce oscillations in the loss - Dropout regularization helps reduce overfitting, but the described issue is a training instability problem caused by a high learning rate.

Increase the learning rate to allow the gradient updates to converge more smoothly and prevent oscillations in the loss values - Increasing the learning rate would further exacerbate the problem. Too large a learning rate might prevent the weights from approaching the optimal solution.

Increase the number of training epochs to give the model more time to learn the patterns in the data - Increasing epochs won’t resolve oscillations caused by a high learning rate. The model will continue to oscillate without convergence.

Reference:

https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters1.html

Temática
ML Model Development
Pregunta 62

You are an ML engineer at an e-commerce company tasked with building an automated recommendation system that scales during peak shopping seasons. The solution requires provisioning multiple compute resources, including SageMaker for model training, EC2 instances for data preprocessing, and an RDS database for storing user interaction data. You need to automate the deployment and management of these resources, ensuring that the stacks can communicate effectively. The company prioritizes infrastructure as code (IaC) to maintain consistency and scalability across environments.

Which approach is the MOST SUITABLE for automating the provisioning of compute resources and ensuring seamless communication between stacks?

Use AWS CDK (Cloud Development Kit) to define the infrastructure in a high-level programming language, deploying each service as an independent stack without configuring inter-stack communication

Use AWS Elastic Beanstalk to deploy the entire ML solution, relying on its built-in environment management to handle the provisioning and communication between resources automatically

Manually provision the SageMaker, EC2, and RDS resources using the AWS Management Console, ensuring that communication is established by manually updating security groups and networking configurations

Respuesta correcta
Use AWS CloudFormation with nested stacks to automate the provisioning of SageMaker, EC2, and RDS resources, and configure outputs from one stack as inputs to another to enable communication between them

Explicación general
Correct option:

Use AWS CloudFormation with nested stacks to automate the provisioning of SageMaker, EC2, and RDS resources, and configure outputs from one stack as inputs to another to enable communication between them

AWS CloudFormation with nested stacks allows you to modularize your infrastructure, making it easier to manage and reuse components. By passing outputs from one stack as inputs to another, you can automate the provisioning of resources while ensuring that all stacks can communicate effectively. This approach also enables consistent and scalable deployments across environments.

 via - https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html

Incorrect options:

Use AWS CDK (Cloud Development Kit) to define the infrastructure in a high-level programming language, deploying each service as an independent stack without configuring inter-stack communication - AWS CDK allows you to define infrastructure using high-level programming languages, which is flexible and powerful. However, failing to configure inter-stack communication would lead to a disjointed deployment, where services may not function together as required.

Manually provision the SageMaker, EC2, and RDS resources using the AWS Management Console, ensuring that communication is established by manually updating security groups and networking configurations - Manually provisioning resources through the AWS Management Console is error-prone and not scalable. It lacks the automation and repeatability that infrastructure as code provides, making it unsuitable for managing complex ML solutions that require seamless communication between multiple resources.

Use AWS Elastic Beanstalk to deploy the entire ML solution, relying on its built-in environment management to handle the provisioning and communication between resources automatically - AWS Elastic Beanstalk is a managed service for deploying applications, but it is not designed for orchestrating complex ML workflows with multiple resource types like SageMaker, EC2, and RDS. It also lacks fine-grained control over resource provisioning and inter-stack communication.

Reference:

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 63

A financial services company uses Amazon SageMaker to develop and register machine learning (ML) models for various business needs, such as fraud detection, risk assessment, and customer segmentation. These models are stored in model groups within the SageMaker Model Registry. The data science team is categorized into three specialized groups based on their areas of focus: fraud detection models, risk assessment models, and customer segmentation models. An ML engineer needs to implement a solution to organize the existing models into these three business categories to improve discoverability at scale, while ensuring the integrity of the model artifacts and their existing groupings remains unaffected.

Which solution will meet these requirements?

Respuesta correcta
Use SageMaker Model Registry collections to group existing model groups into high-level categories, such as fraud detection, risk assessment, and customer segmentation

Attach custom tags to each model artifact in the SageMaker Model Registry to specify their category and filter models based on tags

Use SageMaker Feature Store to tag models with metadata for fraud detection, risk assessment, and customer segmentation. Filter models using queries on the Feature Store.

Move models into newly created SageMaker model groups for fraud detection, risk assessment, and customer segmentation, reassigning them from their current groups

Explicación general
Correct option:

Use SageMaker Model Registry collections to group existing model groups into high-level categories, such as fraud detection, risk assessment, and customer segmentation

SageMaker Model Registry collections allow users to logically organize model groups into high-level categories without disrupting the integrity of the underlying model groups or artifacts. Collections provide a scalable and efficient way to improve model discoverability across a large registry as well as maintain the existing structure of model groups and their metadata. You can also scale seamlessly as more models and business categories are added.

Key Benefits of SageMaker Model Registry collections :

Non-disruptive reorganization using collections.

Better model management and discoverability at scale.

Incorrect options:

Attach custom tags to each model artifact in the SageMaker Model Registry to specify their category and filter models based on tags - Tags are useful for additional metadata but do not provide a hierarchical organizational structure like model registry collections.

Move models into newly created SageMaker model groups for fraud detection, risk assessment, and customer segmentation, reassigning them from their current groups - Moving models to new groups disrupts the existing integrity of the model groups and their associated lineage, approvals, and metadata.

Use SageMaker Feature Store to tag models with metadata for fraud detection, risk assessment, and customer segmentation. Filter models using queries on the Feature Store. - SageMaker Feature Store is intended for storing and serving ML model features, not for managing model group organization or discoverability in the Model Registry.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/modelcollections.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 64

You are a data scientist working for a financial institution that uses a machine learning model to predict loan defaults. The model was trained on historical data from the past five years, but after being deployed for several months, its accuracy has gradually decreased. Upon investigation, you suspect that the underlying data distribution has changed due to economic shifts and changes in customer behavior. This phenomenon is known as model drift, and you need to address it to ensure the model continues to perform well.

Which of the following approaches would you combine for detecting and managing drift in your ML model? (Select two)

Selección correcta
Retrain the model on the most recent data to ensure it captures current trends, and use model versioning to track performance improvements over time

Decrease the complexity of the model by removing features and layers, thereby turning it into a simpler model that can various types of data distributions

Selección correcta
Implement continuous monitoring of input data features and model predictions using statistical tests to detect shifts in data distribution or performance, triggering an alert when drift is detected

Increase the complexity of the model by adding more features and deeper layers, ensuring it can adapt to changing data distributions over time

Deploy a secondary model trained on different data and compare its predictions with the original model to detect any significant differences, indicating potential drift

Explicación general
Correct options:

Implement continuous monitoring of input data features and model predictions using statistical tests to detect shifts in data distribution or performance, triggering an alert when drift is detected

Retrain the model on the most recent data to ensure it captures current trends, and use model versioning to track performance improvements over time

For a model to predict accurately, the data that it is making predictions on must have a similar distribution as the data on which the model was trained. Because data distributions can be expected to drift over time, deploying a model is not a one-time exercise but rather a continuous process. It is a good practice to continuously monitor the incoming data and retrain your model on newer data if you find that the data distribution has deviated significantly from the original training data distribution. If monitoring data to detect a change in the data distribution has a high overhead, then a simpler strategy is to retrain the model periodically, for example, daily, weekly, or monthly.

 via - https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/

Incorrect options:

Deploy a secondary model trained on different data and compare its predictions with the original model to detect any significant differences, indicating potential drift - Comparing predictions from a secondary model might indicate drift, but it’s not a robust or scalable solution. It requires maintaining multiple models, which can be resource-intensive and complex to manage.

Increase the complexity of the model by adding more features and deeper layers, ensuring it can adapt to changing data distributions over time - Increasing model complexity may help capture more nuances in the data, but it does not address drift directly. In fact, a more complex model could exacerbate issues related to overfitting or make the model more sensitive to minor changes in data distribution.

Decrease the complexity of the model by removing features and layers, thereby turning it into a simpler model that can various types of data distributions - Decreasing the model complexity and turning the model into a simpler one would introduce more bias into the model, thereby rendering it ineffective to address changes in the data distribution.

References:

https://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html

https://aws.amazon.com/blogs/machine-learning/automate-model-retraining-with-amazon-sagemaker-pipelines-when-drift-is-detected/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 65

A media streaming company processes video metadata using AWS Glue jobs orchestrated by an AWS Glue workflow. These jobs can run on a schedule or be triggered manually. The company is building Amazon SageMaker Pipelines to develop ML models for predicting user engagement with video content. The pipelines require the processed metadata from the AWS Glue jobs during the feature engineering phase of the ML workflow. The company needs a solution to integrate the AWS Glue jobs with SageMaker Pipelines to ensure seamless data flow.

Which solution will meet these requirements with the LEAST operational overhead?

Configure an Amazon EventBridge rule to trigger the SageMaker pipeline after the completion of AWS Glue jobs and pass job metadata via Lambda

Set up a custom Python script to orchestrate both the AWS Glue workflow and SageMaker Pipelines, ensuring data dependencies are met

Integrate the AWS Glue jobs as processing steps in the SageMaker Pipelines workflow to handle data preprocessing

Respuesta correcta
Use SageMaker Pipelines callback steps to wait for the AWS Glue jobs to complete and retrieve the outputs directly from Amazon S3

Explicación general
Correct option:

Use SageMaker Pipelines callback steps to wait for the AWS Glue jobs to complete and retrieve the outputs directly from Amazon S3

SageMaker Pipelines callback steps are specifically designed to integrate external processes into the SageMaker pipeline workflow. By using a callback step, the SageMaker pipeline waits until the AWS Glue jobs complete. The output of the AWS Glue jobs, stored in Amazon S3, is then passed to subsequent steps in the pipeline. This approach eliminates the need for custom orchestration scripts, manual intervention, or redundant scheduling, ensuring minimal operational overhead. Callback steps are an efficient way to synchronize external workflows, like AWS Glue, with SageMaker Pipelines.

Incorrect options:

Integrate the AWS Glue jobs as processing steps in the SageMaker Pipelines workflow to handle data preprocessing - AWS Glue jobs cannot be directly integrated as processing steps in SageMaker Pipelines because Glue is an independent service, not a SageMaker-native processing task.

Configure an Amazon EventBridge rule to trigger the SageMaker pipeline after the completion of AWS Glue jobs and pass job metadata via Lambda - While this approach is technically possible, it introduces additional operational complexity by requiring EventBridge rules and Lambda for orchestration. Callback steps in SageMaker Pipelines are a more direct solution.

Set up a custom Python script to orchestrate both the AWS Glue workflow and SageMaker Pipelines, ensuring data dependencies are met - Custom orchestration scripts require additional development and maintenance, increasing operational overhead compared to the built-in callback steps feature in SageMaker Pipelines.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps-types.html

Temática
Deployment and Orchestration of ML Workflows