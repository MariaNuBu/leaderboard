Pregunta 1

A healthcare company needs to fine-tune a large language model (LLM) for a medical document summarization application. The dataset includes anonymized patient records, stored in Amazon S3. The company requires a low-code/no-code (LCNC) solution to simplify the fine-tuning process while ensuring scalability and fast deployment. The solution should leverage Amazon SageMaker capabilities to minimize manual coding and operational overhead.

Which of the following Amazon SageMaker services can be used to implement the solution? (Select three)

Amazon SageMaker Clarify

Amazon SageMaker Model Monitor

Selección correcta
Amazon SageMaker JumpStart

Selección correcta
Amazon SageMaker AutoPilot

Amazon SageMaker Data Wrangler

Selección correcta
Amazon SageMaker Canvas

Explicación general
Correct options:

Amazon SageMaker AutoPilot

Amazon SageMaker Autopilot offers low-code machine learning capabilities by automating the process of building, training, and tuning models while providing transparency into the model-building steps. Users provide a dataset and define the target variable, and Autopilot automatically preprocesses data, selects algorithms, and optimizes hyperparameters. Unlike black-box AutoML solutions, Autopilot generates Jupyter notebooks, allowing developers to review, customize, and further refine the models if needed. This makes it a powerful tool for organizations seeking a balance between automation and control, enabling data scientists and non-experts alike to deploy ML models with minimal coding effort.

Amazon SageMaker JumpStart

Amazon SageMaker JumpStart is highly relevant for leveraging Large Language Models (LLMs) in a low-code/no-code approach, enabling businesses and developers to integrate powerful natural language processing (NLP) capabilities without deep expertise in machine learning. JumpStart provides access to pre-trained LLMs, such as models for text generation, summarization, sentiment analysis, and question answering, which can be fine-tuned on custom datasets with minimal effort. The no-code interface within SageMaker Studio allows users to deploy these models directly or fine-tune them by uploading datasets and configuring a few parameters, eliminating the need for extensive coding or ML knowledge.

For low-code users, JumpStart offers example notebooks and APIs for further customization, allowing developers to tweak prompts, integrate models into applications, or chain tasks for more complex workflows. This accessibility accelerates the adoption of LLMs across industries for applications like automated customer support, content creation, or personalized recommendations, empowering teams to build sophisticated NLP solutions with efficiency and scalability.

Amazon SageMaker Canvas

Amazon SageMaker Canvas is a no-code tool designed for business analysts and non-technical users to build machine learning models without writing any code. It provides an intuitive, drag-and-drop interface for preparing data, exploring datasets, and training predictive models. Users can connect to a variety of data sources such as Amazon S3, Redshift, or local files, and quickly generate insights by selecting target variables for predictions. With built-in automated machine learning (AutoML) capabilities, Canvas simplifies model selection and training, making it ideal for scenarios like demand forecasting, customer churn analysis, or inventory optimization, empowering non-technical stakeholders to leverage ML for business decision-making.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html

Incorrect options:

Amazon SageMaker Model Monitor - Amazon SageMaker Model Monitor monitors the quality of Amazon SageMaker AI machine learning models in production. With Model Monitor, you can set alerts that notify you when there are deviations in the model quality.

Amazon SageMaker Clarify - You can use Amazon SageMaker Clarify to understand fairness and model explainability and to explain and detect bias in your models.

Amazon SageMaker Data Wrangler - Amazon SageMaker Data Wrangler (Data Wrangler) is a feature of Amazon SageMaker Studio Classic that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. Y

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html

Temática
ML Model Development
Pregunta 2

A healthcare company is building an application that uses multiple scikit-learn models to predict disease risk factors based on patient data. The company wants to deploy these models using Amazon SageMaker to serve real-time predictions with low latency. The solution must be cost-effective while ensuring efficient traffic handling for all deployed models.

Which of the following options must be combined to develop a solution that meets these requirements? (Select two)

Selección correcta
Use Amazon SageMaker Multi-Model Endpoint to host multiple scikit-learn models in one container behind a single endpoint, reducing infrastructure costs

Deploy each scikit-learn model individually on separate SageMaker Real-Time Endpoints to ensure traffic is isolated for each model

Package all scikit-learn models into separate containers and deploy them behind a single SageMaker Real-Time Endpoint

Selección correcta
Use SageMaker Real-Time Inference to deploy the Multi-Model Endpoint for low-latency predictions and optimized traffic handling

Use SageMaker Serverless Inference to deploy each model and configure an AWS Application Load Balancer (ALB) to route traffic across all model endpoints

Explicación general
Correct options:

Use Amazon SageMaker Multi-Model Endpoint to host multiple scikit-learn models in one container behind a single endpoint, reducing infrastructure costs

With Amazon SageMaker Multi-Model Endpoints, customers can create an endpoint that seamlessly hosts up to thousands of models. These endpoints are well suited to use cases where any one of a large number of models, which can be served from a common inference container to save inference costs, needs to be invokable on-demand and where it is acceptable for infrequently invoked models to incur some additional latency. For applications which require consistently low inference latency, an endpoint deploying a single model is still the best choice.

For the given use case, SageMaker Multi-Model Endpoints allow multiple models to share the same compute resources, reducing operational and infrastructure costs. This setup simplifies management by hosting all models in a single container.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

Use SageMaker Real-Time Inference to deploy the Multi-Model Endpoint for low-latency predictions and optimized traffic handling

SageMaker Real-Time Inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker AI hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.

For the given use case, SageMaker Real-Time Inference ensures efficient handling of live requests to the Multi-Model Endpoint, enabling low-latency predictions for real-time applications.

SageMaker AI provides different options for your inference use cases:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html

Incorrect options:

Deploy each scikit-learn model individually on separate SageMaker Real-Time Endpoints to ensure traffic is isolated for each model - Deploying individual Real-Time Endpoints for each model increases infrastructure costs and complicates traffic management, making it less cost-effective for hosting multiple models.

Use SageMaker Serverless Inference to deploy each model and configure an AWS Application Load Balancer (ALB) to route traffic across all model endpoints - While Serverless Inference can handle sporadic traffic, it is not optimized for low-latency, high-volume use cases. Adding an ALB introduces unnecessary complexity and operational overhead.

Package all scikit-learn models into separate containers and deploy them behind a single SageMaker Real-Time Endpoint - SageMaker endpoints cannot host multiple containers behind a single endpoint without a Multi-Model Endpoint setup. This configuration would require custom orchestration, increasing complexity.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html

https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/multi_model_sklearn_home_value/sklearn_multi_model_endpoint_home_value.html

https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html

https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 3

You are a machine learning engineer at a biotechnology company working on a deep learning model to analyze genomic sequences. The model requires significant computational resources for training due to the complexity and size of the dataset, which consists of billions of nucleotide sequences. Additionally, once deployed, the model must provide real-time inferences for clinical applications, requiring low-latency predictions. Your task is to choose the appropriate compute environment for both training and inference.

Which of the following compute environment configurations is the MOST SUITABLE for meeting the training and inference requirements of this use case?

Use Amazon EC2 with g4dn instances (NVIDIA T4 GPUs) for both training and inference, optimizing for cost-efficiency while maintaining moderate GPU performance

Respuesta correcta
Use Amazon SageMaker with p4d instances (NVIDIA A100 GPUs) for training to handle large-scale deep learning workloads, and deploy the model using ml.inf1 instances (AWS Inferentia chips) for low-latency inference

Use Amazon SageMaker with ml.c5.large instances (Intel Cascade Lake CPUs) for training and inference, relying on the CPU's high memory bandwidth for both tasks

Use Amazon SageMaker with p3 instances (NVIDIA V100 GPUs) for training, and deploy the model using ml.m5.large instances (Intel Skylake CPUs) for real-time inference to balance cost and performance

Explicación general
Correct option:

Use Amazon SageMaker with p4d instances (NVIDIA A100 GPUs) for training to handle large-scale deep learning workloads, and deploy the model using ml.inf1 instances (AWS Inferentia chips) for low-latency inference

p4d instances with NVIDIA A100 GPUs are optimized for large-scale deep learning workloads, providing the necessary computational power for training complex models on large datasets. For inference, ml.inf1 instances with AWS Inferentia chips are designed specifically for high-performance, low-latency inference, making this combination ideal for both training and real-time clinical applications.

Incorrect options:

Use Amazon SageMaker with p3 instances (NVIDIA V100 GPUs) for training, and deploy the model using ml.m5.large instances (Intel Skylake CPUs) for real-time inference to balance cost and performance - While p3 instances with NVIDIA V100 GPUs are powerful for deep learning training, ml.m5.large instances with Intel Skylake CPUs may not deliver the low-latency inference needed for real-time clinical applications. The CPU-based inference environment could be a bottleneck.

Use Amazon SageMaker with ml.c5.large instances (Intel Cascade Lake CPUs) for training and inference, relying on the CPU's high memory bandwidth for both tasks - ml.c5.large instances with Intel Cascade Lake CPUs are suitable for general-purpose compute tasks, but they lack the specialized GPU performance needed for deep learning training and the low-latency capabilities required for real-time inference.

Use Amazon EC2 with g4dn instances (NVIDIA T4 GPUs) for both training and inference, optimizing for cost-efficiency while maintaining moderate GPU performance - g4dn instances with NVIDIA T4 GPUs are cost-efficient and suitable for moderate workloads, but they may not provide the high-performance training environment needed for a large genomic dataset. Additionally, they might not meet the low-latency inference requirements as effectively as ml.inf1 instances.

References:

https://aws.amazon.com/ec2/instance-types/

https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 4

A research institute is planning to use Amazon SageMaker to train a machine learning model for object detection in satellite imagery. The training dataset, consisting of 8 TB of high-resolution images, is stored on an Amazon FSx for NetApp ONTAP system virtual machine (SVM) that is configured in the same VPC as the SageMaker environment. The ML engineer needs to ensure that the SageMaker training job can efficiently access the dataset during training.

Which of the following represents the best solution for the given use case?

Export the training data from the FSx for NetApp ONTAP system to a local system and re-upload the data to SageMaker through the SageMaker Python SDK

Copy the training data from the FSx for NetApp ONTAP system to an Amazon S3 bucket and configure SageMaker to use the S3 bucket for training

Respuesta correcta
Mount the FSx for NetApp ONTAP file system directly as a volume to the SageMaker training instance

Use AWS DataSync to continuously synchronize the FSx for NetApp ONTAP system with Amazon S3 and set up SageMaker to read the data from S3 during training

Explicación general
Correct option:

Mount the FSx for NetApp ONTAP file system directly as a volume to the SageMaker training instance

Mounting the FSx file system directly using NFS ensures efficient and seamless access to the training data without duplicating or transferring it. This can be achieved by mounting FSx ONTAP as a volume to SageMaker. Once the file system is successfully mounted, you can upload your dataset to the mounted location, making it accessible for training your models within the SageMaker environment. This approach allows you to leverage the storage capacity and capabilities of FSx ONTAP while working with SageMaker for model development and training.

Mount the FSx for NetApp ONTAP file system as a volume to the SageMaker training instance:  via - https://aws.amazon.com/blogs/big-data/7-most-common-data-preparation-transformations-in-aws-glue-databrew/

Incorrect options:

Copy the training data from the FSx for NetApp ONTAP system to an Amazon S3 bucket and configure SageMaker to use the S3 bucket for training - Copying 8 TB of data to S3 introduces significant time and storage costs, making this solution less efficient compared to direct access via NFS.

Use AWS DataSync to continuously synchronize the FSx for NetApp ONTAP system with Amazon S3 and set up SageMaker to read the data from S3 during training - While DataSync can automate synchronization, it adds unnecessary operational complexity and cost when the data is already in the same VPC as SageMaker.

Export the training data from the FSx for NetApp ONTAP system to a local system and re-upload the data to SageMaker through the SageMaker Python SDK - Exporting data to a local system and re-uploading it is a time-consuming and inefficient process, especially for 8 TB of data. This approach does not leverage the existing VPC setup.

Reference:

https://docs.netapp.com/us-en/netapp-solutions/ai/mlops_fsxn_sagemaker_integration_training.html#network-environment

Temática
Data Preparation for Machine Learning (ML)
Pregunta 5

An organization requires a scalable solution for preprocessing and transforming big data for machine learning. The data is spread across multiple sources and needs to be processed using a distributed computing framework.

Which of the following solutions would you recommend?

Respuesta correcta
Use Amazon EMR in Amazon SageMaker Studio

Use built-in SQL extension in Amazon SageMaker Studio

Use Data Wrangler within Amazon SageMaker Canvas

Use Amazon SageMaker Ground Truth

Explicación general
Correct option:

Use Amazon EMR in Amazon SageMaker Studio

For users focused on scalable data preparation, SageMaker offers capabilities that leverage the Hadoop/Spark ecosystem for the distributed processing of big data. The integration between Amazon EMR and Amazon SageMaker Studio provides a scalable environment for large-scale data preparation for machine learning using open-source frameworks such as Apache Spark, Apache Hive, or Presto. Users can access Amazon EMR clusters and data directly from their Studio notebooks to perform their preparation tasks.

This solution is optimized for scaling long-running or batch-oriented data preprocessing and feature engineering workloads on Amazon EMR while taking advantage of SageMaker's machine-learning capabilities.

Incorrect options:

Use Data Wrangler within Amazon SageMaker Canvas - Amazon SageMaker Data Wrangler (Data Wrangler) is a feature of Amazon SageMaker Studio Classic that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. Amazon SageMaker Canvas is a no-code visual interface that empowers you to prepare data, build, and deploy highly accurate ML models, streamlining the end-to-end ML lifecycle in a unified environment. Data Wrangler within Amazon SageMaker Canvas is not the right combination for transforming big data for ML.

Use built-in SQL extension in Amazon SageMaker Studio - Amazon SageMaker Studio provides a built-in SQL extension. This extension allows data scientists to perform tasks such as sampling, exploratory analysis, and feature engineering directly within their JupyterLab notebooks. This extension is not useful for transforming big data for ML.

Use Amazon SageMaker Ground Truth - You can use SageMaker Ground Truth to manage the data labeling workflows using humans for your training datasets. You can use workers from either Amazon Mechanical Turk, a vendor company that you choose, or an internal, private workforce along with machine learning to enable you to create a labeled dataset. SageMaker Ground Truth is not the right tool for transforming big data needed for ML.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-prep.html

https://docs.aws.amazon.com/sagemaker/latest/dg/sms.html

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-sql-extension.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 6

A data scientist is tasked with preparing datasets for an ML project. The datasets contain missing values, duplicate records, and outliers. The data scientist must clean and consolidate these datasets into a single data frame, ensuring the data is properly prepared for training a machine learning model.

What is the most efficient approach to address these requirements?

Manually import and merge the datasets, then save the consolidated data to an Amazon S3 bucket. Use Amazon Athena to perform data transformations, and leverage its integration with Amazon QuickSight for visualizing the transformed data

Use Amazon SageMaker Python SDK to consolidate these datasets into a single pandas data frame and run data transformations on it

Utilize Amazon SageMaker Ground Truth to import and consolidate the datasets into a single data frame. Employ the human-in-the-loop functionality to clean and prepare the data for further processing

Respuesta correcta
Leverage Amazon SageMaker Data Wrangler to import the datasets and merge them into a unified data frame. Utilize Data Wrangler's transformation functionalities to clean and preprocess the data

Explicación general
Correct option:

Leverage Amazon SageMaker Data Wrangler to import the datasets and merge them into a unified data frame. Utilize Data Wrangler's transformation functionalities to clean and preprocess the data

Amazon SageMaker Data Wrangler (Data Wrangler) is a feature of Amazon SageMaker Studio Classic that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. You can integrate a Data Wrangler data preparation flow into your machine learning (ML) workflows to simplify and streamline data pre-processing and feature engineering using little to no coding.

Data Wrangler provides severak functionalities to import, analyze and prepare data for machine learning applications.

Data Wrangler core functionalities:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html

Use the Data Quality and Insights Report to perform an analysis of the data that you've imported into Data Wrangler. AWS recommends that you create the report after you import your dataset. You can use the report to help you clean and process your data. It gives you information such as the number of missing values and the number of outliers. Data Wrangler detects duplicate rows and calculates the ratio of duplicate rows in your data. You can remove duplicate samples from the dataset using the Drop duplicates transform under Manage rows. Data Wrangler shows you the most frequently duplicated rows.

The insights report has a brief summary of the data that includes general information such as missing values, invalid values, feature types, outlier counts, and more. It can also include high severity warnings that point to probable issues with the data. We recommend that you investigate the warnings.

Example of a report summary:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-insights.html

Incorrect options:

Utilize Amazon SageMaker Ground Truth to import and consolidate the datasets into a single data frame. Employ the human-in-the-loop functionality to clean and prepare the data for further processing - Ground Truth helps you build high-quality training datasets for your machine learning models. With Ground Truth, you can use workers from either Amazon Mechanical Turk, a vendor company that you choose, or an internal, private workforce along with machine learning to enable you to create a labeled dataset. You can use the labeled dataset output from Ground Truth to train your own models. Amazon SageMaker Ground Truth is designed for creating large labeled datasets and is not suitable for transforming or cleaning existing datasets.

Use Amazon SageMaker Python SDK to consolidate these datasets into a single pandas data frame and run data transformations on it - Using the SageMaker Python SDK is not the easiest solution among the options provided, as it requires coding expertise.

Manually import and merge the datasets, then save the consolidated data to an Amazon S3 bucket. Use Amazon Athena to perform data transformations, and leverage its integration with Amazon QuickSight for visualizing the transformed data - Manually importing and merging data is not an efficient method for cleaning and transforming datasets.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-insights.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 7

You are a DevOps engineer at a tech company that is building a scalable microservices-based application. The application is composed of several containerized services, each responsible for different parts of the application, such as user authentication, data processing, and recommendation systems. The company wants to standardize and automate the deployment and management of its infrastructure using Infrastructure as Code (IaC). You need to choose between AWS CloudFormation and AWS Cloud Development Kit (CDK) for defining the infrastructure. Additionally, you must decide on the appropriate AWS container service to manage and deploy these microservices efficiently.

Given the requirements, which combination of IaC option and container service is MOST SUITABLE for this scenario, and why?

Respuesta correcta
Use AWS CDK for infrastructure as code, allowing you to define the infrastructure in a high-level programming language, and deploy the containerized microservices using Amazon EKS (Elastic Kubernetes Service) for advanced orchestration and scalability

Use AWS CloudFormation to define and deploy the infrastructure as code, and Amazon ECR (Elastic Container Registry) with Fargate for running the containerized microservices without needing to manage the underlying servers

Use AWS CDK with Amazon ECS on EC2 instances to combine the flexibility of programming languages with direct control over the underlying server infrastructure for the microservices

Use AWS CloudFormation with YAML templates for infrastructure automation and deploy the containerized microservices using Amazon Lightsail Containers to simplify management and reduce costs

Explicación general
Correct option:

Use AWS CDK for infrastructure as code, allowing you to define the infrastructure in a high-level programming language, and deploy the containerized microservices using Amazon EKS (Elastic Kubernetes Service) for advanced orchestration and scalability

AWS CDK offers the flexibility of using high-level programming languages (e.g., Python, JavaScript) to define infrastructure, making it easier to manage complex infrastructure setups programmatically.

 via - https://docs.aws.amazon.com/cdk/v2/guide/home.html

Amazon EKS is designed for running containerized microservices with Kubernetes, providing advanced orchestration, scalability, and integration with CI/CD pipelines. This combination is ideal for a microservices-based application with complex deployment and scaling needs.

 via - https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html

Incorrect options:

Use AWS CloudFormation to define and deploy the infrastructure as code, and Amazon ECR (Elastic Container Registry) with Fargate for running the containerized microservices without needing to manage the underlying servers - AWS CloudFormation is powerful for defining infrastructure using JSON or YAML. However, Amazon ECR is an AWS managed container image registry service and it cannot be used to manage and run containers.

Use AWS CloudFormation with YAML templates for infrastructure automation and deploy the containerized microservices using Amazon Lightsail Containers to simplify management and reduce costs - AWS CloudFormation with YAML templates is suitable for traditional IaC, but Amazon Lightsail Containers is better for simple, low-cost container deployments. It may lack the scalability and orchestration features required for a complex microservices architecture.

Use AWS CDK with Amazon ECS on EC2 instances to combine the flexibility of programming languages with direct control over the underlying server infrastructure for the microservices - AWS CDK combined with Amazon ECS on EC2 gives more control over the underlying infrastructure but adds complexity in managing the servers. For a microservices-based application, this might introduce unnecessary overhead compared to using managed services like Fargate or EKS.

References:

https://docs.aws.amazon.com/cdk/v2/guide/home.html

https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 8

A media streaming company manages a data lake using AWS Lake Formation to store and analyze user behavior and content preferences. The data lake contains both structured and unstructured data, including video metadata, user activity logs, and clickstream data. The company’s data analysts are assigned to specific content categories, such as sports, movies, or documentaries. Each analyst needs access to only the data relevant to their assigned category for analysis through Amazon Athena. The company wants to ensure fine-grained access control while reducing operational complexity.

Which solution will meet these requirements?

Use AWS Glue workflows to tag data with category-specific tags and manage access using IAM policy conditions based on these tags

Use S3 bucket policies to restrict access to folders corresponding to each content category and configure Athena workgroups for query-level access control

Respuesta correcta
Use AWS Lake Formation to define permissions based on Lake Formation tags. Assign category-specific tags to the data and associate them with the analysts’ IAM roles for fine-grained access control

Create separate S3 buckets for each content category and assign bucket-level permissions to analysts based on their roles

Explicación general
Correct option:

Use AWS Lake Formation to define permissions based on Lake Formation tags. Assign category-specific tags to the data and associate them with the analysts’ IAM roles for fine-grained access control

AWS Lake Formation provides a powerful framework for managing fine-grained access controls in a data lake, making it the ideal solution for the company’s requirements. By leveraging Lake Formation tags, the company can assign metadata tags to datasets, such as sports, movies, or documentaries, to categorize the data based on content type. These tags can then be used to define tag-based permissions that are mapped to the IAM roles of analysts. This approach ensures that analysts have access only to the data relevant to their assigned categories, such as sports-related data for sports analysts.

The integration of Lake Formation with tools like Amazon Athena enables seamless enforcement of these permissions, ensuring that data access is consistent across query and visualization platforms. Using tag-based permissions also simplifies the management of access controls, eliminating the need to manually create and update complex bucket policies or folder-level restrictions. This centralized, scalable, and operationally efficient approach aligns perfectly with the company’s need to manage a large and diverse dataset in the data lake.

 via - https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html

Incorrect options:

Use S3 bucket policies to restrict access to folders corresponding to each content category and configure Athena workgroups for query-level access control - S3 bucket policies cannot provide fine-grained access at the query level in Athena. Managing multiple bucket policies increases operational complexity.

Use AWS Glue workflows to tag data with category-specific tags and manage access using IAM policy conditions based on these tags - While AWS Glue workflows can assign tags, they do not enforce access control. Lake Formation is required to interpret and enforce tag-based permissions effectively.

Create separate S3 buckets for each content category and assign bucket-level permissions to analysts based on their roles - Splitting data into multiple buckets leads to fragmentation and operational overhead. This approach does not scale well for large datasets or diverse content categories.

References:

https://docs.aws.amazon.com/lake-formation/latest/dg/what-is-lake-formation.html

https://docs.aws.amazon.com/lake-formation/latest/dg/tag-based-access-control.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 9

A media streaming company is using Amazon Redshift ML in its primary AWS account to build recommendation models for personalized content suggestions. The training data for these models, which includes user watch history and preferences, is stored in an Amazon S3 bucket in a secondary AWS account. The company requires a secure pipeline that allows Redshift ML in the primary account to access this training data while ensuring that no public IPv4 addresses are used in the data transfer process.

What do you recommend?

Use S3 Transfer Acceleration to optimize data transfers between the accounts. Configure Redshift ML in the primary account to access the S3 bucket through the accelerated endpoint

Respuesta correcta
Set up a VPC endpoint for Amazon S3 in the primary account to enable private communication. Configure a cross-account bucket policy in the secondary account to grant Redshift ML access to the S3 bucket securely

Deploy a VPN connection between the VPCs in the primary and secondary accounts to securely transfer training data to Redshift ML in the primary account

Export the data from the S3 bucket in the secondary account to a new bucket in the primary account using AWS Glue, then configure Redshift ML to access the data from the primary bucket

Explicación general
Correct option:

Set up a VPC endpoint for Amazon S3 in the primary account to enable private communication. Configure a cross-account bucket policy in the secondary account to grant Redshift ML access to the S3 bucket securely

Amazon Redshift ML uses a model to generate results. You can provide the data that you want to train a model, and metadata associated with data inputs to Amazon Redshift. Then Amazon Redshift ML creates models in Amazon SageMaker AI that capture patterns in the input data.

The combination of a VPC endpoint for Amazon S3 and a cross-account bucket policy is the most secure and efficient solution. The VPC endpoint enables private communication over the AWS private network, eliminating the need for public IPv4 addresses. By configuring a cross-account bucket policy, the S3 bucket in the secondary account can securely grant permissions to Redshift ML in the primary account, ensuring seamless access to the training data without duplication or additional operational overhead.

Incorrect options:

Deploy a VPN connection between the VPCs in the primary and secondary accounts to securely transfer training data to Redshift ML in the primary account - This option is a distractor. Amazon S3 buckets are not created inside a VPC. So, creating a VPN connection between the VPCs in the primary and secondary accounts serves no real purpose for the given use case.

Use S3 Transfer Acceleration to optimize data transfers between the accounts. Configure Redshift ML in the primary account to access the S3 bucket through the accelerated endpoint - S3 Transfer Acceleration uses the public internet to optimize transfers, which violates the requirement to avoid public IPv4 addresses.

Export the data from the S3 bucket in the secondary account to a new bucket in the primary account using AWS Glue, then configure Redshift ML to access the data from the primary bucket - This approach introduces redundant data duplication and additional operational overhead, making it less efficient than direct access via a VPC endpoint.

References:

https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html

https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html

https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.html

https://docs.aws.amazon.com/redshift/latest/dg/getting-started-machine-learning.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 10

A logistics company needs to deploy a custom ML model for daily demand forecasting. The workload is predictable and occurs within a 90-minute window each day. During this period, multiple concurrent invocations are expected, requiring low-latency responses. The company prefers minimal involvement in infrastructure maintenance or configuration. The company wants AWS to manage the underlying infrastructure and auto scaling functionality.

Which is a cost-effective solution to meet these requirements?

Respuesta correcta
Deploy the model using Amazon SageMaker Serverless Inference with provisioned concurrency

Deploy the model using Amazon SageMaker Asynchronous Inference with auto scaling

Deploy the model on Amazon Elastic Compute Cloud (EC2) instance using SageMaker Python SDK. This spins up a managed endpoint with multiple EC2 instances. Each instance has a webserver that provides low-latency responses to the requests

Deploy the model using Amazon SageMaker Real-time Inference with auto scaling

Explicación general
Correct option:

Deploy the model using Amazon SageMaker Serverless Inference with provisioned concurrency

Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models without configuring or managing any of the underlying infrastructure. Serverless Inference with provisioned concurrency is a cost-effective option when you have predictable bursts in your traffic. Provisioned Concurrency allows you to deploy models on serverless endpoints with predictable performance, and high scalability by keeping your endpoints warm. SageMaker AI ensures that for the number of Provisioned Concurrency that you allocate, the compute resources are initialized and ready to respond within milliseconds.

For Serverless Inference with Provisioned Concurrency, you pay for the compute capacity used to process inference requests, billed by the millisecond, and the amount of data processed. You also pay for Provisioned Concurrency usage, based on the memory configured, duration provisioned, and the amount of concurrency enabled.

Serverless Inference workflow:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html

Incorrect options:

Deploy the model using Amazon SageMaker Asynchronous Inference with auto scaling - Amazon SageMaker AI supports automatic scaling (autoscaling) your asynchronous endpoint. Autoscaling dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. Unlike other hosted models Amazon SageMaker AI supports, with Asynchronous Inference you can also scale down your asynchronous endpoints instances to zero. Since the company does not want to manage the underlying infrastructure or its configuration, so this option is ruled out.

Deploy the model using Amazon SageMaker Real-time Inference with auto scaling - Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. To use auto scaling, you define a scaling policy that adds and removes the number of instances for your production variant in response to actual workloads. Since the company does not want to manage the underlying infrastructure or its configuration, so this option is ruled out.

Deploy the model on Amazon Elastic Compute Cloud (EC2) instance using SageMaker Python SDK. This spins up a managed endpoint with multiple EC2 instances. Each instance has a webserver that provides low-latency responses to the requests - Amazon EC2 is a good choice for hosting because of its flexibility and the ability to provide a customizable environment. However, deploying the model on EC2 involves additional setup, configuration and maintenance which the company wants to avoid.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html

https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-policy.html

Temática
ML Model Development
Pregunta 11

A financial services company is training a machine learning model in Amazon SageMaker to detect fraudulent transactions. The company has observed that training jobs are consuming a significant amount of computational resources, leading to increased energy costs. The company wants to adopt sustainable practices that reduce energy usage and optimize training efficiency while maintaining model accuracy.

Which actions will reduce the energy usage and computational resources associated with the company's training jobs? (Select two)

Selección correcta
Leverage Amazon SageMaker Debugger to monitor training jobs for non-converging conditions and terminate jobs early to save computational resources

Selección correcta
Use AWS Trainium-based instances for training the ML model to improve energy efficiency and reduce costs with optimized deep learning hardware

Disable distributed training to reduce energy consumption by training models on a single instance

Increase the size of the training dataset to ensure the model has more data to process, reducing the likelihood of non-convergence

Increase the instance size for training jobs to ensure that each job uses more resources for faster execution and reduced energy consumption

Explicación general
Correct options:

Use AWS Trainium-based instances for training the ML model to improve energy efficiency and reduce costs with optimized deep learning hardware

AWS Trainium is purpose-built for training deep learning models with efficiency and scalability in mind. By using Trainium-based instances, the company can significantly reduce energy consumption and computational resource usage compared to traditional GPU-based instances. These instances are optimized to handle large-scale ML training jobs while consuming less power, making them ideal for companies prioritizing sustainability. Additionally, AWS Trainium delivers cost-effective performance for deep learning workloads, helping organizations save on training expenses without sacrificing model accuracy or speed. Leveraging Trainium supports the company's goal of minimizing the environmental impact of its ML operations.

Leverage Amazon SageMaker Debugger to monitor training jobs for non-converging conditions and terminate jobs early to save computational resources

Amazon SageMaker Debugger provides real-time monitoring of training jobs to detect inefficiencies, including non-converging conditions where models fail to improve during training. By identifying issues such as poor gradient updates or stagnant loss functions, SageMaker Debugger can terminate training jobs that are unlikely to produce useful results. This proactive approach not only prevents wasted computational effort but also reduces energy consumption by avoiding prolonged execution of ineffective training tasks. Additionally, the insights provided by SageMaker Debugger help ML engineers optimize their training workflows, further enhancing resource efficiency while maintaining model performance standards. This feature aligns perfectly with the company’s objective of achieving sustainable and efficient ML operations.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html

Incorrect options:

Increase the size of the training dataset to ensure the model has more data to process, reducing the likelihood of non-convergence - Increasing the dataset size may improve model accuracy but will likely increase computational resource usage, contrary to the goal of reducing energy consumption.

Increase the instance size for training jobs to ensure that each job uses more resources for faster execution and reduced energy consumption - Increasing the instance size does not necessarily reduce energy usage. Over-provisioning instances can lead to wasted resources and higher energy consumption.

Disable distributed training to reduce energy consumption by training models on a single instance - Disabling distributed training can increase the time required to complete training jobs, which may lead to higher energy consumption overall.

References:

https://aws.amazon.com/ec2/instance-types/trn1/

https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 12

A healthcare research organization is running ML models on-premises to analyze patient data for early disease detection. The organization uses custom Python scripts written with PyTorch and relies on proprietary medical datasets for training the models. The models are highly customized and require domain-specific knowledge for accurate predictions. The organization plans to migrate the ML workflows to AWS to leverage scalable and managed infrastructure. The migration must require the least effort while ensuring minimal changes to the existing PyTorch scripts.

What do you recommend?

Respuesta correcta
Use Amazon SageMaker Script Mode to import the existing PyTorch scripts and proprietary datasets into SageMaker for training and inference without significant modifications

Develop a custom PyTorch container with the existing scripts and datasets, then deploy it on Amazon SageMaker for distributed training and inference

Migrate the datasets to Amazon S3 and use Amazon EMR to preprocess the data before deploying the PyTorch models on EC2 instances

Refactor the PyTorch scripts to integrate them with SageMaker’s built-in algorithms, and upload the datasets to Amazon S3 for preprocessing

Explicación general
Correct option:

Use Amazon SageMaker Script Mode to import the existing PyTorch scripts and proprietary datasets into SageMaker for training and inference without significant modifications

Script mode in Amazon SageMaker allows the use of custom Python scripts directly with prebuilt Docker images that include popular ML frameworks like PyTorch. This makes it easy to migrate on-premises models with minimal changes to the existing codebase. This solution involves the least effort as SageMaker provides prebuilt images for PyTorch, eliminating the need to manually build and maintain custom containers. The ML engineers can reuse existing Python scripts with minimal modifications, leveraging SageMaker’s managed infrastructure for training and hosting. In addition, this solution leverages domain knowledge and flexibility, since the company’s model-building process involves unique domain knowledge, script mode allows flexibility to incorporate this knowledge into the existing code without significant rewrites.

 via - https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/

Incorrect options:

Develop a custom PyTorch container with the existing scripts and datasets, then deploy it on Amazon SageMaker for distributed training and inference - Building and managing custom containers involves significant development and operational effort, which is unnecessary given that SageMaker Script Mode natively supports PyTorch.

Refactor the PyTorch scripts to integrate them with SageMaker’s built-in algorithms, and upload the datasets to Amazon S3 for preprocessing - SageMaker’s built-in algorithms are not designed for highly customized workflows and would require significant refactoring, increasing the migration effort.

Migrate the datasets to Amazon S3 and use Amazon EMR to preprocess the data before deploying the PyTorch models on EC2 instances - While EMR is effective for big data processing, it is not optimized for ML model training. This approach is overly complex and does not address the need to run existing PyTorch scripts with minimal changes.

References:

https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/

https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-adapt-your-own.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 13

Your company has a significant amount of data stored in an Amazon DynamoDB database. As part of a new machine learning project, you need to access this data from an Amazon SageMaker notebook for analysis and model development. The goal is to ensure that the data is efficiently accessible from the notebook while maintaining performance and minimizing potential delays.

Use Sagemaker Data Wrangler to directly import data from Amazon DynamoDb database to Sagemaker notebook

Respuesta correcta
Use the AWS SDK for Python (Boto3) to query DynamoDB directly from the SageMaker notebook

Export your DynamoDB table into a .csv file and upload this file to the S3 bucket. Configure Sagemaker notebook to connect to S3 bucket for accessing the data

Use AWS Glue to transfer your table from DynamoDB to S3 bucket. Configure Sagemaker notebook to connect to S3 bucket for accessing the data

Explicación general
Correct option:

Use the AWS SDK for Python (Boto3) to query DynamoDB directly from the SageMaker notebook

You can access the data in your SageMaker Notebook by reading it using the boto3 client. Initialize a DynamoDB Client and do a Scan which will return all the data that you require. The data will be in JSON format and you would need to convert that to a format that is useful for the rest of your notebook, such as converting to a pandas dataframe.

Incorrect options:

Export your DynamoDB table into a .csv file and upload this file to the S3 bucket. Configure Sagemaker notebook to connect to S3 bucket for accessing the data

Use AWS Glue to transfer your table from DynamoDB to S3 bucket. Configure Sagemaker notebook to connect to S3 bucket for accessing the data

These options are valid but are not optimal for the given use case.

Use Sagemaker Data Wrangler to directly import data from Amazon DynamoDb database to Sagemaker notebook - Sagemaker Data Wrangler cannot import from Amazon DynamoDb database. You can use Amazon SageMaker Data Wrangler to import data only from the following data sources: Amazon Simple Storage Service (Amazon S3), Amazon Athena, Amazon Redshift, and Snowflake.

References:

https://repost.aws/questions/QUowJvrZpiRjOS5v0OhDkEXQ/accessing-dynamodb-data-from-sagemaker

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.html

https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/dynamodb.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 14

A company uses an ML model to generate text descriptions for product images uploaded by customers. Each uploaded image can be as large as 40 MB in size. To cost-effectively handle the inference request for these uploads, the images are saved in an Amazon S3 bucket. A data engineer needs to implement a scalable processing solution that can adapt to fluctuations in demand while minimizing operational overhead.

What is the best solution to meet these requirements?

Create an Amazon SageMaker Real-Time Inference endpoint. Upon each image upload, trigger an AWS Lambda function to make an inference request

Create an Amazon SageMaker Serverless Inference endpoint and a scaling policy. Run a script to make an inference request for each image

Utilize Amazon SageMaker's Batch Transform for inference to process the images stored in the Amazon S3 bucket in bulk

Respuesta correcta
Create an Amazon SageMaker Asynchronous Inference endpoint and a scaling policy. Upon each image upload, trigger an AWS Lambda function to make an inference request

Explicación general
Correct option:

Create an Amazon SageMaker Asynchronous Inference endpoint and a scaling policy. Upon each image upload, trigger an AWS Lambda function to make an inference request

Amazon SageMaker Asynchronous Inference is a capability in SageMaker AI that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.

How Asynchronous inference works:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html

Incorrect options:

Create an Amazon SageMaker Serverless Inference endpoint and a scaling policy. Run a script to make an inference request for each image - Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models without configuring or managing any of the underlying infrastructure. On-demand Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. The maximum request and response payload size for serverless invocations is 4 MB, so this option is incorrect.

Utilize Amazon SageMaker's Batch Transform for inference to process the images stored in the Amazon S3 bucket in bulk - Batch transform is a serverless, scalable, and cost-effective solution for offline predictions on large batches of data that is available upfront. It allows users to perform bulk inferences on their data in the form of a CSV or JSON file. Image file formats are not supported by Batch Transform.

Create an Amazon SageMaker Real-Time Inference endpoint. Upon each image upload, trigger an AWS Lambda function to make an inference request - Real-time Inference is suitable for workloads with millisecond latency requirements, payload sizes up to 6 MB, and processing times of up to 60 seconds. So, this option is incorrect.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html

https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html

https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-invoke.html

https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-batch-code.html

Temática
ML Model Development
Pregunta 15

You are a machine learning engineer managing an ML model deployed on Amazon SageMaker to provide real-time predictions for a financial application. The model is critical for fraud detection and must operate with high availability and low latency. Recently, users have reported increased latency in receiving predictions, and there have been occasional timeout errors. To maintain the performance and reliability of the model, you need to configure monitoring and alerting tools to troubleshoot and resolve these issues promptly.

Which of the following approaches is the MOST EFFECTIVE for configuring and using Amazon CloudWatch tools to troubleshoot and analyze the performance of your ML solution?

Use CloudWatch Logs Insights to manually search and analyze logs from the SageMaker training jobs, and configure CloudWatch Alarms to track CPU and memory usage on the EC2 instances running the model

Rely on the SageMaker console to monitor model performance metrics, and configure CloudWatch Logs to store training logs without setting up specific alarms, analyzing logs only when issues arise

Enable Amazon CloudTrail for logging all API calls, and use CloudWatch Logs to monitor network traffic. Create CloudWatch Alarms based on API activity to detect performance issues

Respuesta correcta
Enable Amazon CloudWatch Logs to capture detailed logs from the SageMaker endpoint, configure CloudWatch Alarms to monitor invocation latency and error rates, and set up notifications to alert the team when thresholds are breached

Explicación general
Correct option:

Enable Amazon CloudWatch Logs to capture detailed logs from the SageMaker endpoint, configure CloudWatch Alarms to monitor invocation latency and error rates, and set up notifications to alert the team when thresholds are breached

This option is the most comprehensive approach to monitoring and troubleshooting an ML solution. YYou can monitor Amazon SageMaker using Amazon CloudWatch, which collects raw data and processes it into readable, near real-time metrics. These statistics are kept for 15 months, so that you can access historical information and gain a better perspective on how your web application or service is performing. You can also set alarms that watch for certain thresholds and send notifications or take actions when those thresholds are met. By capturing detailed logs from the SageMaker endpoint with CloudWatch Logs, you can analyze the performance and identify specific issues contributing to latency and errors. Configuring CloudWatch Alarms to monitor key metrics like invocation latency and error rates ensures that you receive timely alerts when performance degrades, enabling quick resolution. Notifications allow the team to respond proactively to issues before they impact users.

Incorrect options:

Use CloudWatch Logs Insights to manually search and analyze logs from the SageMaker training jobs, and configure CloudWatch Alarms to track CPU and memory usage on the EC2 instances running the model - While CloudWatch Logs Insights is a powerful tool for analyzing logs, focusing only on SageMaker training jobs and EC2 instance metrics misses critical aspects of real-time inference performance, such as latency and error rates at the endpoint level.

Rely on the SageMaker console to monitor model performance metrics, and configure CloudWatch Logs to store training logs without setting up specific alarms, analyzing logs only when issues arise - Relying solely on the SageMaker console for monitoring is insufficient for proactive troubleshooting. Without specific CloudWatch Alarms and log analysis, issues may go unnoticed until they significantly impact performance.

Enable Amazon CloudTrail for logging all API calls, and use CloudWatch Logs to monitor network traffic. Create CloudWatch Alarms based on API activity to detect performance issues - While CloudTrail is useful for auditing API activity, it is not focused on resource performance. Monitoring network traffic alone does not provide insights into timeout errors and application-level issues. This option acts as a distractor.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-incident-response.html

https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 16

A company operates an Amazon SageMaker training job in a public subnet within an Amazon VPC. The network is properly configured, allowing seamless data transfer between the SageMaker training job and Amazon S3. Recently, the company identified malicious traffic originating from a specific IP address, targeting the resources within the VPC. The company needs to block all traffic from the suspicious IP address while ensuring legitimate traffic remains unaffected.

Which of the following would you recommend to address this requirement?

Enable AWS WAF (Web Application Firewall) for the SageMaker domain and create a rule to block traffic from the specific IP address

Respuesta correcta
Create a network ACL (NACL) for the subnet hosting the SageMaker training job and add a deny rule to block traffic from the specific IP address

Update the security group associated with the SageMaker training job to include a deny rule for traffic from the specific IP address

Modify the VPC route table to direct all traffic from the specific IP address to a target with a blackhole routing state

Explicación general
Correct option:

Create a network ACL (NACL) for the subnet hosting the SageMaker training job and add a deny rule to block traffic from the specific IP address

Network ACLs (NACLs) are associated with subnets and allow for controlling traffic at the subnet level by applying allow or deny rules. A NACL with a deny rule for the specific IP address can block traffic before it reaches the SageMaker domain.

Why NACL works best for this use case:

Subnet-level filtering - NACLs apply rules for both inbound and outbound traffic to the subnet, effectively blocking unwanted traffic.

Explicit deny rules - Unlike security groups, NACLs support deny rules, making them suitable for blocking specific IPs.

Efficient traffic control - The NACL rule will apply without impacting the broader network configuration or user access.

Control subnet traffic with network access control lists:  via - https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html

Incorrect options:

Update the security group associated with the SageMaker training job to include a deny rule for traffic from the specific IP address - Security groups are stateful and only allow allow rules; they do not support explicit deny rules for traffic. This makes them unsuitable for blocking specific IP addresses.

Modify the VPC route table to direct all traffic from the specific IP address to a target with a blackhole routing state - This option acts as a distractor. VPC route tables are used to route traffic to specific destinations, such as subnets or internet gateways. They do not provide a mechanism to block traffic. The blackhole state indicates that the route's target isn't available (for example, the specified gateway isn't attached to the VPC, or the specified NAT instance has been terminated).

Enable AWS WAF (Web Application Firewall) for the SageMaker domain and create a rule to block traffic from the specific IP address - AWS WAF is designed for web application traffic, not for managing access to SageMaker domains. It cannot be directly integrated with SageMaker domain traffic filtering.

References:

https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html

https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_Route.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 17

A marketing analyst has developed a binary classification model using an external framework and stored the model artifacts in an Amazon S3 bucket. The analyst now wants to make the model available to a colleague using Amazon SageMaker Canvas for further tuning and analysis. Both the analyst and the colleague are part of the same SageMaker domain.

What requirements must be fulfilled to ensure the model can be shared with the SageMaker Canvas user? (Select two)

The Canvas user must be configured to the SageMaker domain. Autopilot should be used to share the ML model with SageMaker Canvas

The Canvas user must be configured in the SageMaker domain. The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored

Latest version of Amazon SageMaker Studio is needed, SageMaker Studio Classic does not support sharing models to Canvas

Selección correcta
The model must be registered in the SageMaker Model Registry

Selección correcta
The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored

Explicación general
Correct option:

The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored

The model must be registered in the SageMaker Model Registry

To bring your model into SageMaker Canvas, you need to meet the following requirements:

You must have a Amazon SageMaker Studio Classic user who has onboarded to Amazon SageMaker AI domain. The Studio Classic user must be in the same domain as the Canvas user. This configuration is already implemented according to the provided use case.

For any model that you’ve built outside of SageMaker AI, you must register your model in Model Registry before importing it into Canvas.

The Canvas user with whom you want to share your model must have permission to access the Amazon S3 bucket in which you store your datasets and model artifacts.

Prerequisites to bring your model into SageMaker Canvas:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-byom.html

Incorrect options:

The Canvas user must be configured to the SageMaker domain. Autopilot should be used to share the ML model with SageMaker Canvas - Autopilot automates key tasks of an automatic ML (AutoML) process like exploring data, selecting the relevant algorithm for the problem type, and then training and tuning it. Autopilot cannot be used to import models built outside of SageMaker AI into SageMaker Canvas.

Latest version of Amazon SageMaker Studio is needed, SageMaker Studio Classic does not support sharing models to Canvas - Currently, you can only share models to Canvas (or view shared Canvas models) in Studio Classic. If you’re currently using the latest version of Studio, you must run Studio Classic from within the latest version of Studio to share models to Canvas or view models shared from Canvas.

The Canvas user must be configured in the SageMaker domain. The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored - This option acts as a distractor. Configuring the Canvas user to the SageMaker domain is redundant, as the user is already part of the SageMaker domain, as stated in the use case.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-byom.html

https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-collaborate-permissions.html

Temática
ML Model Development
Pregunta 18

You are a data engineer responsible for monitoring the performance of a suite of machine learning models deployed across multiple environments at an e-commerce company. The models are used for various tasks, including recommendation engines, demand forecasting, and customer segmentation. To ensure that these models are performing optimally, you need to set up a centralized dashboard that allows stakeholders to monitor key performance metrics such as latency, accuracy, throughput, and resource utilization. The dashboard should be user-friendly, provide insights at a glance, and support both technical and non-technical users.

Which approach is MOST SUITABLE for setting up a dashboard to monitor the performance metrics of these ML models?

Set up a CloudWatch Dashboard to aggregate and display performance metrics such as CPU utilization, memory usage, and latency from various services. Use Amazon SNS to send daily reports based on these metrics to stakeholders

Create a custom web application that pulls data from CloudWatch Logs, generating real-time visualizations of performance metrics. Embed the application within the company’s internal portal for easy access

Use AWS Lambda to query CloudWatch Logs and send the results to an S3 bucket daily, where stakeholders can manually review the metrics and create their own visualizations using spreadsheet software

Respuesta correcta
Use Amazon QuickSight to create a visual dashboard that integrates data from Amazon CloudWatch Logs via Amazon S3, providing interactive charts and graphs that allow stakeholders to drill down into specific metrics as needed

Explicación general
Correct option:

Use Amazon QuickSight to create a visual dashboard that integrates data from Amazon CloudWatch Logs via Amazon S3, providing interactive charts and graphs that allow stakeholders to drill down into specific metrics as needed

Amazon QuickSight is a powerful BI tool that allows you to create interactive and visually appealing dashboards. It can integrate data from multiple sources, including CloudWatch Logs (via a CloudWatch Logs subscription that sends any incoming log events that match your defined filters to your Amazon Data Firehose delivery stream and finally stored in Amazon S3), making it an ideal choice for monitoring the performance of ML models across different environments. The ability to create interactive charts and drill down into specific metrics makes the dashboard accessible to both technical and non-technical users, providing insights at various levels of detail.

Incorrect options:

Set up a CloudWatch Dashboard to aggregate and display performance metrics such as CPU utilization, memory usage, and latency from various services. Use Amazon SNS to send daily reports based on these metrics to stakeholders - CloudWatch Dashboards are excellent for aggregating and displaying performance metrics directly from AWS services. However, they are more suited to technical users and might lack the advanced visualization capabilities and interactivity that stakeholders might need for deeper analysis.

Create a custom web application that pulls data from CloudWatch Logs, generating real-time visualizations of performance metrics. Embed the application within the company’s internal portal for easy access - A custom web application could provide the required functionality, but it would require significant development and maintenance efforts. QuickSight, on the other hand, offers similar capabilities out-of-the-box with less overhead.

Use AWS Lambda to query CloudWatch Logs and send the results to an S3 bucket daily, where stakeholders can manually review the metrics and create their own visualizations using spreadsheet software While using AWS Lambda and S3 to gather CloudWatch Logs data is possible, this option results in a manual process for visualization and lacks the real-time, interactive capabilities needed for effective monitoring. Stakeholders would also need to create their own visualizations, which may not be practical.

References:

https://aws.amazon.com/quicksight/

https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 19

A company's data scientist has deployed a sentiment analysis machine learning model to an Amazon SageMaker endpoint. Given the involvement of multiple stakeholders, the data scientist needs to explain how the model generates its predictions and clarify the factors influencing the model's decisions.

Which solution will provide an explanation for the model's predictions?

Respuesta correcta
Configure a SageMaker Clarify processing job to analyze your model for bias and explainability

Use Amazon SageMaker Model Monitor that offers tools to provide explainations on model quality

Leverage CloudWatch Insights derived from SageMaker AI endpoint invocation metrics to analyze your model for bias and explainability

Use Amazon Textract to automatically extract text and data from the ML model and provide insights on its predictions

Explicación general
Correct option:

Configure a SageMaker Clarify processing job to analyze your model for bias and explainability

You can use Amazon SageMaker Clarify to understand fairness and model explainability and to explain and detect bias in your models. You can configure an SageMaker Clarify processing job to compute bias metrics and feature attributions and generate reports for model explainability.

Machine learning (ML) models are helping make decisions in domains including financial services, healthcare, education, and human resources. Policymakers, regulators, and advocates have raised awareness about the ethical and policy challenges posed by ML and data-driven systems. Amazon SageMaker Clarify can help you understand why your ML model made a specific prediction and whether this bias impacts this prediction during training or inference.

More on SageMaker Clarify:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html

Incorrect options:

Use Amazon SageMaker Model Monitor that offers tools to provide explainations on model quality - Amazon SageMaker Model Monitor helps to continuously monitor the quality of your machine learning models in real time. Amazon SageMaker Model Monitor enables you to set up an automated alert triggering system when there are deviations in the model quality, such as data drift and anomalies. This option acts as a distractor, as SageMaker Model Monitor cannot help explain how the model generates its predictions and clarify the factors influencing the model's decisions.

Use Amazon Textract to automatically extract text and data from the ML model and provide insights on its predictions - Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents. Amazon Textract can detect words and lines of words in formatted text. Textract can analyze documents for items like tables, key-value pairs, and selection elements. Textract is irrelevant for the given use case.

Leverage CloudWatch Insights derived from SageMaker AI endpoint invocation metrics to analyze your model for bias and explainability - You can monitor Amazon SageMaker AI using Amazon CloudWatch, which collects raw data and processes it into readable, near real-time metrics. Invocation metrics like latency, error rates, and invocation counts offer insights into model performance and usage patterns but do not inherently reveal biases or explain decision-making. So, this option acts as a distractor.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality.html

https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 20

A financial services company operates in a hybrid cloud environment and has deployed a machine learning model on premises to provide personalized financial recommendations. The model processes data stored in Amazon S3, which includes sensitive customer information such as Social Security numbers and financial transaction details. The company must ensure that all sensitive data is identified and removed before it is processed by the model. The company needs to implement a solution that can handle these requirements with the least operational overhead.

Which solution will meet these requirements?

Deploy an AWS Glue ETL job to scan the S3 bucket, detect sensitive data using a custom Python script, and redact the sensitive information

Use AWS DataBrew to create a data transformation pipeline that scans for sensitive data and applies predefined rules to clean the dataset

Leverage Amazon Comprehend to classify and redact sensitive entities from the data stored in Amazon S3 before it is used by the model

Respuesta correcta
Use Amazon Macie to automatically identify sensitive data in Amazon S3 and then call a Lambda function to remove the sensitive data

Explicación general
Correct option:

Use Amazon Macie to automatically identify sensitive data in Amazon S3 and then call a Lambda function to remove the sensitive data

Amazon Macie is specifically designed for detecting and managing sensitive data stored in Amazon S3. It offers:

Automated scans for sensitive data such as personally identifiable information (PII) and financial data.

Detailed findings and alerts that can be used to take corrective actions.

Minimal operational overhead since it is fully managed and does not require custom coding or manual interventions.

This makes Macie the most efficient and secure solution for identifying and removing sensitive data in this scenario.

Automate the archival and deletion of sensitive data using Amazon Macie:  via - https://aws.amazon.com/blogs/big-data/automate-the-archival-and-deletion-of-sensitive-data-using-amazon-macie/

Incorrect options:

Leverage Amazon Comprehend to classify and redact sensitive entities from the data stored in Amazon S3 before it is used by the model - Amazon Comprehend specializes in text analytics, not large-scale sensitive data scanning in S3. It requires additional integration to handle such use cases effectively.

Use AWS DataBrew to create a data transformation pipeline that scans for sensitive data and applies predefined rules to clean the dataset - DataBrew is excellent for preparing and transforming datasets, but it lacks the advanced automated sensitive data detection capabilities of Macie.

Deploy an AWS Glue ETL job to scan the S3 bucket, detect sensitive data using a custom Python script, and redact the sensitive information - Custom development is required, thereby increasing operational overhead. AWS Glue is better suited for general data transformation rather than automated sensitive data detection.

References:

https://aws.amazon.com/blogs/big-data/automate-the-archival-and-deletion-of-sensitive-data-using-amazon-macie/

https://aws.amazon.com/blogs/security/use-macie-to-discover-sensitive-data-as-part-of-automated-data-pipelines/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 21

A financial institution uses an Amazon Redshift database to store transactional and customer data, including sensitive information such as personally identifiable information (PII). A data analyst needs access to specific columns of the database to perform customer behavior analysis. The institution wants to ensure that sensitive data, such as PII, is not exposed to the analyst while maintaining the integrity of the source data. The solution must avoid duplicating or transforming the data into a separate dataset.

What do you recommend to address these requirements with the LEAST implementation effort?

Export the data to Amazon S3, apply anonymization to sensitive fields using AWS Glue, and allow the analyst to query the transformed data

Respuesta correcta
Implement Amazon Redshift dynamic data masking policies to obfuscate sensitive columns dynamically while allowing the analyst to access permitted data

Create a materialized view in Amazon Redshift that excludes sensitive fields and grant the analyst access to the view

Use Amazon Redshift row-level security to restrict access to sensitive rows while allowing access to all columns

Explicación general
Correct option:

Implement Amazon Redshift dynamic data masking policies to obfuscate sensitive columns dynamically while allowing the analyst to access permitted data

Dynamic data masking in Amazon Redshift provides a seamless way to protect sensitive information, such as personally identifiable information (PII), while maintaining the integrity of the source data. This feature dynamically obscures sensitive columns at query time, ensuring that users without explicit permissions only see masked or obfuscated data. Unlike other approaches, dynamic data masking eliminates the need for duplicating or transforming datasets, reducing operational complexity and storage overhead.

It integrates directly with Redshift’s access control mechanisms, enabling granular permissions that can be tailored to each user or role. This solution allows the data analyst to access the required data without exposing sensitive information, meeting the institution's requirements for data security and operational efficiency with minimal implementation effort.

 via - https://docs.aws.amazon.com/redshift/latest/dg/t_ddm.html

Incorrect options:

Create a materialized view in Amazon Redshift that excludes sensitive fields and grant the analyst access to the view - Materialized views require periodic refreshes and consume additional storage. They are static representations and do not dynamically adapt to changing data access needs, increasing operational complexity.

 via - https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html

Use Amazon Redshift row-level security to restrict access to sensitive rows while allowing access to all columns - Row-level security controls access to specific rows but does not address column-level sensitivity, leaving sensitive fields unprotected.

Export the data to Amazon S3, apply anonymization to sensitive fields using AWS Glue, and allow the analyst to query the transformed data - Exporting and transforming data adds unnecessary operational overhead and costs. It also introduces data duplication, increasing maintenance complexity.

References:

https://docs.aws.amazon.com/redshift/latest/dg/t_ddm.html

https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html

https://docs.aws.amazon.com/redshift/latest/dg/t_rls.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 22

What is a key difference in feature engineering tasks for structured data compared to unstructured data in the context of machine learning?

Feature engineering tasks for structured data and unstructured data are identical and do not vary based on data type

Respuesta correcta
Feature engineering for structured data often involves tasks such as normalization and handling missing values, while for unstructured data, it involves tasks such as tokenization and vectorization

Feature engineering for structured data is not necessary as the data is already in a usable format, whereas for unstructured data, extensive preprocessing is always required

Feature engineering for structured data focuses on image recognition, whereas for unstructured data, it focuses on numerical data analysis

Explicación general
Correct option:

Feature engineering for structured data often involves tasks such as normalization and handling missing values, while for unstructured data, it involves tasks such as tokenization and vectorization

Feature engineering for structured data typically includes tasks like normalization, handling missing values, and encoding categorical variables. For unstructured data, such as text or images, feature engineering involves different tasks like tokenization (breaking down text into tokens), vectorization (converting text or images into numerical vectors), and extracting features that can represent the content meaningfully.

Incorrect options:

Feature engineering for structured data focuses on image recognition, whereas for unstructured data, it focuses on numerical data analysis - Structured data can include numerical and categorical data, while unstructured data includes text, images, audio, etc. The focus is not limited to image recognition or numerical data analysis.

Feature engineering for structured data is not necessary as the data is already in a usable format, whereas for unstructured data, extensive preprocessing is always required - Feature engineering is important for both structured and unstructured data. While structured data may require less preprocessing, tasks like normalization and handling missing values are still crucial. Unstructured data typically requires more extensive preprocessing.

Feature engineering tasks for structured data and unstructured data are identical and do not vary based on data type - Feature engineering tasks vary significantly between structured and unstructured data due to the inherent differences in data types and the requirements for preprocessing each type.

Reference:

https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 23

A retail company uses a customer service chatbot powered by a large language model (LLM) through Amazon Bedrock to handle product inquiries and returns. Customers have reported that the chatbot gives slightly different answers when asked the same questions about return policies or product details. The company needs to ensure the chatbot provides consistent responses.

Which solution will meet these requirements?

Enable caching in the chatbot application to reuse previously generated responses for frequently asked questions

Respuesta correcta
Adjust the temperature parameter in the Amazon Bedrock API to a lower value and decrease the top-K parameter to limit the number of possible tokens the model can choose from, ensuring consistent and controlled responses

Retrain the LLM with a retail-specific dataset to improve consistency in responses related to product information and return policies

Adjust the temperature parameter in the Amazon Bedrock API to a higher value and increase the top-K parameter to limit the number of possible tokens the model can choose from, ensuring consistent and controlled responses

Explicación general
Correct option:

Adjust the temperature parameter in the Amazon Bedrock API to a lower value and decrease the top-K parameter to limit the number of possible tokens the model can choose from, ensuring consistent and controlled responses

Temperature and top-K parameters directly control the randomness and variability of the model’s responses:

Temperature - A lower value (e.g., 0.2) reduces randomness, ensuring the model generates consistent outputs.

Top-K - Restricts the model to selecting from the top K most likely tokens (e.g., K = 50), which balances consistency and creativity by limiting the range of possibilities. Choose a lower value to decrease the size of the pool and limit the options to more likely outputs.

So, setting a low value for the temperature as well as the top-K parameter allows the chatbot to provide consistent and accurate responses while maintaining flexibility for slightly varied inputs.

 via - https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html

Incorrect options:

Adjust the temperature parameter in the Amazon Bedrock API to a higher value and increase the top-K parameter to limit the number of possible tokens the model can choose from, ensuring consistent and controlled responses - This option contradicts the explanation provided above, so this option is incorrect.

Retrain the LLM with a retail-specific dataset to improve consistency in responses related to product information and return policies - Retraining is resource-intensive and unnecessary for achieving consistency when temperature and top-K adjustments can accomplish this more efficiently.

Enable caching in the chatbot application to reuse previously generated responses for frequently asked questions - Caching may reduce some variability but does not address the root cause of inconsistency and could lead to outdated or irrelevant responses for dynamic queries.

Reference:

https://docs.aws.amazon.com/bedrock/latest/userguide/inference-parameters.html

Temática
ML Model Development
Pregunta 24

A travel company is building a customer support application that provides automated responses to user queries. The application uses Amazon Q Business to retrieve answers to customer questions about travel packages. However, the company must ensure that the API responses do not include any references to outdated or unavailable travel destinations.

Which solution will meet this requirement?

Respuesta correcta
Use the blocked phrase functionality in Amazon Q Business to filter out responses containing references to outdated travel destinations before returning results

Use Amazon Comprehend to analyze the text of API responses and block any responses containing outdated travel destinations

Set up an Amazon Lambda function to process API responses from Amazon Q Business and remove any references to outdated destinations before displaying the results

Configure an Amazon Kendra retriever to exclude any content related to outdated travel destinations from the response dataset

Explicación general
Correct option:

Use the blocked phrase functionality in Amazon Q Business to filter out responses containing references to outdated travel destinations before returning results

The blocked phrase functionality in Amazon Q Business is specifically designed to filter out specific terms or phrases from API responses. By leveraging this feature, the travel company can ensure that outdated or unavailable destinations are excluded from the answers provided to customers. This approach integrates seamlessly with Amazon Q Business, eliminating the need for additional custom processing. It minimizes operational overhead by applying filters directly at the API level, ensuring accurate and real-time responses.

The following diagram shows you how Amazon Q Business uses these guardrails to direct queries:  via - https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/guardrails-global-controls.html

Incorrect options:

Configure an Amazon Kendra retriever to exclude any content related to outdated travel destinations from the response dataset - Amazon Kendra is designed for enterprise search and retrieval, not filtering specific phrases in API responses. It cannot enforce filtering directly within Amazon Q Business responses.

Set up an Amazon Lambda function to process API responses from Amazon Q Business and remove any references to outdated destinations before displaying the results - This approach introduces unnecessary latency and operational complexity, as filtering is handled externally rather than within the Amazon Q Business API.

Use Amazon Comprehend to analyze the text of API responses and block any responses containing outdated travel destinations - Amazon Comprehend is a Natural Language Processing (NLP) service designed for text analysis. It is not suitable for filtering API responses from Amazon Q Business.

References:

https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/guardrails-global-controls.html

https://docs.aws.amazon.com/amazonq/latest/api-reference/API_BlockedPhrasesConfiguration.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 25

A global news agency operates a document retrieval system that helps journalists find relevant articles using natural language queries. The existing system uses a vector database for storing embeddings of news articles. The agency has migrated all its archived articles to an Amazon S3 bucket and now needs a solution on AWS to provide semantic search capabilities for these articles.

Which solution will meet these requirements most efficiently?

Respuesta correcta
Use Amazon Kendra to index the archived articles in the S3 bucket and enable semantic search with natural language queries

Use Amazon Comprehend to extract key phrases and entities from the articles and build a custom search engine for semantic retrieval

Leverage AWS Glue to preprocess the archived articles and create a metadata catalog, then use Athena to perform semantic queries on the catalog

Deploy a SageMaker endpoint with a fine-tuned language model to process user queries and manually integrate the model with S3 for retrieving articles

Explicación general
Correct option:

Use Amazon Kendra to index the archived articles in the S3 bucket and enable semantic search with natural language queries

Amazon Kendra is the ideal solution for this scenario because it is specifically designed for semantic search and supports natural language queries. It integrates directly with Amazon S3 to index documents and provides advanced search capabilities with minimal development effort.

Key Benefits:

Native integration with S3 - Kendra can easily index the archived articles in the S3 bucket.

Semantic search capabilities - Kendra supports natural language understanding for accurate and context-aware search results.

Easy setup - Fully managed service reduces the need for custom development and ongoing maintenance.

 via - https://aws.amazon.com/kendra/

Incorrect options:

Use Amazon Comprehend to extract key phrases and entities from the articles and build a custom search engine for semantic retrieval - Amazon Comprehend is designed for text analytics, not for building semantic search engines. Creating a custom solution would require substantial additional effort.

Leverage AWS Glue to preprocess the archived articles and create a metadata catalog, then use Athena to perform semantic queries on the catalog - Glue and Athena are ideal for structured data analysis but do not support semantic search or natural language queries.

Deploy a SageMaker endpoint with a fine-tuned language model to process user queries and manually integrate the model with S3 for retrieving articles - This approach requires building and managing a custom pipeline for semantic search, which increases complexity and development time compared to using Amazon Kendra.

Reference:

https://aws.amazon.com/kendra/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 26

Your data science team is working on developing a machine learning model to predict customer churn. The dataset that you are using contains hundreds of features, but you suspect that not all of these features are equally important for the model's accuracy. To improve the model's performance and reduce its complexity, the team wants to focus on selecting only the most relevant features that contribute significantly to minimizing the model's error rate.

Which feature engineering process should your team apply to select a subset of features that are the most relevant towards minimizing the error rate of the trained model?

Feature creation

Feature transformation

Respuesta correcta
Feature selection

Feature extraction

Explicación general
Correct option:

Feature selection

Feature selection is the process of selecting a subset of extracted features. This is the subset that is relevant and contributes to minimizing the error rate of a trained model. Feature importance score and correlation matrix can be factors in selecting the most relevant features for model training.

Incorrect options:

Feature creation - Feature creation refers to the creation of new features from existing data to help with better predictions. Examples of feature creation include: one-hot-encoding, binning, splitting, and calculated features.

Feature transformation - Feature transformation and imputation include steps for replacing missing features or features that are not valid. Some techniques include: forming Cartesian products of features, non-linear transformations (such as binning numeric variables into categories), and creating domain-specific features.

Feature extraction - Feature extraction involves reducing the amount of data to be processed using dimensionality reduction techniques. These techniques include: Principal Components Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA).

Reference:

https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/feature-engineering.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 27

A data science team is working on a machine learning project that requires them to ingest large volumes of raw data for analysis and feature engineering. The team plans to use Amazon SageMaker for the project, and they need to efficiently ingest the data into a platform where they can perform transformations and store features for future model training.

Which of the following approaches is the most efficient for ingesting data, performing data transformations, and then storing the engineered features?

Ingest the data into AWS Glue for transformations, then store the transformed data in Amazon S3 and load it into Amazon SageMaker using the AWS SDK for Python

Respuesta correcta
Ingest the data into Amazon SageMaker Data Wrangler, perform transformations, and then store the transformed data in SageMaker Feature Store

Load the data into an Amazon RDS database, then use SageMaker Clarify to ingest and transform the data for storing in the Feature Store

Use Amazon S3 to store the raw data, then directly connect SageMaker Feature Store to ingest and transform the data

Explicación general
Correct option:

Ingest the data into Amazon SageMaker Data Wrangler, perform transformations, and then store the transformed data in SageMaker Feature Store

This is the optimal approach because SageMaker Data Wrangler is specifically designed to perform data transformations through a visual interface. After the transformations, the cleaned and engineered features can be stored in the SageMaker Feature Store for efficient retrieval during model training.

Incorrect options:

Use Amazon S3 to store the raw data, then directly connect SageMaker Feature Store to ingest and transform the data - While Amazon S3 is commonly used to store raw data, SageMaker Feature Store is designed to store engineered features, not to perform data transformations. A Data Wrangler is needed to transform the data before it’s stored in the Feature Store.

By using feature processing, you can specify your batch data source and feature transformation function (for example, count of product views or time window aggregates) and SageMaker Feature Store transforms the data at the time of ingest into ML features. However, performing complex transformations on petabyte-scale data is most efficient with SageMaker Data Wrangler, which is specifically designed for this purpose.

Ingest the data into AWS Glue for transformations, then store the transformed data in Amazon S3 and load it into Amazon SageMaker using the AWS SDK for Python - This option doesn't provide a method for storing the engineered features, making it an unsuitable choice.

Load the data into an Amazon RDS database, then use SageMaker Clarify to ingest and transform the data for storing in the Feature Store - Amazon RDS is not necessary for this workflow, and SageMaker Clarify is used for bias detection and explainability rather than data transformation. Data Wrangler is the correct tool for transformations, followed by storing features in the Feature Store.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler.html

https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 28

A financial institution has developed and deployed a machine learning model to predict credit scores. The institution is required to comply with regulatory guidelines for transparency, fairness, and security in its ML workflows. The team needs a solution that will enable them to track and audit the model’s decisions, ensure model explainability, and monitor compliance.

Which of the following AWS services would best support this need for governance in the machine learning lifecycle?

AWS CloudTrail

Amazon SageMaker Model Monitor

Respuesta correcta
Amazon SageMaker Clarify

Amazon SageMaker Autopilot

Explicación general
Correct option:

Amazon SageMaker Clarify

Machine learning (ML) models are helping make decisions in domains including financial services, healthcare, education, and human resources. Policymakers, regulators, and advocates have raised awareness about the ethical and policy challenges posed by ML and data-driven systems. Amazon SageMaker Clarify can help you understand why your ML model made a specific prediction and whether this bias impacts this prediction during training or inference. SageMaker Clarify also provides tools that can help you build less biased and more understandable machine learning models. SageMaker Clarify can also generate model governance reports that you can provide to risk and compliance teams and external regulators. With SageMaker Clarify, you can do the following:

Detect bias in and help explain your model predictions.

Identify types of bias in pre-training data.

Identify types of bias in post-training data that can emerge during training or when your model is in production.

SageMaker Clarify helps explain how your models make predictions using feature attributions. It can also monitor inference models that are in production for both bias and feature attribution drift. This information can help you in the following areas:

Regulatory – Policymakers and other regulators can have concerns about discriminatory impacts of decisions that use output from ML models. For example, an ML model may encode bias and influence an automated decision.

Business – Regulated domains may need reliable explanations for how ML models make predictions. Model explainability may be particularly important to industries that depend on reliability, safety, and compliance. These can include financial services, human resources, healthcare, and automated transportation. For example, lending applications may need to provide explanations about how ML models made certain predictions to loan officers, forecasters, and customers.

Data Science – Data scientists and ML engineers can debug and improve ML models when they can determine if a model is making inferences based on noisy or irrelevant features. They can also understand the limitations of their models and failure modes that their models may encounter.

Fairness and explainability by design in the ML lifecycle:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html

Incorrect options:

Amazon SageMaker Model Monitor - Amazon SageMaker Model Monitor is designed to monitor deployed models for drift, bias, and data quality, but it does not directly address transparency, explainability, or regulatory compliance in the model development and decision-making process. SageMaker Model Monitor depends on SageMaker Clarify for bias detection and explainability, whereas SageMaker Clarify is a standalone service that can be used independently of SageMaker Model Monitor.

AWS CloudTrail - AWS CloudTrail is important for tracking and auditing API activity, providing security and operational monitoring. While it can be used for logging events, it does not have the specialized capabilities needed for governance in machine learning, such as ensuring explainability, transparency, or fairness in model decisions.

Amazon SageMaker Autopilot - SageMaker Autopilot is designed for users with limited machine learning experience. It automatically handles the machine learning workflow, from preprocessing the dataset to choosing the best model and optimizing it. The business analyst can simply upload their data, and Autopilot will handle the rest, making it the ideal choice for someone with little coding knowledge. Autopilot does not offer capabilities needed for governance in machine learning, such as ensuring explainability, transparency, or fairness in model decisions.

References:

https://aws.amazon.com/sagemaker-ai/clarify/

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 29

A company uses Amazon SageMaker Studio to develop its ML models. A team of ten developers are working on a proof-of-concept model with all the users linked to a single SageMaker Studio domain. The company needs an automated alert system that notifies them when the SageMaker compute costs exceed a specific threshold.

Which of the following options would meet these requirements?

Add resource tagging to IM profile of each user. Configure AWS Trusted Advisor to track the resource costs and generate recommendations

Add custom tags to resources by editing the SageMaker user profile in the SageMaker domain. Modify the custom tag propagation settings at the user profile level to suit each user's requirements. Configure AWS Budgets to send an alert when the threshold is breached

Add tags to the user profile in the SageMaker domain. Configure AWS Cost Explorer to send an alert when the threshold is breached

Respuesta correcta
Add tags to the user profile in the SageMaker domain. Configure AWS Budgets to send an alert when the threshold is breached

Explicación general
Correct option:

Add tags to the user profile in the SageMaker domain. Configure AWS Budgets to send an alert when the threshold is breached

A user profile represents a single user within an Amazon SageMaker AI domain. The user profile is the main way to reference a user for the purposes of sharing, reporting, and other user-oriented features. This entity is created when a user is onboarded to the Amazon SageMaker AI domain. You can add tags to the user profile. All resources that the user profile creates will have a domain ARN tag and a user profile ARN tag. The domain ARN tag is based on domain ID, while the user profile ARN tag is based on the user profile name.

Amazon SageMaker AI domain overview:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html

AWS Budgets is an excellent way to provide an early warning if spend spikes unexpectedly. You can create custom budgets that alert you when your ML costs and usage exceed (or are forecasted to exceed) your user-defined thresholds. With AWS Budgets, you can monitor your total monthly ML costs or filter your budgets to track costs associated with specific usage dimensions.

With budget alerts, you can send notifications when your budget limits are (or are about to be) exceeded. These alerts can also be posted to an Amazon Simple Notification Service (Amazon SNS) topic. An AWS Lambda function that subscribes to the SNS topic is then invoked, and any programmatically implementable actions can be taken.

Budget monitoring using tags:  via - https://aws.amazon.com/blogs/machine-learning/set-up-enterprise-level-cost-allocation-for-ml-environments-and-workloads-using-resource-tagging-in-amazon-sagemaker/

Incorrect options:

Add custom tags to resources by editing the SageMaker user profile in the SageMaker domain. Modify the custom tag propagation settings at the user profile level to suit each user's requirements. Configure AWS Budgets to send an alert when the threshold is breached - Custom tag propagation can only be set at the domain level, which means that all users and spaces in a domain use the feature when it is activated. It is not possible to modify custom tag propagation settings for a specific user profile.

Add tags to the user profile in the SageMaker domain. Configure AWS Cost Explorer to send an alert when the threshold is breached - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You need to use AWS Budgets to send alerts for the given use case.

Add resource tagging to IM profile of each user. Configure AWS Trusted Advisor to track the resource costs and generate recommendations - AWS Trusted Advisor inspects your AWS environment and provides recommendations to improve performance, security, and cost optimization. You need to use AWS Budgets to send alerts for the given use case.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/domain-user-profile-add.html

https://aws.amazon.com/blogs/machine-learning/set-up-enterprise-level-cost-allocation-for-ml-environments-and-workloads-using-resource-tagging-in-amazon-sagemaker/

https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 30

You are a data scientist working on a loan approval model for a bank. The model predicts whether a loan application should be approved or rejected based on various features such as income, credit score, and employment history. The bank is particularly concerned about ensuring that the model is fair and does not discriminate against any demographic group, such as age, gender, or ethnicity. To address this, you need to select the appropriate evaluation metrics to assess both the model’s performance and any potential bias.

Given these requirements, which combination of evaluation metrics and bias detection methods is MOST APPROPRIATE for ensuring fair and accurate model predictions?

Respuesta correcta
Evaluate the model using F1 score and AUC-ROC, and assess whether the model has similar true positive rates across different demographic groups

Use accuracy as the primary evaluation metric and perform feature importance analysis to ensure that the model’s decisions are driven by relevant features

Evaluate the model using F1 score and AUC-ROC, and and check for bias by comparing feature distributions across demographic groups

Measure the model’s performance using accuracy and AUC-ROC, and check for bias by comparing feature distributions across demographic groups

Explicación general
Correct option:

Evaluate the model using F1 score and AUC-ROC, and assess whether the model has similar true positive rates across different demographic groups

The F1 score balances precision and recall, making it suitable for imbalanced datasets. AUC-ROC provides a comprehensive view of model performance across different thresholds. You must also assess whether the model gives different demographic groups similar true positive rates, making it a key measure for detecting bias and ensuring fairness in sensitive applications like loan approvals.

Incorrect options:

Evaluate the model using F1 score and AUC-ROC, and and check for bias by comparing feature distributions across demographic groups

Measure the model’s performance using accuracy and AUC-ROC, and check for bias by comparing feature distributions across demographic groups

Simply comparing feature distributions across demographic groups does not directly address potential bias in model predictions. So, both these options are incorrect.

Use accuracy as the primary evaluation metric and perform feature importance analysis to ensure that the model’s decisions are driven by relevant features - Accuracy alone is not sufficient in scenarios with imbalanced data or where fairness is a concern. Feature importance analysis helps explain model predictions but does not directly address bias across demographic groups.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-data-bias-metric-cddl.html

https://aws.amazon.com/blogs/machine-learning/learn-how-amazon-sagemaker-clarify-helps-detect-bias/

Temática
ML Model Development
Pregunta 31

A machine learning (ML) engineer has developed a model using Amazon SageMaker and deployed it into production. The engineer wants to track the performance of the model in real-time, including metrics such as prediction accuracy and model drift, and ensure that the model maintains high quality over time.

Which of the following AWS services is the most suitable for monitoring the model's performance for the given scenario?

Amazon CloudWatch

Respuesta correcta
Amazon SageMaker Model Monitor

Amazon SageMaker Autopilot

AWS Glue

Explicación general
Correct option:

Amazon SageMaker Model Monitor

Amazon SageMaker Model Monitor monitors the quality of Amazon SageMaker AI machine learning models in production. With Model Monitor, you can set up:

Continuous monitoring with a real-time endpoint.

Continuous monitoring with a batch transform job that runs regularly.

On-schedule monitoring for asynchronous batch transform jobs.

Model Monitor provides the following types of monitoring:

Data quality - Monitor drift in data quality.

Model quality - Monitor drift in model quality metrics, such as accuracy.

Bias drift for models in production - Monitor bias in your model's predictions.

Feature attribution drift for models in production - Monitor drift in feature attribution.

After you deploy a model into your production environment, use Amazon SageMaker Model Monitor to continuously monitor the quality of your machine learning models in real time. Amazon SageMaker Model Monitor enables you to set up an automated alert triggering system when there are deviations in the model quality, such as data drift and anomalies. Amazon CloudWatch Logs collects log files of monitoring the model status and notifies when the quality of your model hits certain thresholds that you preset.

Incorrect options:

Amazon CloudWatch - Amazon CloudWatch is a general monitoring and logging service for AWS resources and applications. While it can track certain system-level metrics, it does not specifically cater to monitoring machine learning models or detecting model drift.

Amazon SageMaker Autopilot - SageMaker Autopilot is focused on automating the model building process, including data preprocessing, model selection, and training. However, it does not provide real-time monitoring or performance tracking for models after they have been deployed into production.

AWS Glue - AWS Glue is a data integration service primarily used for extracting, transforming, and loading (ETL) data. AWS Glue is used to prepare data for analysis and machine learning, not for monitoring model performance. It can automate data processing workflows but lacks features for tracking model metrics or detecting performance issues like model drift, which are crucial for real-time monitoring of deployed models.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html

https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-model-monitor.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 32

A healthcare company is running containerized ML applications to process patient data and generate predictive insights. These applications are deployed across Amazon EC2 instances, Amazon Elastic Container Service (Amazon ECS) cluster and AWS Lambda functions. The EC2 and ECS workloads store predictions and artifacts on Amazon Elastic Block Store (Amazon EBS) volumes. To optimize costs, the company wants to identify inefficiently used resources and receive actionable recommendations to reduce expenses.

What is the most efficient solution to achieve this goal with minimal development effort?

Respuesta correcta
Use AWS Compute Optimizer to analyze the specifications and utilization metrics of your AWS compute resources

Use AWS Budgets to analyze the specifications and utilization metrics of your AWS resources

Use AWS Trusted Advisor to analyze the specifications and utilization metrics of your AWS resources

Use AWS Cost Explorer to analyze the specifications and utilization metrics of your AWS compute resources

Explicación general
Correct option:

Use AWS Compute Optimizer to analyze the specifications and utilization metrics of your AWS compute resources

AWS Compute Optimizer is a service that analyzes your AWS resources' configuration and utilization metrics to provide you with rightsizing recommendations. It reports whether your resources are optimal, and generates optimization recommendations to reduce the cost and improve the performance of your workloads. Compute Optimizer also provides graphs showing recent utilization metric history data, as well as projected utilization for recommendations, which you can use to evaluate which recommendation provides the best price-performance trade-off. The analysis and visualization of your usage patterns can help you decide when to move or resize your running resources, and still meet your performance and capacity requirements.

AWS Compute Optimizer delivers intuitive and easily actionable AWS resource recommendations to help you quickly identify optimal AWS resources for your workloads without requiring specialized expertise or investing substantial time and money. The Compute Optimizer console provides you with a global, cross-account view of all resources analyzed by Compute Optimizer and recommendations so that you can quickly identify the most impactful optimization opportunities.

You can quickly identify and prioritize top optimization opportunities through two new sets of dashboard-level metrics: savings opportunity and performance improvement opportunity.

Savings opportunity metrics quantify the Amazon EC2, Amazon EBS, Amazon ECS services on AWS Fargate, commercial software licenses, Amazon RDS, and AWS Lambda monthly savings you can achieve at the account level, resource type level, or resource level by adopting AWS Compute Optimizer recommendations. You can use these metrics to evaluate and prioritize cost efficiency opportunities, as well as monitor your cost efficiency over time. Performance improvement opportunity metrics quantify the percentage and number of underprovisioned resources at the account level and resource type level. You can use these metrics to evaluate and prioritize performance improvement opportunities that address resource bottleneck risks.

Metrics analyzed by AWS Compute Optimizer:  via - https://docs.aws.amazon.com/compute-optimizer/latest/ug/metrics.html

Incorrect options:

Use AWS Budgets to analyze the specifications and utilization metrics of your AWS resources - You can use AWS Budgets to track and take action on your AWS costs and usage. You can use AWS Budgets to monitor your aggregate utilization and coverage metrics for your Reserved Instances (RIs) or Savings Plans. You can use AWS Budgets to enable simple-to-complex cost and usage tracking. However, AWS Compute Optimizer is optimal choice for teams focusing on optimizing compute resources like EC2 instances, Lambda functions, or EBS volumes for better performance and cost efficiency.

Use AWS Cost Explorer to analyze the specifications and utilization metrics of your AWS compute resources - AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You can quickly get started by creating custom reports that analyze cost and usage data. Analyze your data at a high level (for example, total costs and usage across all accounts), or dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. While AWS Cost Explorer is good for budget planning and cost optimization, Compute Optimizer helps in performance tuning the compute resources of the AWS account. Note that AWS Cost Explorer rightsizing recommendations integrates with AWS Compute Optimizer for pulling in recommendations.

Use AWS Trusted Advisor to analyze the specifications and utilization metrics of your AWS resources - AWS Trusted Advisor draws upon best practices learned from serving hundreds of thousands of AWS customers. Trusted Advisor inspects your AWS environment, and then makes recommendations when opportunities exist to save money, improve system availability and performance, or help close security gaps. While Trusted Advisor has broad tools that focus on overall account health and optimization across multiple areas, including cloud cost optimization, performance, resilience, security, operational excellence, and service limits; Compute optimizer offers more specific tools that focuses on compute resource optimization.

References:

https://aws.amazon.com/compute-optimizer/faqs/

https://docs.aws.amazon.com/compute-optimizer/latest/ug/what-is-compute-optimizer.html

https://aws.amazon.com/about-aws/whats-new/2020/04/aws-cost-explorer-rightsizing-recommendations-integrates-with-aws-compute-optimizer/

https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 33

A hospital wants to use computer vision to monitor the entry and exit of doctors and staff into secure areas such as operating rooms. Cameras installed at entry points upload images to an Amazon S3 bucket. The hospital has limited images of authorized personnel, making it impractical to train a custom facial recognition model from scratch. The solution must minimize operational overhead while accurately identifying authorized staff members. The hospital also wants to store and manage the data for audit purposes.

Which of the following options must be combined to develop a solution that meets these requirements? (Select two)

Set up an AWS Lambda function to trigger a face detection operation when new images are uploaded to the S3 bucket. Write the results to an Amazon DynamoDB table for auditing

Selección correcta
Use Amazon Rekognition to create a face collection for authorized personnel

Use Amazon Rekognition to create custom labels for authorized personnel

Use Amazon Lookout for Vision to create custom labels for authorized personnel

Selección correcta
Set up an AWS Lambda function to trigger a face match operation when new images are uploaded to the S3 bucket. Write the results to an Amazon DynamoDB table for auditing

Explicación general
Correct options:

Use Amazon Rekognition to create a face collection for authorized personnel

Amazon Rekognition's face search feature works by matching a provided face image against a collection of stored face data (face collections) to identify or verify individuals. The process begins with analyzing the input image to detect and extract facial features, generating a unique face vector or feature representation. This vector is then compared against the indexed face vectors in the collection using machine learning algorithms optimized for speed and accuracy. Rekognition returns the closest matches along with confidence scores, enabling applications like authentication, access control, or person identification. The service ensures scalability and handles variations in facial expressions, lighting, and angles, offering high accuracy across diverse scenarios.

Amazon Rekognition’s face collection feature allows for efficient matching without training a model.

 via - https://docs.aws.amazon.com/rekognition/latest/dg/collections.html

Set up an AWS Lambda function to trigger a face match operation when new images are uploaded to the S3 bucket. Write the results to an Amazon DynamoDB table for auditing

By integrating Amazon Rekognition’s face collection feature with AWS Lambda, the process becomes automated: each time a new image is uploaded to the Amazon S3 bucket, a Lambda function is triggered to perform a face match operation using the Rekognition face collection. This approach ensures low latency and eliminates the need for manual intervention or custom model training. The match results are then stored in an Amazon DynamoDB table, providing a scalable and cost-effective solution for maintaining a real-time audit trail of access attempts.

Incorrect options:

Set up an AWS Lambda function to trigger a face detection operation when new images are uploaded to the S3 bucket. Write the results to an Amazon DynamoDB table for auditing - Face comparison is different from face detection. Face detection (which uses DetectFaces) only identifies the presence and location of faces in an image or video. In contrast, face comparison involves comparing a detected face in a source image to faces in a target image to find matches. So, this option is incorrect.

Use Amazon Lookout for Vision to create custom labels for authorized personnel - Amazon Lookout for Vision is an ML service that uses computer vision to spot defects in manufactured products at scale. So, this option is incorrect.

Use Amazon Rekognition to create custom labels for authorized personnel - With Amazon Rekognition Custom Labels, you can identify the objects and scenes in images that are specific to your business needs. For example, you can find your logo in social media posts, identify your products on store shelves, classify machine parts in an assembly line, distinguish healthy and infected plants, or detect animated characters in videos. So, this option is incorrect.

References:

https://docs.aws.amazon.com/rekognition/latest/dg/collections.html

https://docs.aws.amazon.com/rekognition/latest/dg/face-feature-differences.html

https://docs.aws.amazon.com/rekognition/latest/dg/faces-comparefaces.html

https://aws.amazon.com/about-aws/whats-new/2022/11/amazon-rekognition-new-pre-trained-labels-color-detection/

https://aws.amazon.com/lookout-for-vision/

https://aws.amazon.com/rekognition/custom-labels-features/

Temática
ML Model Development
Pregunta 34

A data scientist at an e-commerce company needs to analyze customer purchasing behavior by running ML models on large historical datasets. The analysis must be conducted in an asynchronous manner to handle the large data volume efficiently. Additionally, the company wants to monitor the data quality of models over time. The company must receive a notification if any significant change in data quality is detected.

Which of the following solutions should the data scientist implement to meet these requirements?

Use Docker to containerize your ML model and deploy it on AWS Lambda. Configure AWS CloudTrail to monitor the data quality and send alerts

Use Amazon SageMaker Batch Transform to deploy the model. Set up SageMaker Data Wrangler to track data quality and configure it to send alerts for any significant changes

Respuesta correcta
Use Amazon SageMaker Batch Transform to deploy the model. Set up SageMaker Model Monitor to track data quality and configure it to send alerts for any significant changes

Use Amazon SageMaker Real-time inference to deploy the model. Set up SageMaker Model Monitor to track data quality and configure it to send alerts for any significant changes

Explicación general
Correct option:

Use Amazon SageMaker Batch Transform to deploy the model. Set up SageMaker Model Monitor to track data quality and configure it to send alerts for any significant changes

Use batch transform for inference with Amazon SageMaker when you need to do the following:

Preprocess datasets to remove noise or bias that interferes with training or inference from your dataset.

Get inferences from large datasets.

Run inference when you don't need a persistent endpoint.

Associate input records with inferences to help with the interpretation of results.

If the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, the model begins to lose accuracy in its predictions. Amazon SageMaker Model Monitor uses rules to detect data drift and alerts you when it happens.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html

Incorrect options:

Use Amazon SageMaker Real-time inference to deploy the model. Set up SageMaker Model Monitor to track data quality and configure it to send alerts for any significant changes - Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. For large datasets, AWS recommends using SageMaker Batch Transform.

Use Docker to containerize your ML model and deploy it on AWS Lambda. Configure AWS CloudTrail to monitor the data quality and send alerts - AWS CloudTrail is an AWS service that helps you enable operational and risk auditing, governance, and compliance of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. CloudTrail cannot send alerts for deviations in data quality of models, so this option is ruled out.

Use Amazon SageMaker Batch Transform to deploy the model. Set up SageMaker Data Wrangler to track data quality and configure it to send alerts for any significant changes - Amazon SageMaker Data Wrangler reduces the time it takes to aggregate and prepare data for ML. With SageMaker Data Wrangler you can simplify data preparation and feature engineering through a visual and natural language interface. SageMaker Data Wrangler cannot be used to monitor deviations in data quality of models, so this option is incorrect.

Amazon SageMaker Data Wrangler Overview:  via - https://aws.amazon.com/sagemaker-ai/data-wrangler/

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html

https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html

https://aws.amazon.com/sagemaker-ai/data-wrangler/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 35

A healthcare analytics company is using Amazon SageMaker to train a deep learning model for medical image analysis. The training process requires distributed training across multiple GPU instances due to the large size of the dataset. During training, the ML engineer observes that the model training is slower than expected because of network communication delays between the instances. The engineer must optimize the network configuration to minimize communication overhead and improve training performance.

Which of the following options should be combined for a solution that meets these requirements? (Select two)

Store the training data in a different AWS Region from where the instances are deployed

Selección correcta
Store the training data in the same AWS Region and Availability Zone where the instances are deployed

Selección correcta
Launch the training instances in the same VPC subnet

Store the training data in the same AWS Region but different Availability Zones from where the instances are deployed

Launch the training instances in different subnets within the same VPC to balance network traffic and reduce communication bottlenecks

Explicación general
Correct options:

Launch the training instances in the same VPC subnet

A virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual networks in the AWS Cloud. A subnet is a range of IP addresses in your VPC.

This option ensures that all instances communicate over the local network within the same VPC subnet, reducing routing overhead and improving latency.

How Amazon VPC works:  via - https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html

Store the training data in the same AWS Region and Availability Zone where the instances are deployed

Storing training data in a single Availability Zone (AZ) is advantageous in this scenario because it minimizes data transfer latency and communication overhead between the training instances. When both the data and the instances are located within the same AZ, the network communication occurs over the high-speed, low-latency infrastructure specific to that AZ, eliminating the additional latency and potential data transfer costs associated with cross-AZ communication. This proximity ensures that the training instances can access the data more quickly and efficiently, which is critical for distributed training where timely synchronization between instances significantly impacts overall performance.

 via - https://aws.amazon.com/about-aws/global-infrastructure/regions_az/

Incorrect options:

Store the training data in a different AWS Region from where the instances are deployed - Cross-region communication introduces significant latency and is highly unsuitable for distributed training, which requires near-instantaneous communication between instances.

Store the training data in the same AWS Region but different Availability Zones from where the instances are deployed - Cross-AZ network latency and communication overhead is unsuitable for distributed training, which requires near-instantaneous communication between instances.

Launch the training instances in different subnets within the same VPC to balance network traffic and reduce communication bottlenecks - Inter-subnet communication within the same VPC introduces routing overhead, which increases latency compared to keeping instances in the same subnet.

References:

https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html

https://aws.amazon.com/about-aws/global-infrastructure/regions_az/

Temática
Deployment and Orchestration of ML Workflows
Pregunta 36

A media company collects real-time streaming logs of video content views and stores them in an Amazon S3 bucket. The raw logs contain millions of records daily, including timestamps, user IDs, and video metadata. The data analytics team uses Amazon Athena to generate weekly reports and analyze trends in video views over the past 7 days. The company needs to store the data for 90 days before archiving it to lower-cost storage. The solution must optimize Athena query performance while maintaining cost efficiency.

What do you recommend

Use an S3 lifecycle policy to delete logs older than 90 days and configure Athena to query the raw logs directly from the S3 bucket without using partitions or prefixes

Store all logs in uncompressed JSON format and rely on Athena filters to query the data for the 7-day trend analysis

Compress all daily logs into a single large file and store them in the S3 bucket. Use Athena to scan the compressed files for the required 7-day trend analysis

Respuesta correcta
Partition the data in the S3 bucket by year, month, and day (e.g., year=YYYY/month=MM/day=DD). Use prefixes for organizing logs in daily folders and configure an S3 lifecycle policy to transition data to Amazon S3 Glacier after 90 days

Explicación general
Correct option:

Partition the data in the S3 bucket by year, month, and day (e.g., year=YYYY/month=MM/day=DD). Use prefixes for organizing logs in daily folders and configure an S3 lifecycle policy to transition data to Amazon S3 Glacier after 90 days

Partitioning the data by date enables Athena partition pruning, allowing it to scan only the relevant partitions for the 7-day trend analysis. Partition pruning gathers metadata and "prunes" it to only the partitions that apply to your query. This often speeds up queries. Athena uses partition pruning for all tables with partition columns, including those tables configured for partition projection. This improves query performance and reduces costs by minimizing the amount of data scanned. The S3 lifecycle policy ensures that older data is moved to Amazon S3 Glacier after 90 days, reducing long-term storage costs.

 via - https://docs.aws.amazon.com/athena/latest/ug/partitions.html

Incorrect options:

Compress all daily logs into a single large file and store them in the S3 bucket. Use Athena to scan the compressed files for the required 7-day trend analysis - Compressing data reduces storage costs and the volume of data scanned during queries. However, storing all logs in a single file per day prevents Athena from leveraging partition pruning. As a result, Athena must scan the entire file for each query, increasing query execution time and reducing performance.

Store all logs in uncompressed JSON format and rely on Athena filters to query the data for the 7-day trend analysis - Storing data in an uncompressed format increases storage costs and query latency. Without compression or partitioning, Athena must scan the entire dataset and rely solely on query filters, which leads to inefficient queries and higher costs.

Use an S3 lifecycle policy to delete logs older than 90 days and configure Athena to query the raw logs directly from the S3 bucket without using partitions or prefixes - To meet the requirements, you must archive and not delete the logs data after 90 days. So, this option is incorrect.

References:

https://docs.aws.amazon.com/athena/latest/ug/partitions.html

https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html

https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-prefixes.html

https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 37

A company wants to build an ML solution to predict customer churn. The company's dataset includes thousands of rows with features related to customer demographics, transaction history, and support interactions. The dataset is stored in CSV format in Amazon S3. The ML engineer must select an appropriate built-in Amazon SageMaker algorithm for training the model. The algorithm should handle tabular data and be capable of performing supervised learning for a binary classification problem.

Which Amazon SageMaker built-in algorithm should the ML engineer use?

K-Means

Respuesta correcta
XGBoost (eXtreme Gradient Boosting)

Linear Learner

BlazingText

Explicación general
Correct option:

XGBoost (eXtreme Gradient Boosting)

The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. The XGBoost algorithm performs well in machine learning competitions for the following reasons:

Its robust handling of a variety of data types, relationships, distributions.

The variety of hyperparameters that you can fine-tune.

You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.

XGBoost is a robust and efficient algorithm designed for supervised learning tasks, including binary classification, and works well with tabular data.

Case study on customer churn using XGBoost:  via - https://aws.amazon.com/blogs/machine-learning/build-tune-and-deploy-an-end-to-end-churn-prediction-model-using-amazon-sagemaker-pipelines/

Incorrect options:

K-Means - K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity. K-Means is an unsupervised learning algorithm used for clustering and it is not suitable for binary classification tasks.

BlazingText - The Amazon SageMaker AI BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc. Text classification is an important task for applications that perform web searches, information retrieval, ranking, and document classification. BlazingText is designed for natural language processing tasks and it is not optimized for binary classification on tabular data.

Linear Learner - Linear models are supervised learning algorithms used for solving either classification or regression problems. For input, you give the model labeled examples (x, y). x is a high-dimensional vector and y is a numeric label. While Linear Learner can be used for binary classification, XGBoost is generally more effective for tabular data and predictive tasks.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html

https://aws.amazon.com/blogs/machine-learning/build-tune-and-deploy-an-end-to-end-churn-prediction-model-using-amazon-sagemaker-pipelines/

Temática
ML Model Development
Pregunta 38

You are preparing a dataset for training a machine learning model using SageMaker Data Wrangler. The dataset has several missing values spread across different columns, and these columns contain numeric data. Before training the model, it is essential to handle these missing values to ensure the model performs optimally. The goal is to replace the missing values in each numeric column with the mean of that column.

Which transformation in SageMaker Data Wrangler should you apply to replace the missing values in numeric columns with the mean of those columns?

Drop

Respuesta correcta
Impute

Encode

Scale

Explicación general
Correct option:

Impute

Amazon SageMaker Data Wrangler provides numerous ML data transforms to streamline cleaning, transforming, and featurizing your data. When you add a transform, it adds a step to the data flow. Each transform you add modifies your dataset and produces a new dataframe. All subsequent transforms apply to the resulting dataframe.

Use the Impute missing transform to create a new column that contains imputed values where missing values were found in input categorical and numerical data. The configuration depends on your data type.

For numeric data, choose an imputing strategy, the strategy used to determine the new value to impute. You can choose to impute the mean (as needed in the use case)or the median over the values that are present in your dataset. Data Wrangler uses the value that it computes to impute the missing values.

For categorical data, Data Wrangler imputes missing values using the most frequent value in the column. To impute a custom string, use the Fill missing transform instead.

Incorrect options:

Drop - Use the Drop missing option to drop rows that contain missing values from the Input column.

Scale - This action normalizes or standardizes the data but does not handle missing values.

Encode - This action transforms categorical data into numerical data. Encoding categorical data is the process of creating a numerical representation for categories. For example, if your categories are Dog and Cat, you may encode this information into two vectors, [1,0] to represent Dog, and [0,1] to represent Cat. Encoding does not address missing values.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 39

You are a machine learning engineer at a financial services company responsible for maintaining a fraud detection model deployed on Amazon SageMaker. The model processes a high volume of real-time transactions and must respond with low latency to ensure a seamless customer experience. Recently, the model has experienced increased latency during peak traffic times, and there have been instances where requests were dropped due to insufficient capacity.

Which approach is the MOST EFFECTIVE for monitoring and resolving these latency and scaling issues in your ML solution?

Respuesta correcta
Configure Amazon CloudWatch to monitor key metrics such as invocation latency, model error rates, and CPU utilization. Set up CloudWatch Alarms to automatically trigger an increase in instance count when latency exceeds a predefined threshold, and enable auto-scaling on the SageMaker endpoint to handle traffic spikes

Manually monitor the model’s performance using the SageMaker console, and manually increase the instance size whenever latency is detected, to ensure the model remains responsive during peak periods

Implement a serverless architecture using AWS Lambda for model inference to eliminate latency and scaling issues. Use Amazon CloudFront to cache inference results for faster response times

Use Amazon SageMaker Model Monitor to track the performance of the model and identify data drift. Adjust the model’s hyperparameters based on the results to reduce latency and improve scalability during peak times

Explicación general
Correct option:

Configure Amazon CloudWatch to monitor key metrics such as invocation latency, model error rates, and CPU utilization. Set up CloudWatch Alarms to automatically trigger an increase in instance count when latency exceeds a predefined threshold, and enable auto-scaling on the SageMaker endpoint to handle traffic spikes

This approach uses Amazon CloudWatch to monitor critical performance metrics such as latency, error rates, and CPU utilization, which are directly related to the model’s performance and ability to scale. By setting up CloudWatch Alarms and enabling auto-scaling on the SageMaker endpoint, you can automatically adjust resources to handle traffic spikes, reducing latency and preventing dropped requests. This solution provides a proactive and automated way to address both latency and scaling issues.

Incorrect options:

Use Amazon SageMaker Model Monitor to track the performance of the model and identify data drift. Adjust the model’s hyperparameters based on the results to reduce latency and improve scalability during peak times - SageMaker Model Monitor is useful for tracking model performance and detecting data drift, but it is not directly focused on resolving latency or scaling issues. Adjusting hyperparameters may improve model performance but does not address the need for scalable infrastructure during traffic spikes.

Implement a serverless architecture using AWS Lambda for model inference to eliminate latency and scaling issues. Use Amazon CloudFront to cache inference results for faster response times - Moving to a serverless architecture with AWS Lambda can simplify scaling but may not be suitable for high-throughput, low-latency ML models that require persistent instances. CloudFront is effective for caching static content but is not typically used for caching dynamic ML inference results.

Manually monitor the model’s performance using the SageMaker console, and manually increase the instance size whenever latency is detected, to ensure the model remains responsive during peak periods - Manually monitoring and scaling resources is inefficient and may not respond quickly enough to sudden traffic spikes. Automated monitoring and scaling are essential for maintaining performance in a real-time, high-volume environment.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 40

A financial services company uses an AWS Lambda function to monitor transaction fraud detection metrics from an ML model deployed in Amazon SageMaker. The company wants to be notified via email whenever the fraud detection model’s confidence score drops below a specific threshold, such as 80%. The ML engineer needs to implement an automated solution that triggers email notifications whenever the confidence score breaches the threshold.

Which of the following options should be combined to develop a solution that addresses these requirements? (Select two)

Log the metrics from the Lambda function to AWS CloudFormation

Log the metrics from the Lambda function to AWS CloudTrail

Set up a CloudFormation change set to send the email message

Selección correcta
Log the metrics from the Lambda function to AWS CloudWatch

Selección correcta
Set up a CloudWatch alarm to send the email message

Explicación general
Correct options:

Log the metrics from the Lambda function to AWS CloudWatch

AWS Lambda integrates seamlessly with Amazon CloudWatch, allowing you to log custom metrics directly from the Lambda function. These metrics can be used to monitor the performance and health of the ML model.

Lambda integrates natively with CloudWatch, allowing you to log custom metrics using the CloudWatch API from the AWS SDK. For example, you can use the putMetricData method to log performance metrics such as execution duration, memory usage, or invocation counts. Additionally, you can use the Lambda execution environment to automatically capture logs like errors or custom messages with console.log, which appear in the CloudWatch Logs console. These metrics and logs can then be visualized in dashboards, used to set up alarms for anomalies, or analyzed further to optimize your function's performance and cost efficiency. To implement this, ensure the Lambda function has the necessary IAM permissions to write to CloudWatch.

Set up a CloudWatch alarm to send the email message

CloudWatch Alarms offer a native way to monitor metrics and automate notifications via Amazon SNS. Setting up a CloudWatch alarm to send an email message involves monitoring a specific metric and configuring Amazon Simple Notification Service (SNS) for email notifications. First, identify the metric you want to monitor and create an alarm in the CloudWatch console. During the alarm configuration, define the threshold conditions (e.g., metric value breaching a certain value) and the evaluation period. Next, create an SNS topic and subscribe the desired email addresses to this topic. When setting up the alarm's actions, specify the SNS topic as the target for notifications when the alarm state changes (e.g., from OK to ALARM). Once configured, the alarm will automatically send email alerts to the subscribers whenever the specified condition is met (such as, for the given use case, the fraud detection model’s confidence score drops below a specific threshold of 80%), ensuring timely notifications about critical events or performance issues.

Incorrect options:

Log the metrics from the Lambda function to AWS CloudTrail - AWS CloudTrail is designed for auditing API calls and tracking changes in AWS accounts, not for logging or monitoring application metrics.

Log the metrics from the Lambda function to AWS CloudFormation - AWS CloudFormation is a service used for defining and provisioning infrastructure as code; it is not a logging or monitoring service.

Set up a CloudFormation change set to send the email message - Email notifications based on metrics are managed through CloudWatch and SNS, and not via a CloudFormation change set. CloudFormation change sets allow you to see how your changes might impact your running resources (managed via CloudFormation), especially for critical resources, before implementing them.

References:

https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html

https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html

https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 41

How would you differentiate between K-Means and K-Nearest Neighbors (KNN) algorithms in machine learning?

K-Means is a supervised learning algorithm used for classification, while KNN is an unsupervised learning algorithm used for clustering

K-Means requires labeled data to form clusters, whereas KNN does not use labeled data for making predictions

K-Means is primarily used for regression tasks, while KNN is used for reducing the dimensionality of data

Respuesta correcta
K-Means is an unsupervised learning algorithm used for clustering data points into groups, while KNN is a supervised learning algorithm used for classifying data points based on their proximity to labeled examples

Explicación general
Correct option:

K-Means is an unsupervised learning algorithm used for clustering data points into groups, while KNN is a supervised learning algorithm used for classifying data points based on their proximity to labeled examples

K-Means is an unsupervised learning algorithm used to partition a dataset into distinct clusters by minimizing the variance within each cluster. KNN, on the other hand, is a supervised learning algorithm that classifies new data points based on the majority class among its k-nearest neighbors in the training data.

Incorrect options:

K-Means is a supervised learning algorithm used for classification, while KNN is an unsupervised learning algorithm used for clustering - K-Means is an unsupervised learning algorithm, and KNN is a supervised learning algorithm.

K-Means requires labeled data to form clusters, whereas KNN does not use labeled data for making predictions - K-Means does not require labeled data; it is used for clustering. KNN, however, requires labeled data for classification.

K-Means is primarily used for regression tasks, while KNN is used for reducing the dimensionality of data - K-Means is not used for regression tasks, and KNN is not primarily used for dimensionality reduction. KNN is used for classification and regression tasks based on proximity to neighbors.

References:

https://aws.amazon.com/blogs/machine-learning/k-means-clustering-with-amazon-sagemaker/

https://docs.aws.amazon.com/sagemaker/latest/dg/k-nearest-neighbors.html

Temática
ML Model Development
Pregunta 42

A retail company trained a sales forecasting ML model by preprocessing the training data using AWS Glue DataBrew. During preprocessing, the ML engineer used z-score normalization to standardize the training data. Now, the company needs to preprocess the real-time sales data for inference by applying the same z-score normalization parameters as the training data to ensure consistency in predictions.

Which solution will meet this requirement?

Apply a SageMaker Processing job to perform z-score normalization on the real-time sales data and automatically derive new parameters

Respuesta correcta
Export the z-score normalization recipe from AWS Glue DataBrew and apply the same recipe to preprocess the real-time sales data for inference

Manually calculate the z-score normalization parameters again for the real-time sales data and use these parameters during inference preprocessing

Use AWS Lambda to create a custom function for z-score normalization that calculates new parameters for each batch of sales data

Explicación general
Correct option:

Export the z-score normalization recipe from AWS Glue DataBrew and apply the same recipe to preprocess the real-time sales data for inference

The DataBrew recipe RESCALE_OUTLIERS_WITH_Z_SCORE returns a new column with a rescaled outlier value in each row, based on the settings in the parameters. This action also applies Z-score normalization to linearly scale data values to have a mean (μ) of 0 and standard deviation (σ) of 1.

For the given use case, reusing the DataBrew recipe ensures that the same normalization parameters derived from the training data are applied to the real-time data, maintaining consistency and reducing errors during inference.

Handling Numerical values:  via - https://aws.amazon.com/blogs/big-data/7-most-common-data-preparation-transformations-in-aws-glue-databrew/

Incorrect options:

Manually calculate the z-score normalization parameters again for the real-time sales data and use these parameters during inference preprocessing - Manually recalculating parameters for real-time data introduces inconsistency and violates the requirement to use the same parameters derived from the training data.

Use AWS Lambda to create a custom function for z-score normalization that calculates new parameters for each batch of sales data - Calculating new normalization parameters for each batch of data will lead to inconsistencies with the training data and increase the risk of incorrect model predictions.

Apply a SageMaker Processing job to perform z-score normalization on the real-time sales data and automatically derive new parameters - Automatically deriving new parameters during preprocessing fails to maintain consistency with the training data normalization, leading to potentially skewed predictions.

References:

https://docs.aws.amazon.com/databrew/latest/dg/recipe-actions.RESCALE_OUTLIERS_WITH_Z_SCORE.html

https://aws.amazon.com/blogs/big-data/7-most-common-data-preparation-transformations-in-aws-glue-databrew/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 43

A healthcare company is developing an ML model using the Amazon SageMaker XGBoost algorithm to classify patients as either high-risk or low-risk for a specific disease. During evaluation, the model performs exceptionally well on the training dataset but fails to accurately classify new patient data. The ML engineer suspects that the model’s performance issues may be related to noise in the dataset and needs to optimize the model to improve its performance on unseen data.

As an AWS Certified Machine Learning Engineer Associate, what do you recommend?

Increase the learning_rate parameter to ensure the model converges faster and performs better on unseen transactions

Increase the size of the training dataset by duplicating examples of fraudulent transactions to improve the model's recall

Remove irrelevant features from the training dataset to reduce noise and improve the generalization of the model

Respuesta correcta
Reduce the max_depth parameter in the XGBoost algorithm to prevent the model from overfitting to the training dataset

Explicación general
Correct option:

Reduce the max_depth parameter in the XGBoost algorithm to prevent the model from overfitting to the training dataset

The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models.

The max_depth parameter for the SageMaker AI XGBoost algorithm refers to the maximum depth of the tree. Increasing this value makes the model more complex and likely to be overfit. A high max_depth can lead to overfitting, where the model learns to memorize patterns in the training data rather than generalizing to unseen data. Reducing this parameter helps the model focus on broader trends, improving its ability to detect fraud in new transactions.

Incorrect options:

Increase the learning_rate parameter to ensure the model converges faster and performs better on unseen transactions - While increasing the learning_rate speeds up convergence, it can lead to suboptimal performance by overshooting the optimal weights, reducing the model’s ability to generalize effectively.

Remove irrelevant features from the training dataset to reduce noise and improve the generalization of the model - While feature selection is important, the question focuses on tuning model parameters to improve performance, making this less relevant in this specific scenario.

Increase the size of the training dataset by duplicating examples of fraudulent transactions to improve the model's recall - Duplicating data introduces data leakage and does not represent real-world scenarios, leading to biased training and poor generalization.

Reference:

https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html

Temática
ML Model Development
Pregunta 44

You are a data scientist working on a regression model to predict housing prices in a large metropolitan area. The dataset contains many features, including location, square footage, number of bedrooms, and amenities. After initial testing, you notice that some features have very high variance, leading to overfitting. To address this, you are considering applying regularization to your model. You need to choose between L1 (Lasso) and L2 (Ridge) regularization.

Given the goal of reducing overfitting while also simplifying the model by eliminating less important features, which regularization method should you choose and why?

L2 regularization, because it eliminates less important features by setting their coefficients to zero, simplifying the model

L1 regularization, because it penalizes large coefficients more heavily, making the model less sensitive to high-variance features

L2 regularization, because it evenly reduces all feature coefficients, leading to a more stable model

Respuesta correcta
L1 regularization, because it can shrink some feature coefficients to zero, effectively performing feature selection

Explicación general
Correct option:

L1 regularization, because it can shrink some feature coefficients to zero, effectively performing feature selection

Regularization helps prevent linear models from overfitting training data examples by penalizing extreme weight values. L1 regularization reduces the number of features used in the model by pushing the weight of features that would otherwise have very small weights to zero. L1 regularization produces sparse models and reduces the amount of noise in the model. L2 regularization results in smaller overall weight values, which stabilizes the weights when there is high correlation between the features.

L1 regularization (Lasso) applies a penalty proportional to the absolute value of the coefficients. This characteristic allows it to shrink some coefficients to exactly zero, effectively removing less important features and performing feature selection. This makes L1 regularization particularly useful when you suspect that only a subset of features are truly important.

Incorrect options:

L2 regularization, because it evenly reduces all feature coefficients, leading to a more stable model - L2 regularization (Ridge) penalizes the sum of the squared coefficients, which tends to reduce the coefficients evenly but does not shrink any to zero. While this can lead to a more stable model by controlling high variance, it does not simplify the model by removing features.

L1 regularization, because it penalizes large coefficients more heavily, making the model less sensitive to high-variance features - While L1 regularization does penalize large coefficients, the key advantage here is its ability to perform feature selection by setting some coefficients to zero, not just by reducing high variance.

L2 regularization, because it eliminates less important features by setting their coefficients to zero, simplifying the model - This statement incorrectly attributes L2 regularization with the ability to eliminate features by setting coefficients to zero, which is actually a characteristic of L1 regularization.

References:

https://docs.aws.amazon.com/machine-learning/latest/dg/training-parameters.html

https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html

Temática
ML Model Development
Pregunta 45

A biotechnology company is building a machine learning solution to predict the effectiveness of new drug compounds. The company stores its dataset of molecular simulations, totaling over 6 TB, in Amazon S3. The dataset contains CSV, Parquet, and JSON files. The preprocessing involves complex calculations and data transformations, including specialized scientific computations that can take hours to execute. The company needs to automate the entire workflow, from preprocessing to model training, while ensuring efficiency and scalability.

Which solution will meet these requirements?

Use AWS Glue workflows to perform the data transformations and preprocessing before exporting the data to SageMaker for model training

Use Amazon EMR to preprocess the large dataset, run the scientific computations, and store the transformed data in Amazon S3 before initiating SageMaker training

Respuesta correcta
Use Amazon SageMaker Pipelines to define and automate the entire ML workflow, including data preprocessing, data transformations, and model training

Use AWS Step Functions to orchestrate the data preprocessing steps with Amazon Glue and integrate with SageMaker for model training

Explicación general
Correct option:

Use Amazon SageMaker Pipelines to define and automate the entire ML workflow, including data preprocessing, data transformations, and model training

Amazon SageMaker Pipelines is specifically designed for ML workflow automation. It enables the creation of end-to-end pipelines that include data preprocessing, data transformations, and model training. The modular structure of SageMaker Pipelines allows for the seamless integration of long-running data transformations, ensuring scalability and efficiency for large datasets. By using SageMaker Pipelines, the company can easily automate the entire workflow while maintaining a single, cohesive system for execution and monitoring.

What is Amazon SageMaker Pipelines:  via - https://aws.amazon.com/sagemaker-ai/pipelines/

Incorrect options:

Use AWS Step Functions to orchestrate the data preprocessing steps with Amazon Glue and integrate with SageMaker for model training - While Step Functions can orchestrate workflows across multiple services, it is not specifically designed for ML workflows. Integrating Glue for preprocessing and SageMaker for training requires additional custom configuration and lacks the seamless automation provided by SageMaker Pipelines.

Use Amazon EMR to preprocess the large dataset, run the scientific computations, and store the transformed data in Amazon S3 before initiating SageMaker training - While EMR is suitable for large-scale data processing, it does not provide orchestration for end-to-end ML workflows. The lack of built-in integration with SageMaker increases complexity.

Use AWS Glue workflows to perform the data transformations and preprocessing before exporting the data to SageMaker for model training - AWS Glue workflows are focused on ETL tasks and do not provide the orchestration capabilities required for an ML pipeline. They lack the modularity and monitoring features of SageMaker Pipelines.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-overview.html

https://aws.amazon.com/sagemaker-ai/pipelines/

https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 46

A company is building a recommendation system to provide personalized product suggestions to its customers. The company has collected historical user interaction data - including product views, purchases, and ratings. An ML engineer needs to select the most suitable built-in Amazon SageMaker algorithm to train the recommendation model efficiently. The engineer wants a solution that minimizes development effort while providing high-quality recommendations.

Which built-in SageMaker algorithm should the ML engineer use to meet these requirements?

K-Means Clustering

Respuesta correcta
Factorization Machines

Linear Learner

XGBoost (eXtreme Gradient Boosting)

Explicación general
Correct option:

Factorization Machines

The Factorization Machines algorithm is a general-purpose supervised learning algorithm that you can use for both classification and regression tasks. It is an extension of a linear model that is designed to capture interactions between features within high dimensional sparse datasets economically.

For example, in a click prediction system, the Factorization Machines model can capture click rate patterns observed when ads from a certain ad-category are placed on pages from a certain page-category. Factorization machines are a good choice for tasks dealing with high dimensional sparse datasets, such as click prediction and item recommendation.

How Factorization Machines Work:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines-howitworks.html

Incorrect options:

XGBoost (eXtreme Gradient Boosting) - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models.

While XGBoost is a powerful algorithm for general-purpose machine learning tasks, it is not specifically designed for recommendation systems.

Linear Learner - The Amazon SageMaker AI linear learner algorithm provides a solution for both classification and regression problems. With the SageMaker AI algorithm, you can simultaneously explore different training objectives and choose the best solution from a validation set. You can also explore a large number of models and choose the best. The best model optimizes either of the following: Continuous objectives, such as mean square error, cross entropy loss, absolute error and Discrete objectives suited for classification, such as F1 measure, precision, recall, or accuracy.

Linear Learner is suitable for binary classification or regression tasks and does not efficiently handle the complexity of recommendation systems.

K-Means Clustering - The K-means algorithm attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups (see the following figure). You define the attributes that you want the algorithm to use to determine similarity. Another way you can define k-means is that it is a clustering problem that finds k cluster centroids for a given set of records, such that all points within a cluster are closer in distance to their centroid than they are to any other centroid. K-Means is not suitable for building recommendation systems based on historical user interaction data.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html

https://aws.amazon.com/blogs/machine-learning/k-means-clustering-with-amazon-sagemaker/

Temática
ML Model Development
Pregunta 47

A research institute stores encrypted genomic datasets in an Amazon S3 bucket. The S3 bucket uses server-side encryption with AWS KMS keys (SSE-KMS) to secure the sensitive data. A data scientist needs to use an Amazon SageMaker notebook instance to analyze the datasets stored in the bucket. The solution must ensure that the notebook instance can access the data in S3 bucket and decrypt the data using the KMS key, while adhering to AWS best practices for security and permissions.

Which options can meet these requirements independently? (Select two)

Selección correcta
Attach an IAM role to the SageMaker notebook instance with s3:GetObject permissions for the S3 bucket and kms:Decrypt permissions for the KMS key

Add the SageMaker notebook instance’s IAM role to the S3 bucket’s access control list (ACL) and include the kms:Decrypt action in the IAM role’s policy

Use the SageMaker notebook instance’s IAM role to configure an inline policy that grants kms:Decrypt permissions for the KMS key and s3:GetBucketAcl permissions for the S3 bucket

Update the security group of the SageMaker notebook to allow inbound access from the S3 bucket and outbound access to the KMS key

Selección correcta
Grant the SageMaker notebook instance’s IAM role s3:GetObject permissions for the S3 bucket. Include the role’s ARN in the KMS key policy to allow kms:Decrypt permissions

Explicación general
Correct options:

Attach an IAM role to the SageMaker notebook instance with s3:GetObject permissions for the S3 bucket and kms:Decrypt permissions for the KMS key

This approach uses an IAM role to grant the necessary permissions directly to the SageMaker notebook instance. By including s3:GetObject for the S3 bucket, the notebook can read the encrypted objects, while kms:Decrypt allows the notebook to decrypt the KMS-encrypted data. This solution follows AWS best practices by using an IAM role for fine-grained access control without requiring additional configuration in the bucket or KMS key policies.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html

Grant the SageMaker notebook instance’s IAM role s3:GetObject permissions for the S3 bucket. Include the role’s ARN in the KMS key policy to allow kms:Decrypt permissions

This approach configures a bucket policy to explicitly allow the notebook instance to access the S3 bucket and a KMS key policy to grant decryption permissions. While functional, this method introduces additional complexity by requiring updates to multiple policies, making it less streamlined than associating permissions directly to the IAM role attached to the notebook instance.

Incorrect options:

Update the security group of the SageMaker notebook to allow inbound access from the S3 bucket and outbound access to the KMS key - Security groups are used to control network traffic, such as inbound and outbound requests, but they do not manage permissions for S3 buckets or KMS keys. This solution would not allow the SageMaker notebook instance to access or decrypt the encrypted data, as it does not provide the necessary IAM or resource-level permissions.

Add the SageMaker notebook instance’s IAM role to the S3 bucket’s access control list (ACL) and include the kms:Decrypt action in the IAM role’s policy - Using S3 bucket ACLs is an outdated and less secure method of managing access. Modern AWS environments recommend using bucket policies or IAM roles for permissions. Additionally, ACLs do not integrate well with the fine-grained controls required for KMS keys, making this solution less secure and harder to manage.

Use the SageMaker notebook instance’s IAM role to configure an inline policy that grants kms:Decrypt permissions for the KMS key and s3:GetBucketAcl permissions for the S3 bucket - This solution fails to grant the necessary s3:GetObject permissions required to access the dataset stored in the S3 bucket. While s3:GetBucketAcl allows viewing the bucket's ACL, it does not grant access to the objects themselves. Additionally, granting only kms:Decrypt without proper S3 permissions makes this configuration incomplete and ineffective for accessing the encrypted dataset.

References:

https://docs.aws.amazon.com/kms/latest/developerguide/cmks-in-iam-policies.html

https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-overview.html

https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 48

A technology company manages an application that performs data enrichment by integrating with various external APIs. To enhance security, the company must implement a solution to automatically rotate the API tokens used by the application every 90 days.

Which approach should the company use to meet this requirement?

Store the tokens in Parameter Store of AWS Systems Manager. Create an AWS Lambda function to perform the token rotation

Store the API tokens in AWS Secrets Manager. Use AWS Key Management Service (KMS) to encrypt the keys and configure automatic rotation of the keys

Respuesta correcta
Store the API tokens in AWS Secrets Manager. Configure an AWS Lambda function to perform the token rotation

Store the API tokens in AWS Secrets Manager. Configure Managed Rotation to perform the token rotation automatically

Explicación general
Correct option:

Store the API tokens in AWS Secrets Manager. Configure an AWS Lambda function to perform the token rotation

Rotation is the process of periodically updating a secret. When you rotate a secret, you update the credentials in both the secret and the database or service that the secret is for. For many types of secrets, Secrets Manager uses an AWS Lambda function to update the secret and the database or service. To rotate a secret, Secrets Manager calls a Lambda function according to the rotation schedule you set up. During rotation, Secrets Manager logs events that indicate the state of rotation.

While Parameter Store can be used to store tokens/secrets, Parameter Store does not offer secrets rotation functionality. Also, secrets can be versioned in AWS Secrets Manager. Secrets Manager allows multiple versions of a secret to exist at the same time.

Rotation by Lambda function for non-database secrets:  via - https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_turn-on-for-other.html

Incorrect options:

Store the API tokens in AWS Secrets Manager. Configure Managed Rotation to perform the token rotation automatically - Some services offer managed rotation, where the service configures and manages rotation for you. With managed rotation, you don't use an AWS Lambda function to update the secret and the credentials in the database. The services that offer managed rotation are as follows: Amazon Aurora, Amazon ECS, Amazon RDS and Amazon Redshift.

Store the API tokens in AWS Secrets Manager. Use AWS Key Management Service (KMS) to encrypt the keys and configure automatic rotation of the keys - To encrypt the secret value in a secret, AWS Secrets Manager uses AWS KMS. However, AWS KMS does not help in key rotation.

Store the tokens in Parameter Store of AWS Systems Manager. Create an AWS Lambda function to perform the token rotation - Parameter Store is a capability of AWS Systems Manager that provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. However, Parameter Store doesn't provide automatic rotation services for stored secrets. Instead, Parameter Store enables you to store your secret in Secrets Manager, and then reference the secret as a Parameter Store parameter.

References:

https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_lambda.html

https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_turn-on-for-other.html

https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotate-secrets_managed.html

https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 49

A company has recently migrated to AWS Cloud and it wants to optimize the hardware used for its AI workflows.

Which of the following would you suggest?

Respuesta correcta
Leverage AWS Trainium for high-performance, cost-effective Deep Learning training. Leverage AWS Inferentia for the deep learning (DL) and generative AI inference applications

Leverage either AWS Trainium or AWS Inferentia for the deep learning (DL) and generative AI inference applications

Leverage AWS Inferentia for high-performance, cost-effective Deep Learning training. Leverage AWS Trainium for the deep learning (DL) and generative AI inference applications

Leverage either AWS Trainium or AWS Inferentia for high-performance, cost-effective Deep Learning training

Explicación general
Correct option:

Leverage AWS Trainium for high-performance, cost-effective Deep Learning training. Leverage AWS Inferentia for the deep learning (DL) and generative AI inference applications

AWS Inferentia accelerators are designed by AWS to deliver high performance at the lowest cost in Amazon EC2 for your deep learning (DL) and generative AI inference applications. The first-generation AWS Inferentia accelerator powers Amazon Elastic Compute Cloud (Amazon EC2) Inf1 instances, which deliver up to 2.3x higher throughput and up to 70% lower cost per inference than comparable Amazon EC2 instances.

AWS Trainium is the machine learning (ML) chip that AWS purpose-built for deep learning (DL) training of 100B+ parameter models. Each Amazon Elastic Compute Cloud (Amazon EC2) Trn1 instance deploys up to 16 Trainium accelerators to deliver a high-performance, low-cost solution for DL training in the cloud.

Incorrect options:

Leverage either AWS Trainium or AWS Inferentia for the deep learning (DL) and generative AI inference applications

Leverage either AWS Trainium or AWS Inferentia for high-performance, cost-effective Deep Learning training

Leverage AWS Inferentia for high-performance, cost-effective Deep Learning training. Leverage AWS Trainium for the deep learning (DL) and generative AI inference applications

These three options contradict the explanation provided above, so these options are incorrect.

References:

https://aws.amazon.com/machine-learning/inferentia/

https://aws.amazon.com/machine-learning/trainium/

Temática
ML Model Development
Pregunta 50

A healthcare organization is developing an ML model to predict whether patients are at risk of developing a rare but critical disease. Missing a patient with a disease (a false negative) could delay treatment, potentially leading to severe consequences. However, incorrectly predicting a disease (a false positive) may result in unnecessary follow-up tests, which are less costly compared to missing an actual case.

Which model characteristic should the healthcare organization prioritize to best meet its requirements?

Prioritize a model with low recall

Prioritize a model with high precision

Prioritize a model with high accuracy

Respuesta correcta
Prioritize a model with high recall

Explicación general
Correct option:

Prioritize a model with high recall

Recall is defined as follows: Recall = true positives / ( true positives + false negatives). Recall measures the proportion of actual positive cases (high-risk patients) that the model correctly identifies. In a critical healthcare scenario, minimizing false negatives is essential to avoid missing high-risk patients who require timely treatment. By prioritizing high recall, the organization ensures that most potential cases are identified, even if it means accepting more false positives (low precision), which have a lower cost compared to missing true positives.

Key metrics:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

Incorrect options:

Prioritize a model with low recall - Low recall increases false negatives, which is unacceptable in this scenario where missing high-risk patients can have severe consequences.

Prioritize a model with high precision - High precision reduces false positives but can lead to higher false negatives, missing many high-risk patients, which is not acceptable in this healthcare context.

Prioritize a model with high accuracy - Accuracy considers both true positives and true negatives equally. For imbalanced datasets, where fraud cases are rare, a high accuracy score may be misleading and not reflect the model's performance in identifying fraudulent cases.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-metrics-validation.html

https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/im-metrics-use.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 51

A customer service organization has collected historical data with information about clients that required extended assistance from the support team. The organization now seeks to create an ML model to forecast whether future clients will need long-term support.

What approach should the organization take to build and implement this predictive model?

Linear regression

Principal Component Analysis (PCA) Algorithm

Respuesta correcta
Logistic regression

Anomaly Detection

Explicación general
Correct option:

Logistic regression

Logistic regression is a data analysis technique that uses mathematics to find the relationships between two data factors. It then uses this relationship to predict the value of one of those factors based on the other. The prediction usually has a finite number of outcomes, like yes or no.

For example, let’s say you want to guess if your website visitor will click the checkout button in their shopping cart or not. Logistic regression analysis looks at past visitor behavior, such as time spent on the website and the number of items in the cart. It determines that, in the past, if visitors spent more than five minutes on the site and added more than three items to the cart, they clicked the checkout button. Using this information, the logistic regression function can then predict the behavior of a new website visitor.

Logistic regression is an important technique in the field of artificial intelligence and machine learning (AI/ML). ML models are software programs that you can train to perform complex data processing tasks without human intervention. ML models built using logistic regression help organizations gain actionable insights from their business data. They can use these insights for predictive analysis to reduce operational costs, increase efficiency, and scale faster. For example, businesses can uncover patterns that improve employee retention or lead to more profitable product design.

Real-world applications of Logistic regression:  via - https://aws.amazon.com/what-is/logistic-regression/

Incorrect options:

Linear regression - Linear regression is a data analysis technique that predicts the value of unknown data by using another related and known data value. It mathematically models the unknown or dependent variable and the known or independent variable as a linear equation. For instance, suppose that you have data about your expenses and income for last year. Linear regression techniques analyze this data and determine that your expenses are half your income. They then calculate an unknown future expense by halving a future known income. You can use simple linear regression to model the relationship between two variables, such as these: Rainfall and crop yield, Age and height in children, Temperature and expansion of the metal mercury in a thermometer. For the given scenario, Logistic regression is the right choice.

Anomaly Detection - Anomaly detection is examining specific data points and detecting rare occurrences that seem suspicious because they’re different from the established pattern of behaviors. Anomaly detection is especially important in industries like finance, retail, and cybersecurity, but every business should consider an anomaly detection solution. It provides an automated means of detecting harmful outliers and protects your data.

Principal Component Analysis (PCA) Algorithm - PCA is an unsupervised machine learning algorithm that attempts to reduce the dimensionality (number of features) within a dataset while still retaining as much information as possible. This is done by finding a new set of features called components, which are composites of the original features that are uncorrelated with one another.

References:

https://aws.amazon.com/what-is/logistic-regression/

https://aws.amazon.com/what-is/linear-regression/

https://aws.amazon.com/what-is/anomaly-detection/

https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html

Temática
ML Model Development
Pregunta 52

What is the bias versus variance trade-off in machine learning?

The bias versus variance trade-off involves choosing between a model with high complexity that may capture more noise (high bias) and a simpler model that may generalize better but miss important patterns (high variance)

The bias versus variance trade-off is a technique used to improve model performance by increasing both bias and variance simultaneously to achieve better generalization

Respuesta correcta
The bias versus variance trade-off refers to the challenge of balancing the error due to the model's complexity (variance) and the error due to incorrect assumptions in the model (bias), where high bias can cause underfitting and high variance can cause overfitting

The bias versus variance trade-off refers to the balance between underfitting and overfitting, where high bias leads to overfitting and high variance leads to underfitting

Explicación general
Correct option:

The bias versus variance trade-off refers to the challenge of balancing the error due to the model's complexity (variance) and the error due to incorrect assumptions in the model (bias), where high bias can cause underfitting and high variance can cause overfitting

The bias versus variance trade-off in machine learning is about finding a balance between bias (error due to overly simplistic assumptions in the model, leading to underfitting) and variance (error due to the model being too sensitive to small fluctuations in the training data, leading to overfitting). The goal is to achieve a model that generalizes well to new data.

Incorrect options:

The bias versus variance trade-off refers to the balance between underfitting and overfitting, where high bias leads to overfitting and high variance leads to underfitting - High bias leads to underfitting, not overfitting, and high variance leads to overfitting, not underfitting.

The bias versus variance trade-off involves choosing between a model with high complexity that may capture more noise (high bias) and a simpler model that may generalize better but miss important patterns (high variance) - The explanation reverses the definitions of bias and variance. High complexity leads to high variance, and simpler models typically have higher bias.

The bias versus variance trade-off is a technique used to improve model performance by increasing both bias and variance simultaneously to achieve better generalization - Increasing both bias and variance simultaneously does not improve model performance; the key is to balance them to minimize total error.

References:

https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/mlper-09.html

https://aws.amazon.com/what-is/overfitting/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 53

You are an AI/ML engineer at a company that is rapidly expanding its use of generative AI and machine learning to create personalized customer experiences. The company is exploring AWS services to quickly prototype and deploy both generative AI models and traditional machine learning models with minimal effort. The team is particularly interested in services that provide pre-built models, templates, and the ability to scale solutions into production seamlessly.

Given these requirements, which of the following statements BEST highlights the differences between Amazon Bedrock and Amazon SageMaker JumpStart to help your team make an informed decision?

Respuesta correcta
Amazon Bedrock focuses on providing a managed service for deploying pre-trained foundation models from various providers, whereas Amazon SageMaker JumpStart offers a range of pre-built solutions, including models, notebooks, and algorithms for both machine learning and generative AI use cases

Amazon Bedrock is ideal for quick deployment of computer vision models, while Amazon SageMaker JumpStart specializes in deploying natural language processing models

Amazon Bedrock provides a simplified interface for training and tuning models from scratch, while Amazon SageMaker JumpStart is primarily for deploying third-party models with limited customization

Amazon Bedrock is designed specifically for building and deploying custom machine learning models, while Amazon SageMaker JumpStart is tailored for deploying pre-trained large language models (LLMs) with minimal customization

Explicación general
Correct option:

Amazon Bedrock focuses on providing a managed service for deploying pre-trained foundation models from various providers, whereas Amazon SageMaker JumpStart offers a range of pre-built solutions, including models, notebooks, and algorithms for both machine learning and generative AI use cases

Amazon Bedrock is the easiest way to build and scale generative AI applications with foundation models. Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

Amazon SageMaker JumpStart is a machine learning (ML) hub that can help you accelerate your ML journey. With SageMaker JumpStart, you can evaluate, compare, and select FMs quickly based on pre-defined quality and responsibility metrics to perform tasks like article summarization and image generation. SageMaker JumpStart provides managed infrastructure and tools to accelerate scalable, reliable, and secure model building, training, and deployment of ML models.

This option correctly summarizes the distinction between the two services. Amazon Bedrock is designed for deploying pre-trained foundation models (such as those from AI21 Labs, Anthropic, and Stability AI) and is optimized for generative AI tasks. Amazon SageMaker JumpStart, in contrast, provides a comprehensive set of pre-built solutions that include machine learning models, algorithms, and notebooks, making it versatile for both traditional ML and generative AI.

Incorrect options:

Amazon Bedrock is designed specifically for building and deploying custom machine learning models, while Amazon SageMaker JumpStart is tailored for deploying pre-trained large language models (LLMs) with minimal customization - This option is incorrect as it states that Amazon Bedrock is for building and deploying custom machine learning models. Bedrock is actually focused on deploying and scaling pre-trained foundation models, particularly for generative AI tasks. SageMaker JumpStart, on the other hand, offers pre-built solutions for a variety of use cases, not just LLMs.

Amazon Bedrock is ideal for quick deployment of computer vision models, while Amazon SageMaker JumpStart specializes in deploying natural language processing models - This option oversimplifies the services by suggesting they specialize in different domains (computer vision vs. NLP), which is not accurate. Both services are more versatile than this option suggests.

Amazon Bedrock provides a simplified interface for training and tuning models from scratch, while Amazon SageMaker JumpStart is primarily for deploying third-party models with limited customization - This option is incorrect as it positions Amazon Bedrock as a service for training models from scratch, which is not its primary focus. Bedrock is about deploying pre-trained foundation models, while SageMaker JumpStart provides broader ML solutions, including both pre-trained models and full-featured templates for various use cases.

References:

https://aws.amazon.com/bedrock/

https://aws.amazon.com/sagemaker/jumpstart/

https://aws.amazon.com/what-is/generative-ai/

Temática
ML Model Development
Pregunta 54

You are a machine learning engineer at a fintech company responsible for maintaining the ML infrastructure that powers real-time credit scoring for loan applications. The system must handle high volumes of requests with low latency and be resilient to any failures. To ensure the infrastructure meets the company’s performance and reliability requirements, you need to monitor key performance metrics related to scalability, availability, utilization, throughput and fault tolerance.

Which combination of metrics and monitoring strategies is the MOST EFFECTIVE for ensuring the ML infrastructure meets these requirements?

Monitor training loss and validation accuracy to track model performance, measure network bandwidth to ensure efficient data transfer, and use Amazon CloudWatch alarms to automatically restart failed instances

Measure latency to ensure predictions are delivered quickly, monitor the number of failed requests to assess fault tolerance, and use scheduled scaling to adjust resources based on anticipated demand

Track model accuracy and precision to ensure that predictions are correct, monitor disk space utilization to prevent storage overflows, and manually scale the infrastructure during peak usage periods

Respuesta correcta
Monitor CPU and memory utilization to ensure that compute resources are not overburdened, track request throughput to measure the number of predictions per second, and use auto-scaling policies to maintain high availability and scalability during traffic spikes

Explicación general
Correct option:

Monitor CPU and memory utilization to ensure that compute resources are not overburdened, track request throughput to measure the number of predictions per second, and use auto-scaling policies to maintain high availability and scalability during traffic spikes

This option correctly identifies the key performance metrics that directly impact the reliability and efficiency of ML infrastructure. Monitoring CPU and memory utilization ensures that resources are being used effectively and are not overburdened, which is critical for maintaining system performance. Tracking request throughput helps measure the system’s capacity to handle large volumes of predictions. Auto-scaling policies ensure that the system can scale up or down in response to varying demand, maintaining availability and minimizing costs.

Incorrect options:

Track model accuracy and precision to ensure that predictions are correct, monitor disk space utilization to prevent storage overflows, and manually scale the infrastructure during peak usage periods - While monitoring model accuracy and precision is important for evaluating model performance, these metrics do not directly relate to the infrastructure’s ability to handle requests efficiently. Manual scaling is less effective and responsive than auto-scaling policies.

Measure latency to ensure predictions are delivered quickly, monitor the number of failed requests to assess fault tolerance, and use scheduled scaling to adjust resources based on anticipated demand - Latency and fault tolerance are important metrics, but relying solely on scheduled scaling is less adaptive to real-time traffic fluctuations than auto-scaling. Latency should be monitored alongside other metrics, such as utilization and throughput, for a comprehensive view.

Monitor training loss and validation accuracy to track model performance, measure network bandwidth to ensure efficient data transfer, and use Amazon CloudWatch alarms to automatically restart failed instances - Training loss and validation accuracy are key during the model development phase but are not relevant for monitoring deployed ML infrastructure. While network bandwidth and CloudWatch alarms are useful, they should be part of a broader strategy that includes resource utilization, throughput, and scalability.

References:

https://aws.amazon.com/cloudwatch/

https://docs.aws.amazon.com/machine-learning/latest/dg/cw-doc.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 55

A healthcare company is developing an ML model to predict patient readmission rates. The dataset contains tabular data with ordered features, such as admission dates and billing amounts, alongside sensitive patient information like social security numbers and contact details. The ML engineer must ensure that sensitive data is masked before sharing the dataset with the data science team for model development, while preserving the meaningful structure and order of the data.

Which solution will meet these requirements?

Store the dataset in an Amazon S3 bucket with encryption enabled and share the data using Amazon Athena. Connect Athena to Amazon SageMaker to run the ML model

Use Amazon Inspector to monitor and alert on sensitive data before sharing the dataset with the team

Respuesta correcta
Use Amazon Comprehend with Amazon SageMaker Data Wrangler to mask sensitive data fields. Use Data Wrangler to prepare and transform the dataset

Use AWS Glue to remove sensitive data fields entirely before sharing the dataset

Explicación general
Correct option:

Use Amazon Comprehend with Amazon SageMaker Data Wrangler to mask sensitive data fields. Use Data Wrangler to prepare and transform the dataset

PII is defined as any representation of information that permits the identity of an individual to whom the information applies to be reasonably inferred by either direct or indirect means. PII redaction is the process of masking or removing sensitive information from a document so it can be used and distributed, while still protecting confidential information. The PII is replaced with placeholders (e.g., **, [REDACTED], or pseudonyms) to protect sensitive information while preserving the context for downstream processes.

Amazon Comprehend and SageMaker Data Wrangler can be used to automatically mask PII data from a sample dataset.

Amazon Comprehend is a natural language processing (NLP) service that uses ML to uncover insights and relationships in unstructured data, with no managing infrastructure or ML experience required. It provides functionality to locate various PII entity types within text, for example names or credit card numbers.

Using Amazon Comprehend to mask PII as part of a SageMaker Data Wrangler data preparation workflow keeps all downstream uses of the data, such as model training or inference, in alignment with your organization’s PII requirements. You can integrate SageMaker Data Wrangler with Amazon SageMaker Pipelines to automate end-to-end ML operations, including data preparation and PII redaction.

Automatically redact PII using Comprehend and Data Wrangler:  via - https://aws.amazon.com/blogs/machine-learning/automatically-redact-pii-for-machine-learning-using-amazon-sagemaker-data-wrangler/

Note that AWS Glue DataBrew is another service which can be used to mask PII from data.

Incorrect options:

Store the dataset in an Amazon S3 bucket with encryption enabled and share the data using Amazon Athena. Connect Athena to Amazon SageMaker to run the ML model - While encryption secures data at rest, it does not mask sensitive information for processing or sharing purposes for the ML model.

Use AWS Glue to remove sensitive data fields entirely before sharing the dataset - Removing sensitive data fields entirely does not align with the requirement to preserve the meaningful structure and order of the data. So, this option is incorrect.

Use Amazon Inspector to monitor and alert on sensitive data before sharing the dataset with the team - Amazon Inspector is a security vulnerability assessment service that helps improve the security and compliance of your AWS resources. Amazon Inspector automatically assesses resources for vulnerabilities or deviations from best practices, and then produces a detailed list of security findings prioritized by level of severity. Amazon Inspector cannot be used to mask sensitive information for ML models.

Reference:

https://aws.amazon.com/blogs/machine-learning/automatically-redact-pii-for-machine-learning-using-amazon-sagemaker-data-wrangler/

Temática
Data Preparation for Machine Learning (ML)
Pregunta 56

A retail business relies on an ML model to forecast daily sales, which helps manage inventory across its stores. The model runs once every evening to generate predictions for the next day. It uses sales data from the past few days as input with a maximum payload size of 3.5 MB, and the prediction process completes within 60 seconds.

What is the most suitable deployment option on Amazon SageMaker to meet these requirements?

Use a Real-time inference endpoint, with the Minimum number of copies set to 1

Use Batch transform for inference with Amazon SageMaker AI

Use an Asynchronous Inference endpoint and place the request payload in Amazon S3

Respuesta correcta
Use a serverless inference endpoint, with MaxConcurrency parameter set to 1

Explicación general
Correct option:

Use a serverless inference endpoint, with MaxConcurrency parameter set to 1

Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models without configuring or managing any of the underlying infrastructure. On-demand Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers.

Workflow of on-demand Serverless Inference:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html

Serverless inference is ideal when you have intermittent or unpredictable traffic patterns. SageMaker AI manages all of the underlying infrastructure, so there’s no need to manage instances or scaling policies. You pay only for what you use and not for idle time. It can support payload sizes up to 4 MB and processing times up to 60 seconds. Setting MaxConcurrency to 1 ensures the endpoint can handle one request at a time, which is sufficient for the daily batch inference.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-invoke.html

Incorrect options:

Use a Real-time inference endpoint, with the Minimum number of copies set to 1 - Real-time inference is ideal for online inferences that have low latency or high throughput requirements (which is not the case for the given scenario). Use real-time inference for a persistent and fully managed endpoint (REST API) that can handle sustained traffic, backed by the instance type of your choice. The 'min number of copies' specifies the minimum number of model copies that you want to have hosted on the endpoint at any given time.

Use an Asynchronous Inference endpoint and place the request payload in Amazon S3 - Amazon SageMaker Asynchronous Inference is a capability in SageMaker AI that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. None of these are relevant for the given use case. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.

Use Batch transform for inference with Amazon SageMaker AI - Batch transform is suitable for offline processing when large amounts of data are available upfront and you don’t need a persistent endpoint. You can also use batch transform for pre-processing datasets. So, this option is not relevant for the given use case.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model-options.html

https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html

https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints-invoke.html

https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 57

How does model training work in Deep Learning?

Model training in deep learning requires no data; the neural network automatically learns from predefined algorithms without any input

Model training in deep learning involves manually setting the weights and biases of a neural network based on predefined rules

Respuesta correcta
Model training in deep learning involves using large datasets to adjust the weights and biases of a neural network through multiple iterations, using techniques such as gradient descent to minimize the error

Model training in deep learning involves only the use of support vector machines and decision trees to create predictive models

Explicación general
Correct option:

Model training in deep learning involves using large datasets to adjust the weights and biases of a neural network through multiple iterations, using techniques such as gradient descent to minimize the error

In Deep Learning, model training involves feeding large datasets into the neural network and adjusting the weights and biases through multiple iterations. Techniques such as gradient descent are used to minimize the error by computing the gradient of the loss function and updating the weights to reduce the prediction error. Model training in deep learning involves initializing a neural network, feeding it data, calculating losses, adjusting weights using optimization algorithms, and iterating through this process until the model achieves satisfactory performance. Proper data preparation, validation, and hyperparameter tuning are crucial steps to ensure the model generalizes well to new, unseen data.

Incorrect options:

Model training in deep learning involves manually setting the weights and biases of a neural network based on predefined rules - Weights and biases in a neural network are not set manually; they are learned during the training process.

Model training in deep learning requires no data; the neural network automatically learns from predefined algorithms without any input - Data is crucial for training deep learning models; the network learns from input data.

Model training in deep learning involves only the use of support vector machines and decision trees to create predictive models - Deep learning primarily uses neural networks rather than support vector machines and decision trees, which are more common in traditional machine learning.

Reference:

https://aws.amazon.com/what-is/artificial-intelligence/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 58

A financial services company is building an automated pipeline to update its fraud detection ML model every week using Amazon SageMaker Pipelines. The pipeline will consist of the following steps:

A data preprocessing step to clean and transform transactional data.

A model training step to build the fraud detection model.

An evaluation step to calculate accuracy and other metrics.

A model registration step to store the new model in the SageMaker Model Registry.

The preprocessing step involves large-scale data transformations and joins across multiple datasets stored in Amazon S3. The data transformations currently run on a distributed Amazon EMR cluster. The data science team wants to integrate these transformations into the SageMaker Pipelines workflow seamlessly.

Which options should be combined for a solution that addresses these requirements? (Select two)

Selección correcta
Set up an Amazon EMR job as a callback step of the SageMaker Pipelines workflow

Set up an Amazon EMR job as the first step of the ML workflow orchestrated by AWS Step Functions

Selección correcta
Add a policy to the SageMaker Pipelines execution role to allow the role to invoke an Amazon EMR job flow

Set up an Amazon EMR job as the processing step of the SageMaker Pipelines workflow

Swap out the SageMaker Pipeline with AWS Step Functions as the ML workflow orchestration service

Explicación general
Correct options:

Set up an Amazon EMR job as a callback step of the SageMaker Pipelines workflow

A callback step allows you to integrate any task or job outside Amazon SageMaker as a step in the model building pipeline. When a callback step is invoked, the current execution of a SageMaker model building pipeline will pause and wait for an external task or job to return a task token that was generated by SageMaker at the start of call back step execution. You can use the call back step to include processing jobs external to SageMaker such a Spark job running on an Amazon EMR cluster or an extract-transform-load (ETL) task in AWS Glue as part of the SageMaker model building pipeline.

Add a policy to the SageMaker Pipelines execution role to allow the role to invoke an Amazon EMR job flow

The AmazonSageMakerPipelinesIntegrations managed policy grants permissions commonly needed to use Callback steps and Lambda steps in SageMaker Pipelines workflow. The policy can be attached to any role used for authoring or executing a pipeline. This policy grants appropriate IAM permissions needed when building pipelines that include callback steps which can be used for manual approval steps or running custom workloads.

Incorrect options:

Set up an Amazon EMR job as the first step of the ML workflow orchestrated by AWS Step Functions

Swap out the SageMaker Pipeline with AWS Step Functions as the ML workflow orchestration service

Swapping out SageMaker Pipelines workflow with AWS Step Functions offers no real advantage for the given use case. Rather, SageMaker Pipelines workflow is better suited for SageMaker specific ML operations due to the tighter integrations with the various SageMaker tools. So, both these options are incorrect.

Set up an Amazon EMR job as the processing step of the SageMaker Pipelines workflow - A processing step is designed for running data processing or feature engineering tasks within SageMaker using managed infrastructure. It leverages SageMaker Processing Jobs, where you can execute custom Python scripts or prebuilt containers for tasks like data transformation, cleaning, or validation. For the given use case, you need to set up Amazon EMR job as a callback step of the SageMaker Pipelines workflow.

References:

https://aws.amazon.com/about-aws/whats-new/2022/01/amazon-sagemaker-pipelines-emr-integration/

https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol-pipelines.html

https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 59

A retail company uses a machine learning model to predict customer demand. The vendor of the model supplies cleaned and prepared training data every 3-4 days, which is uploaded to an Amazon S3 bucket. The company has set up an Amazon SageMaker pipeline to retrain the model whenever new data becomes available. Until now, triggering the SageMaker pipeline has been a manual task. An ML engineer is tasked with implementing an automated solution to trigger the pipeline automatically whenever new data is uploaded to the S3 bucket.

What solution should the ML engineer implement to achieve this with the LEAST operational effort?

Set up a cron job to check for S3 data uploads once a day. If an S3 data file is detected, the cron job will invoke an AWS Lambda function, which will then trigger the SageMaker pipeline

Create an S3 Lifecycle configuration for the s3 bucket with an event pattern that matches the S3 upload event. Create a rule to trigger SageMaker pipeline

Using Amazon S3 Event Notifications, add a notification configuration that identifies the event you want Amazon S3 to publish. Add SageMaker pipeline as the destination to this notification configuration

Respuesta correcta
Enable Amazon EventBridge for the S3 bucket and create an EventBridge rule with an event pattern that matches the S3 upload event. Set the SageMaker pipeline as the target for the rule

Explicación general
Correct option:

Enable Amazon EventBridge for the S3 bucket and create an EventBridge rule with an event pattern that matches the S3 upload event. Set the SageMaker pipeline as the target for the rule

You can schedule your Amazon SageMaker Pipelines executions using Amazon EventBridge. Amazon SageMaker Pipelines is supported as a target in Amazon EventBridge. This allows you to initiate the execution of your model building pipeline based on any event in your event bus. Events include a new file being uploaded to your Amazon S3 bucket, a change in status of your Amazon SageMaker AI endpoint due to drift, and Amazon Simple Notification Service (SNS) topics.

Amazon S3 can send events to Amazon EventBridge whenever certain events happen in your bucket. Unlike other destinations, you don't need to select which event types you want to deliver. After EventBridge is enabled, all events below are sent to EventBridge. You can use EventBridge rules to route events to additional targets.

List of events Amazon S3 sends to EventBridge:  via - https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html

Incorrect options:

Create an S3 Lifecycle configuration for the s3 bucket with an event pattern that matches the S3 upload event. Create a rule to trigger SageMaker pipeline - Amazon S3 Lifecycle helps you store objects cost effectively throughout their lifecycle by transitioning them to lower-cost storage classes, or, deleting expired objects on your behalf. To manage the lifecycle of your objects, create an S3 Lifecycle configuration for your bucket. This configuration allows you to store S3 data cost-effectively by automatically transitioning data to more affordable storage classes, but it does not support triggering notifications.

Using Amazon S3 Event Notifications, add a notification configuration that identifies the event you want Amazon S3 to publish. Add SageMaker pipeline as the destination to this notification configuration - You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket. You store this configuration in the notification subresource that's associated with a bucket. Amazon S3 can send event notification messages to the following destinations only- Amazon Simple Notification Service (Amazon SNS) topics, Amazon Simple Notification Service (Amazon SNS) topics, AWS Lambda function and Amazon EventBridge. Since SageMaker pipeline is not included in this list, you need to use Amazon EventBridge to trigger the SageMaker pipeline.

Set up a cron job to check for S3 data uploads once a day. If an S3 data file is detected, the cron job will invoke an AWS Lambda function, which will then trigger the SageMaker pipeline - Creating a cron job and writing an AWS Lambda function involve significant coding and do not qualify as a low-overhead option.

References:

https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventBridge.html

https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.html

https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-event-notifications-eventbridge.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 60

A retail company has developed a recommendation ML model to enhance customer experience on its platform. Before fully deploying the model in production, the company wants to perform online validation by routing 20% of the traffic to the new model while the remaining traffic continues to use the existing model.

Which solution will implement the online validation with minimal operational overhead?

Leverage shadow variants feature to add the new model into the existing SageMaker endpoint. Assign a weight of 0.2 to the new model variant and use Amazon CloudWatch to monitor the number of invocations

Choose canary traffic shifting in SageMaker and configure the capacity_percent parameter to 20%. Set up Amazon CloudWatch alarms to monitor the endpoint's metrics

Respuesta correcta
Leverage production variants feature to add the new model into the existing SageMaker endpoint. Assign a weight of 0.2 to the new model variant and use Amazon CloudWatch to monitor the number of invocations

Leverage production variants feature to add the new model into the existing SageMaker endpoint. Assign a weight of 20 to the new model variant and use Amazon CloudWatch to monitor the number of invocations

Explicación general
Correct option:

Leverage production variants feature to add the new model into the existing SageMaker endpoint. Assign a weight of 0.2 to the new model variant and use Amazon CloudWatch to monitor the number of invocations

With SageMaker AI, you can test multiple models or model versions behind the same endpoint using variants. A variant consists of an ML instance and the serving components specified in a SageMaker AI model. You can have multiple variants behind an endpoint. Each variant can have a different instance type or a SageMaker AI model that can be autoscaled independently of the others. The models within the variants can be trained using different datasets, different algorithms, different ML frameworks, or any combination of all of these. All the variants behind an endpoint share the same inference code. SageMaker AI supports two types of variants, production variants and shadow variants.

If you have multiple production variants behind an endpoint, then you can allocate a portion of your inference requests to each variant. Each request is routed to only one of the production variants. The production variant to which the request was routed provides the response to the caller. You can compare how the production variants perform relative to each other. SageMaker AI emits metrics such as Latency and Invocations for each variant in Amazon CloudWatch.

Test models by specifying traffic distribution:  via - https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html

Incorrect options:

Choose canary traffic shifting in SageMaker and configure the capacity_percent parameter to 20%. Set up Amazon CloudWatch alarms to monitor the endpoint's metrics - With canary traffic shifting, you can test a portion of your endpoint traffic on the new fleet while the old fleet serves the remainder of the traffic. This testing step is a safety guardrail that validates the new fleet’s functionality before shifting all of your traffic to the new fleet. Canary traffic shifting typically requires automation and monitoring to gradually transition traffic, making it more implementation-intensive and operationally demanding. In contrast, SageMaker variants offer a lower operational overhead, as traffic routing is automatically managed by SageMaker based on the assigned weights.

Leverage production variants feature to add the new model into the existing SageMaker endpoint. Assign a weight of 20 to the new model variant and use Amazon CloudWatch to monitor the number of invocations - If you are hosting multiple models, you also assign a VariantWeight to specify how much traffic you want to allocate to each model. For example, suppose that you want to host two models, A and B, and you assign traffic weight 2 for model A and 1 for model B. SageMaker distributes two-thirds of the traffic to Model A, and one-third to model B. Hence, assigning a weight of 20 is incorrect.

Leverage shadow variants feature to add the new model into the existing SageMaker endpoint. Assign a weight of 0.2 to the new model variant and use Amazon CloudWatch to monitor the number of invocations - You can also have a shadow variant corresponding to a production variant behind an endpoint. A portion of the inference requests that goes to the production variant is copied to the shadow variant. The responses of the shadow variant are logged for comparison and not returned to the caller. This lets you test the performance of the shadow variant without exposing the caller to the response produced by the shadow variant. Shadow testing is not relevant for the given use case, so this option is ruled out.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/model-validation.html

https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 61

An e-commerce company collects real-time clickstream data from its website to analyze user behavior and recommend products. The clickstream data is continuously streamed into an Amazon Kinesis Data Streams application. The company needs to aggregate the data at one-minute intervals to calculate features such as page views and time spent per user session. The solution must provide low-latency processing and require minimal operational overhead.

Which of the following options will meet these requirements? (Select two)

Use Amazon OpenSearch Service to ingest data directly from the Kinesis Data Streams application and configure an OpenSearch dashboard to perform real-time minute-by-minute aggregations

Selección correcta
Use Amazon Managed Service for Apache Flink to process and aggregate the data in real time from the Kinesis Data Streams application. Write the aggregated results to an Amazon S3 bucket for downstream analytics and feature generation

Launch an Amazon EMR cluster with Apache Spark Streaming to process the Kinesis data stream. Aggregate the data in real time and save the results to an Amazon Redshift table

Selección correcta
Set up an Amazon Kinesis Data Firehose delivery stream to read data from the Kinesis Data Streams application, perform aggregations via an intermediary Lambda function, and write the results to an Amazon S3 bucket with a buffer window of 1 minute

Use Amazon SageMaker Processing jobs to periodically pull data from the Kinesis Data Streams application, aggregate it, and write the results to Amazon S3

Explicación general
Correct options:

Use Amazon Managed Service for Apache Flink to process and aggregate the data in real time from the Kinesis Data Streams application. Write the aggregated results to an Amazon S3 bucket for downstream analytics and feature generation

Amazon Managed Service for Apache Flink is a fully managed service that efficiently processes real-time data streams with low latency. It allows for seamless integration with Kinesis Data Streams and supports minute-by-minute aggregation with minimal operational effort.

 via - https://aws.amazon.com/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/

Set up an Amazon Kinesis Data Firehose delivery stream to read data from the Kinesis Data Streams application, perform aggregations via an intermediary Lambda function, and write the results to an Amazon S3 bucket with a buffer window of 1 minute

Kinesis Data Firehose provides a simple, low-maintenance solution for data aggregation and delivery. Its built-in transformation capabilities (via intermediary Lambda function) make it ideal for aggregating and storing data in Amazon S3.

 via - https://docs.aws.amazon.com/firehose/latest/dev/writing-with-kinesis-streams.html

Incorrect options:

Use Amazon SageMaker Processing jobs to periodically pull data from the Kinesis Data Streams application, aggregate it, and write the results to Amazon S3 - SageMaker Processing jobs are designed for batch data processing rather than real-time aggregation, resulting in higher latency and less efficiency compared to Flink or Firehose.

Launch an Amazon EMR cluster with Apache Spark Streaming to process the Kinesis data stream. Aggregate the data in real time and save the results to an Amazon Redshift table - Setting up and managing an Amazon EMR cluster requires significant operational effort and cost, which makes it less suitable for a low-maintenance solution. Redshift is also not optimal for storing frequently updated aggregated metrics.

Use Amazon OpenSearch Service to ingest data directly from the Kinesis Data Streams application and configure an OpenSearch dashboard to perform real-time minute-by-minute aggregations - While Amazon OpenSearch Service is effective for search and log analytics, it is not designed for real-time data aggregation tasks like minute-by-minute calculations. Using OpenSearch for this purpose would require additional configurations and workarounds, introducing unnecessary complexity and latency, which goes against the requirement for minimal operational overhead.

References:

https://aws.amazon.com/blogs/big-data/streaming-etl-with-apache-flink-and-amazon-kinesis-data-analytics/

https://docs.aws.amazon.com/firehose/latest/dev/writing-with-kinesis-streams.html

Temática
Data Preparation for Machine Learning (ML)
Pregunta 62

What is the primary distinction between discriminative models and generative models in the context of generative AI?

Generative models are trained on labeled data, while discriminative models can be trained on both labeled and unlabeled data

Respuesta correcta
Generative models focus on generating new data from learned patterns, whereas discriminative models classify data by distinguishing between different classes

Discriminative models are used to generate new data, while generative models are used only for classification

Discriminative models are only used for text classification, while generative models are only used for image classification

Explicación general
Correct option:

Generative models focus on generating new data from learned patterns, whereas discriminative models classify data by distinguishing between different classes

Generative models learn the underlying patterns of data to create new, similar data, while discriminative models learn to distinguish between different classes of data. Generative models, such as GPT-3, can generate new content, whereas discriminative models are used for classification tasks. The former focuses on understanding and replicating the data distribution, while the latter focuses on decision boundaries to classify inputs.

For example, discriminative models look at images - known data like pixel arrangement, line, color, and shape — and then map them to an outcome — the unknown factor. Mathematically, these models work by identifying equations that could numerically map unknown and known factors as x and y variables.

Generative models take this one step further. Instead of predicting a label given some features, they try to predict features given a certain label. Mathematically, generative modeling calculates the probability of x and y occurring together. It learns the distribution of different data features and their relationships. For example, generative models analyze animal images to record variables like different ear shapes, eye shapes, tail features, and skin patterns. They learn features and their relations to understand what different animals look like in general. They can then recreate new animal images that were not in the training set.

Incorrect options:

Discriminative models are used to generate new data, while generative models are used only for classification - Discriminative models are used primarily for classification, not for generating new data.

Discriminative models are only used for text classification, while generative models are only used for image classification - Discriminative models can be used for both text and image classification, while generative models learn the underlying patterns of data to create new data.

Generative models are trained on labeled data, while discriminative models can be trained on both labeled and unlabeled data - The training data type (labeled vs. unlabeled) is not the primary distinction between generative and discriminative models.

Reference:

https://aws.amazon.com/what-is/generative-ai/

Temática
ML Solution Monitoring, Maintenance, and Security
Pregunta 63

A financial services company has tasked its ML team to analyze customer transaction data for enhancing its fraud detection system and identify potential customer segments for targeted marketing campaigns. The company has a dataset with multiple features, including transaction amounts, frequency, and location data.

The tasks are listed below, and the ML team must use Amazon SageMaker built-in algorithms to complete these tasks:

Reduce the dimensionality of the dataset to improve model performance and visualization.

Perform cluster analysis to identify distinct customer groups.

Detect anomalous transactions that may indicate fraud.

Which combination of SageMaker built-in algorithms should the ML team use to meet the requirements?

Linear Learner for dimensionality reduction, DBSCAN for cluster analysis, and Random Cut Forest (RCF) for anomaly detection

Principal Component Analysis (PCA) for dimensionality reduction, K-Means for cluster analysis, and Neural Networks for anomaly detection

XGBoost for dimensionality reduction, K-Means for cluster analysis, and Random Cut Forest (RCF) for both anomaly detection

Respuesta correcta
Principal Component Analysis (PCA) for dimensionality reduction, K-Means for cluster analysis, Random Cut Forest (RCF) for anomaly detection

Explicación general
Correct option:

Principal Component Analysis (PCA) for dimensionality reduction, K-Means for cluster analysis, Random Cut Forest (RCF) for anomaly detection

PCA is an unsupervised machine learning algorithm that attempts to reduce the dimensionality (number of features) within a dataset while still retaining as much information as possible. This is done by finding a new set of features called components, which are composites of the original features that are uncorrelated with one another. They are also constrained so that the first component accounts for the largest possible variability in the data, the second component the second most variability, and so on.

K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data, where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity. K-means is an algorithm that trains a model that groups similar objects together. The k-means algorithm accomplishes this by mapping each observation in the input dataset to a point in the n-dimensional space (where n is the number of attributes of the observation).

Amazon SageMaker AI Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. They are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the "regular" data. Including these anomalies in a data set can drastically increase the complexity of a machine learning task since the "regular" data can often be described with a simple model.

This combination covers all the requirements. PCA reduces dimensionality, K-Means identifies clusters, RCF detects anomalies.

Incorrect options:

XGBoost for dimensionality reduction, K-Means for cluster analysis, and Random Cut Forest (RCF) for both anomaly detection - The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that tries to accurately predict a target variable by combining multiple estimates from a set of simpler models. You can use XGBoost for regression, classification (binary and multiclass), ranking problems, and not for dimensionality reduction.

Principal Component Analysis (PCA) for dimensionality reduction, K-Means for cluster analysis, and Neural Networks for anomaly detection - Deep learning is the field of artificial intelligence (AI) that teaches computers to process data in a way inspired by the human brain. Deep learning models can recognize data patterns like complex pictures, text, and sounds to produce accurate insights and predictions. A neural network is the underlying technology in deep learning. It consists of interconnected nodes or neurons in a layered structure. The nodes process data in a coordinated and adaptive system. They exchange feedback on generated output, learn from mistakes, and improve continuously. Thus, artificial neural networks are the core of a deep learning system. Neural Networks are not a SageMaker built-in algorithm for anomaly detection.

Linear Learner for dimensionality reduction, DBSCAN for cluster analysis, and Random Cut Forest (RCF) for anomaly detection - Linear models are supervised learning algorithms used for solving either classification or regression problems. The Amazon SageMaker AI linear learner algorithm provides a solution for both classification and regression problems. With the SageMaker AI algorithm, you can simultaneously explore different training objectives and choose the best solution from a validation set. Linear Learner is not used for dimensionality reduction, and DBSCAN is not a SageMaker built-in algorithm.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html

https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html

https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html

https://aws.amazon.com/compare/the-difference-between-deep-learning-and-neural-networks/

Temática
ML Model Development
Pregunta 64

A transportation company has trained an ML model in Amazon SageMaker to predict delivery times based on real-time traffic data. The company needs to deploy the model in a production environment to meet the following requirements:

Ensure high availability to provide predictions without interruptions.

Achieve low latency for real-time predictions on delivery schedules.

Process input data sizes ranging between 1 KB and 5 MB.

Handle unpredictable surges in requests during peak traffic hours.

Scale dynamically to match fluctuating demand throughout the day.

Which of the following represents the best solution for the given scenario?

Use Amazon SageMaker serverless inference to automatically scale inferences based on demand, eliminating the need for endpoint management

Respuesta correcta
Deploy the model using Amazon SageMaker real-time inference with an auto-scaling endpoint to manage bursts in traffic and provide high availability

Deploy the model using Amazon SageMaker asynchronous inference to handle unpredictable surges and process requests efficiently to meet strict low-latency requirements

Deploy the model using Amazon SageMaker batch transform to process predictions in batches during peak hours to optimize resource utilization

Explicación general
Correct option:

Deploy the model using Amazon SageMaker real-time inference with an auto-scaling endpoint to manage bursts in traffic and provide high availability

Real-Time Inference is suitable for workloads with millisecond latency requirements, payload sizes up to 6 MB, and processing times of up to 60 seconds.

Amazon SageMaker real-time inference is the optimal solution for the given use case because:

It provides dedicated, scalable endpoints for low-latency, high-availability predictions.

Auto-scaling dynamically adjusts the resources to handle surges in demand.

Supports input sizes that match the requirement.

 via - https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html

Incorrect options:

Use Amazon SageMaker serverless inference to automatically scale inferences based on demand, eliminating the need for endpoint management - Amazon SageMaker Serverless Inference is a purpose-built inference option that enables you to deploy and scale ML models without configuring or managing any of the underlying infrastructure. On-demand Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. It is not suitable for strict real-time requirements.

Deploy the model using Amazon SageMaker batch transform to process predictions in batches during peak hours to optimize resource utilization - Batch transform is meant to get inferences from large datasets. It cannot deliver real-time predictions required for this use case.

Deploy the model using Amazon SageMaker asynchronous inference to handle unpredictable surges and process requests efficiently to meet strict low-latency requirements - Asynchronous inference is designed for use cases where latency is less critical, as it processes requests asynchronously. It is not suitable for strict real-time requirements.

References:

https://docs.aws.amazon.com/sagemaker/latest/dg/hosting-faqs.html

https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html

Temática
Deployment and Orchestration of ML Workflows
Pregunta 65

A research organization is using Amazon SageMaker to train a machine learning model using the AWS infrastructure. The training dataset comprises millions of files, each several megabytes in size. These files are stored in an Amazon S3 bucket.

What immediate change can be made to quickly enhance the training performance with minimal time?

Enable S3 transfer acceleration for the existing Amazon S3 bucket. Update the training job configuration to enhance the training performance by leveraging the accelerated data transfer

Respuesta correcta
Set up an Amazon FSx for Lustre file system and link it to the existing S3 bucket. Update the training job configuration to read data directly from the file system

Set up an Amazon Elastic File System (Amazon EFS) and link it to the existing S3 bucket. Update the training job configuration to read data directly from the file system

Set up an Amazon S3 access point for the existing Amazon S3 bucket. Update the training job configuration to read data directly via the access point

Explicación general
Correct option:

Set up an Amazon FSx for Lustre file system and link it to the existing S3 bucket. Update the training job configuration to read data directly from the file system

Amazon FSx for Lustre makes it easy and cost-effective to launch and run the popular, high-performance Lustre file system. You use Lustre for workloads where speed matters, such as machine learning, high performance computing (HPC), video processing, and financial modeling.

The open-source Lustre file system is designed for applications that require fast storage—where you want your storage to keep up with your compute. Lustre was built to solve the problem of quickly and cheaply processing the world's ever-growing datasets. It's a widely used file system designed for the fastest computers in the world.

Seamlessly integrates with Amazon S3 for importing/exporting data.

How FSx for Lustre works:  via - https://aws.amazon.com/fsx/lustre/

Incorrect options:

Set up an Amazon S3 access point for the existing Amazon S3 bucket. Update the training job configuration to read data directly via the access point - Amazon S3 access points simplify data access for any AWS service or customer application that stores data in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as GetObject and PutObject. This option acts as a distractor, since S3 access point cannot be used to enhance the training performance for the given use case.

 via - https://aws.amazon.com/s3/features/access-points/

Enable S3 transfer acceleration for the existing Amazon S3 bucket. Update the training job configuration to enhance the training performance by leveraging the accelerated data transfer - Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Customers who have either web or mobile applications with widespread users or applications hosted far away from their S3 bucket can experience long and variable upload and download speeds over the Internet. S3 Transfer Acceleration (S3TA) reduces the variability in internet routing, congestion and speeds that can affect transfers, and logically shortens the distance to S3 for remote applications. For the given use case, the Amazon SageMaker model is being trained using the AWS infrastructure, so this option is irrelevant.

Set up an Amazon Elastic File System (Amazon EFS) and link it to the existing S3 bucket. Update the training job configuration to read data directly from the file system - Amazon Elastic File System (Amazon EFS) is a fully managed, serverless, shared file storage service designed for use with Amazon EC2 instances, AWS services, and on-premises resources. It provides a simple, scalable, and elastic file system that automatically grows and shrinks as you add or remove files. FSx for Lustre offers significantly higher throughput and IOPS compared to EFS, making it ideal for demanding workloads like scientific computing or large-scale data analysis.

References:

https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html

https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.html

https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html

https://aws.amazon.com/s3/features/access-points/

https://aws.amazon.com/s3/transfer-acceleration/

Temática
ML Solution Monitoring, Maintenance, and Security
